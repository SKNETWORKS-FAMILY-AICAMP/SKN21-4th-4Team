{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced RAG Evaluation with Hybrid Search & Reranker\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì€ í”„ë¡œì íŠ¸ì˜ **ì‹¤ì œ ê²€ìƒ‰ ë¡œì§(Hybrid Search + Reranker + Dual Query)**ì„ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
                "ê¸°ì¡´ì˜ ë‹¨ìˆœ Vector Search í‰ê°€ë³´ë‹¤ í›¨ì”¬ ì •í™•í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install ragas langchain langchain-openai qdrant-client rank_bm25 sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/kim/SKN/SKN21-4th-4Team/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [00:00<00:00, 1226.21it/s, Materializing param=roberta.encoder.layer.23.output.dense.weight]              \n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:17: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:17: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:17: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:17: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:35: DeprecationWarning: Importing LLMContextRecall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextRecall\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:35: DeprecationWarning: Importing LLMContextPrecisionWithReference from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextPrecisionWithReference\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:35: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/4111042708.py:35: DeprecationWarning: Importing AnswerRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerRelevancy\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (src ëª¨ë“ˆ import ìœ„í•´)\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env ë° .env.local)\n",
                "load_dotenv(override=True)\n",
                "load_dotenv('.env.local', override=True)\n",
                "\n",
                "from src.utils.config import ConfigDB, ConfigLLM, ConfigAPI\n",
                "from src.retrievals.search_agent import execute_dual_query_search\n",
                "from langchain_core.documents import Document\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "from ragas import evaluate\n",
                "from ragas.metrics import (\n",
                "    context_precision,\n",
                "    context_recall,\n",
                "    faithfulness,\n",
                "    answer_relevancy\n",
                ")\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "from langchain_qdrant import QdrantVectorStore\n",
                "from qdrant_client import QdrantClient\n",
                "from langchain_text_splitters import TokenTextSplitter\n",
                "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnablePassthrough \n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.documents import Document\n",
                "from ragas import evaluate\n",
                "from ragas.testset import TestsetGenerator\n",
                "from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                "from ragas.llms import LangchainLLMWrapper\n",
                "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
                "\n",
                "from src.utils.config import ConfigDB, ConfigLLM\n",
                "from src.prompts import ANALYSIS_SYSTEM_PROMPT\n",
                "from src.retrievals.search_agent import execute_dual_query_search\n",
                "from src.retrievals.reranker import Reranker\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM ëª¨ë¸ íŒŒì¸íŠœë‹(Fine tuning)  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(Foundation ëª¨ë¸)\n",
                        "**íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**(**foundation model**)ì€ **ëŒ€ê·œëª¨ ë°ì´í„°**(**í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“±**)ë¡œ ì‚¬ì „ í•™ìŠµ(pre-training)ëœ **ë²”ìš© ì¸ê³µì§€ëŠ¥ ëª¨ë¸**ë¡œ, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…(Downstream task)ë¥¼ ìœ„í•œ **íŒŒì¸íŠœë‹**ì´ë‚˜ **í”„ë¡¬í”„íŠ¸**(**prompt**) ê¸°ë°˜ì˜ ì‘ë‹µ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§í•œë‹¤.  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ íŠ¹ì§•\n",
                        "- **ê´‘ë²”ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ**\n",
                        "- ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ(pre-training)ë˜ë©°, ê·¸ ê²°ê³¼ **ë‹¤ì–‘í•œ ì£¼ì œì™€ ì—¬ëŸ¬ ì–¸ì–´ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ê³¼ ì–¸ì–´ íŒ¨í„´ë“± ì–¸ì–´ì— ëŒ€í•œ ë²”ìš©ì  ì§€ì‹**ì„ í¬í•¨í•˜ê³  ìˆë‹¤.\n",
                        "- ì´ë¯¸ì§€ ê¸°ë°˜ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(image-based foundation model) ë˜í•œ ëŒ€ê·œëª¨ ì´ë¯¸ì§€Â·ì˜ìƒ ë°ì´í„°ì—ì„œ ì‚¬ë¬¼, ì¥ë©´, ë¬¼ì²´ êµ¬ì¡°ì˜ ì‹œê°ì  íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬, **ê°œë³„ ì‚¬ë¬¼ì˜ íŠ¹ì„±(ì˜ˆ: í˜•íƒœ, ì§ˆê°, ë§¥ë½)ê³¼ ë¬¼ì²´ë“¤ì´ ê³µìœ í•˜ëŠ” ì¼ë°˜ì  ë¬¼ë¦¬ íŠ¹ì„±(ì˜ˆ: ê²½ê³„, ì…ì²´ê°, ì¬ì§ˆ, ê³µê°„ ê´€ê³„)ì„ í‘œí˜„ ê³µê°„(representation space)ì— ë°˜ì˜**í•œë‹¤.\n",
                        "- **ëŒ€ê·œëª¨ ëª¨ë¸**\n",
                        "- íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ **Transformer** êµ¬ì¡°ë¥¼ ê¸°ë°˜(íŠ¹íˆ ìì—°ì–´ ëª¨ë¸)ìœ¼ë¡œ í•˜ë©°, **ìˆ˜ì‹­ì–µì—ì„œ ìˆ˜ì¡°ê°œì˜ íŒŒë¼ë¯¸í„°** ê·œëª¨ë¥¼ ê°€ì§€ê³  ìˆë‹¤.\n",
                        "- **ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ ìˆ˜ë¡ ì„±ëŠ¥ê³¼ í‘œí˜„ë ¥ì´ í–¥ìƒëœë‹¤.**\n",
                        "- **ë²”ìš©ì„±**\n",
                        "- **íŠ¹ì • ì‘ì—…ì— êµ­í•œ ë˜ì§€ ì•Šê³ **, ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ì—­ëŸ‰ì„ ì œê³µí•œë‹¤.\n",
                        "- ì¼ë°˜ì ì¸ NLP taskì¸ ë¬¸ì¥ ìƒì„±, ë¬¸ì„œìš”ì•½, ë²ˆì—­, ì§ˆì˜ì‘ë‹µì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\n",
                        "- **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì‹¤í–‰**\n",
                        "- ì¶”ê°€ì ì¸ í•™ìŠµ(íŒŒì¸íŠœë‹) ì—†ì´ë„ **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**ì„ í†µí•´ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n",
                        "- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€ **AI ëª¨ë¸ë¡œë¶€í„° ìµœìƒì˜ ê²°ê³¼ë¬¼ì„ ì–»ì–´ë‚´ê¸° ìœ„í•´ ì§ˆë¬¸ì„ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê³  ìµœì í™”í•˜ëŠ” ê¸°ìˆ ì„ ë§í•œë‹¤.** LLMì€ ì‚¬ì „í•™ìŠµì„ í†µí•´ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— íŒŒì¸íŠœë‹ ì—†ì´ë„ í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸)ì„ ì˜ ì‘ì„±í•˜ë©´ ë‹µì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
                        "- **ì „ì´ í•™ìŠµ/íŒŒì¸íŠœë‹ì˜ ê¸°ë°˜ ëª¨ë¸**\n",
                        "- ëª¨ë¸ì˜ ê¸°ë³¸ ëŠ¥ë ¥(ì–¸ì–´/ì‚¬ë¬¼ì— ëŒ€í•œ ì¼ë°˜ì  íŠ¹ì„±)ì„ ê°€ì§€ê³  ìˆìœ¼ë©° ì´ë¥¼ **íŠ¹ì • ë„ë©”ì¸ ì‘ì—…ì— ë§ê²Œ ì¡°ì •**í•  ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '1_LLM_Finetuningê°œìš”.ipynb', 'lecture_title': '1_LLM_Finetuningê°œìš”', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6061130604709875}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6061130604709875\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM Fine tuningìœ¼ë¡œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
                        "- **ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ í•„ìš”**\n",
                        "- LLMì€ ìµœì†Œ ìˆ˜ì‹­ì–µê°œì—ì„œ ì¡°ë‹¨ìœ„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ëª¨ë¸ì´ë‹¤. ê·¸ë˜ì„œ íŒŒì¸íŠœë‹ì„í•˜ë ¤ë©´ ì´ì „ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë¹„í•´ ë§ì€ GPUë¥¼ ê°€ì§€ëŠ” ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•˜ë‹¤.\n",
                        "- **ì˜¤ëœ í•™ìŠµì‹œê°„**\n",
                        "- íŒŒì¸íŠœë‹ì€ ë°ì´í„° ì „ì²˜ë¦¬, í•™ìŠµ, í‰ê°€ì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.\n",
                        "- **í° ë¹„ìš©**\n",
                        "- ìœ„ì™€ ê°™ì€ ì´ìœ ë¡œ íŒŒì¸íŠœë‹ì€ ë§ì€ ë¹„ìš©ì´ ì†Œìš”ëœë‹¤.\n",
                        "- **ë„ë©”ì¸ í•œì •ì„±**\n",
                        "- íŒŒì¸íŠœë‹ í›„ **ë²”ìš©ì ì¸ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤**.\n",
                        "- íŒŒì¸íŠœë‹ ê³¼ì •ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ë©´ì„œ ê¸°ì¡´ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ ë²”ìš©ì ì¸ ì§€ì‹ì„ ì¼ë¶€ ë®ì–´ì“°ê±°ë‚˜ ì™œê³¡í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.' metadata={'source': '', 'source_file': '1_LLM_Finetuningê°œìš”.ipynb', 'lecture_title': '1_LLM_Finetuningê°œìš”', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.631175932}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.631175932\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "íŒŒì¸íŠœë‹  \n",
                        "Data Collator  \n",
                        "- í•™ìŠµ ë„ì¤‘ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
                        "- Dataset -> Data Collatorí•¨ìˆ˜ -> ëª¨ë¸\n",
                        "- ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì— ì „ë‹¬ë˜ëŠ” batchë¥¼ ë°›ì•„ì„œ ëª¨ë¸ì— ì…ë ¥ì „ì— í•´ì•¼í•˜ëŠ” ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜(Callable).  \n",
                        "- êµ¬í˜„í•  ë‚´ìš©\n",
                        "- ëª¨ë¸ chat í˜•ì‹ì˜ ë¬¸ìì—´ì„ transformers ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ìœ„í•œ inputsë¥¼ ë§Œë“ ë‹¤.  \n",
                        "```json\n",
                        "{\n",
                        "\"input_ids\":ì…ë ¥ sequenceì˜ í† í° IDë“¤,\n",
                        "\"attention_mask\":ì…ë ¥í† í°ê³¼ paddingêµ¬ë¶„,\n",
                        "\"labels\": input_idsì—ì„œ ë‹µë³€ë¶€ë¶„ masking. ë‹µë³€ì€ í† í°ID ë‚˜ë¨¸ì§€ëŠ” -100ìœ¼ë¡œ ì±„ìš´ë‹¤.\n",
                        "}\n",
                        "```' metadata={'source': '', 'source_file': '3_Fine_tuning.ipynb', 'lecture_title': '3_Fine_tuning', 'cell_type': 'markdown', 'cell_index': 36, 'code_snippet': '```python\\nimport torch\\n\\nmax_seq_length = 8192\\n\\ndef collate_fn(batch: list[dict]) -> dict[str, torch.Tensor]:\\n    \"\"\"ëª¨ë¸ í•™ìŠµì‹œ batchë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì²˜ë¦¬í•´ì„œ ë°˜í™˜\\n    Args:\\n        batch (list[dict]): ë°°ì¹˜ ë°ì´í„°, dict: {\"train_prompt\":input text}\\n\\n    Returns:\\n        dict[str, torch.Tensor]: ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì²˜ë¦¬ëœ ë°°ì¹˜ ë°ì´í„° (inputs, attention_mask, labels)\\n    \"\"\"\\n    \\n    new_batch = {\\n        \"input_ids\": [],\\n        \"attention_mask\": [],\\n        \"labels\": []\\n    }\\n    \\n    for prompt in batch:\\n        \\n        text = prompt[\\'train_prompt\\'].strip() #<|begin_of_text|>......\\n        # text -> system prompt / user prompt / ai prompt(label)\\n        tokenized = tokenizer(\\n            text,\\n            truncation=True,\\n            max_length=max_seq_length, # max_lengthì´í•˜ëŠ” ìë¥¸ë‹¤. truncation=True\\n            padding=False,             # paddingì€ ë’¤ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•  ê²ƒì´ê¸° ë•Œë¬¸ì— paddingì²˜ë¦¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë™ì  íŒ¨ë”©. batchì˜ max_lengthì— ë§ì¶° íŒ¨ë”©.\\n            return_tensors=None,       # listë¡œ ë°˜í™˜.\\n        )\\n\\n        input_ids = tokenized[\"input_ids\"]\\n        attention_mask = tokenized[\"attention_mask\"]\\n        labels = [-100] * len(input_ids) \\n        # ë‹µë³€ tokenê°’ì„ ë„£ì„ ë¦¬ìŠ¤íŠ¸. system/user prompt ë¶€ë¶„ì€ -100ìœ¼ë¡œ, ë‹µë³€ ë¶€ë¶„ì€ í† í°ê°’ë“¤ë¡œ ë³€ê²½.\\n        # [-100, -100, ...:system/user, 1289, 32312, ... : Label] \\n        \\n        # -100: íŒŒì´í† ì¹˜ì˜ CrossEntropyLoss()ëŠ” -100ì€ Lossê³„ì‚°ì‹œ ë¬´ì‹œ.\\n\\n        # # chat promptì—ì„œ ë‹µë³€ ë¶€ë¶„ì„ ì°¾ì•„ì„œ labelsë¥¼ êµ¬ì„±í•œë‹¤. \\n\\n        # <|begin_of_text|>\\n        # <|start_header_id|>system<|end_header_id|>\\n        # {ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸}<|eot_id|>\\n        # <|start_header_id|>user<|end_header_id|>\\n        # {ì…ë ¥ - title+document}<|eot_id|>\\n        # <|start_header_id|>assistant<|end_header_id|> -------> ì—¬ê¸°ë¥¼ ì°¾ê¸°. ê·¸ëŒ€ë¡œ ë‘ê³ .(-100)\\n        # {ë‹µë³€ - Label}<|eot_id|>                       -------> ì‹¤ì œ í† í°ìœ¼ë¡œ ë³€ê²½\\n\\n        # assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\\\n\"\\n        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\\n\\n        eot_token = \"<|eot_id|>\"\\n        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\\n\\n        i = 0\\n        while i <= len(input_ids) - len(assistant_tokens):\\n            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens: # assistant_tokens ì°¾ê¸°\\n                start = i + len(assistant_tokens) # \"<|start_header_id|>assistant<|end_header_id|>\\\\n\" ë‹¤ìŒ í† í°ìœ¼ë¡œ ì´ë™\\n                end = start\\n                while end <= len(input_ids) - len(eot_tokens):\\n                    if input_ids[end:end + len(eot_tokens)] == eot_tokens: # ë§ˆì§€ë§‰ í† í°ì¸ \"<|eot_id|>\" ì˜ indexë¥¼ ì°¾ëŠ”ë‹¤.\\n                        break\\n                    end += 1\\n                for j in range(start, end):\\n                    labels[j] = input_ids[j]  # Label ë‹µë³€ í† í°ë§Œ labelsì— ë„£ê¸°.\\n                for j in range(end, end + len(eot_tokens)):\\n                    labels[j] = input_ids[j]\\n                break\\n            i += 1\\n        \\n        # # ìƒì„±ëœ input_ids, attention_mask, labelsë¥¼ new_batchì— ì¶”ê°€í•œë‹¤.\\n        # new_batch[\"input_ids\"].append(input_ids)\\n        new_batch[\"attention_mask\"].append(attention_mask)\\n        new_batch[\"labels\"].append(labels)\\n    # ------------------ë¼ë²¨ ì²˜ë¦¬--------------- \\n\\n    # #  íŒ¨ë”© ì²˜ë¦¬ - ë™ì  íŒ¨ë”©.\\n    #  -  ë°°ì¹˜ë‚´ ì…ë ¥ì¤‘ ê°€ì¥ ê¸´ sampleì— ê¸¸ì´ë¥¼ ë§ì¶˜ë‹¤.\\n    # max_length = max(len(ids) for ids in new_batch[\"input_ids\"])  # batchì˜ max_length ê³„ì‚°.        \\n    for i in range(len(new_batch[\"input_ids\"])):\\n        pad_len = max_length - len(new_batch[\"input_ids\"][i]) \\n        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len) # íŒ¨ë”© í† í° ì¶”ê°€\\n        new_batch[\"attention_mask\"][i].extend([0] * pad_len)                 # íŒ¨ë”© í† í°ì˜ ìœ„ì¹˜ 0 ì„¤ì •.\\n        new_batch[\"labels\"][i].extend([-100] * pad_len)\\n\\n    # list -> torch.Tensorë¡œ ë³€í™˜.\\n    for k in new_batch:\\n        new_batch[k] = torch.tensor(new_batch[k])\\n\\n    return new_batch\\n```\\n\\n```python\\n# # í™•ì¸\\n# example = [trainset[11], trainset[2], trainset[3]]\\nbatch = collate_fn(example)\\n\\nprint(\"batch:\")\\nprint(\"input_ids í¬ê¸°:\", batch[\"input_ids\"].shape)\\nprint(\"attention_mask í¬ê¸°:\", batch[\"attention_mask\"].shape)\\nprint(\"labels í¬ê¸°:\", batch[\"labels\"].shape)\\n```', 'chunk_index': 9, 'original_score': 0.5990258213908755}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5990258213908755\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: 3\n",
                        "[1] [ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM Fine tuningìœ¼ë¡œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
                        "- **ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ í•„ìš”**\n",
                        "- LLMì€ ìµœì†Œ ìˆ˜ì‹­ì–µê°œì—ì„œ ì¡°ë‹¨ìœ„ì˜ ë§¤ê°œë³€ìˆ˜... (Score: 0.6312)\n",
                        "[2] [ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM ëª¨ë¸ íŒŒì¸íŠœë‹(Fine tuning)  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(Foundation ëª¨ë¸)\n",
                        "**íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**(**foundati... (Score: 0.6061)\n",
                        "[3] [ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "íŒŒì¸íŠœë‹  \n",
                        "Data Collator  \n",
                        "- í•™ìŠµ ë„ì¤‘ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
                        "- Dataset -> Data Collatorí•¨ìˆ˜... (Score: 0.5990)\n"
                    ]
                }
            ],
            "source": [
                "#################################################################\n",
                "# 1. Advanced Retriever ì •ì˜ (Hybrid + Reranker)\n",
                "#################################################################\n",
                "\n",
                "def advanced_retriever(query: str):\n",
                "    \"\"\"\n",
                "    src.retrievals.search_agentì˜ execute_dual_query_searchë¥¼ ì‚¬ìš©í•˜ëŠ” Retriever í•¨ìˆ˜\n",
                "    - Dual Query (í•œê¸€/ì˜ì–´)\n",
                "    - Hybrid Search (Vector + Keyword + BM25)\n",
                "    - Reranking (Cross-Encoder)\n",
                "    \"\"\"\n",
                "    # ê²€ìƒ‰ ì‹¤í–‰\n",
                "    results, info = execute_dual_query_search(query)\n",
                "\n",
                "    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (Inference)\n",
                "    reranker = Reranker(top_k=ConfigLLM.TOP_K)\n",
                "    reranked_results = reranker.invoke(query, results)\n",
                "\n",
                "    # Dict ê²°ê³¼ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜\n",
                "    documents = []\n",
                "    for r in reranked_results:\n",
                "        doc = Document(\n",
                "            page_content=r['content'],\n",
                "            metadata=r.get('metadata', {})\n",
                "        )\n",
                "        # ì ìˆ˜ ì •ë³´ë„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€\n",
                "        doc.metadata['score'] = r.get('score', 0)\n",
                "        doc.metadata['source'] = r.get('source', '')\n",
                "        documents.append(doc)\n",
                "        \n",
                "    return documents\n",
                "\n",
                "# í…ŒìŠ¤íŠ¸\n",
                "test_docs = advanced_retriever(\"íŒŒì¸íŠœë‹ì´ ë­ì•¼?\")\n",
                "print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(test_docs)}\")\n",
                "for i, doc in enumerate(test_docs):\n",
                "    print(f\"[{i+1}] {doc.page_content[:100]}... (Score: {doc.metadata.get('score'):.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<>:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "<>:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/3359774379.py:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "  - **ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬**: ë³¸ë¬¸ ë‚´ì˜ í°ë”°ì˜´í‘œ(\"), ë°±ìŠ¬ë˜ì‹œ(\\) ë“± íŠ¹ìˆ˜ ë¬¸ìëŠ” ë°˜ë“œì‹œ ì—­ìŠ¬ë˜ì‹œ(\\\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•˜ì‹­ì‹œì˜¤.\n"
                    ]
                }
            ],
            "source": [
                "llm_context = \"\"\"ë‹¹ì‹ ì€ Python í”„ë¡œê·¸ë˜ë° êµìœ¡ ê³¼ì •ì˜ **AI ì¡°êµ(Teaching Assistant)**ì´ì ë°ì´í„°ì…‹ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
                "ì œê³µëœ [ê°•ì˜ ìë£Œ(Lecture)]ì™€ [Python ê³µì‹ ë¬¸ì„œ(RST)]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, í•™ìŠµì í‰ê°€ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆì˜ JSON í¬ë§· ì§ˆì˜ì‘ë‹µ(QA) ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.\n",
                "\n",
                "ìƒì„± ì‹œ ë‹¤ìŒì˜ **ì§€ì¹¨(Instruction)**ê³¼ ë‹µë³€ ìŠ¤íƒ€ì¼(Tone & Style)ì„ ì² ì €íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "---\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "llm_context += ANALYSIS_SYSTEM_PROMPT\n",
                "\n",
                "llm_context += \"\"\"\n",
                "\n",
                "---\n",
                "\n",
                "## QA ìƒì„± ì œì•½ (Constraints)\n",
                "- **ì–¸ì–´**: ê°€ëŠ¥í•˜ë©´ í•œêµ­ì–´ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µë³€í•˜ë˜, ì½”ë“œ ì˜ˆì œëŠ” ì˜ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
                "- **ë¬¸ë²•**: ë¬¸ì¥ì˜ ëì€ ë§ˆì¹¨í‘œ(.) ë“± **êµ¬ë‘ì ì„ ë°˜ë“œì‹œ í‘œê¸°**í•˜ì—¬ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
                "- **JSON í¬ë§· ì—„ìˆ˜**:\n",
                "  - ì¶œë ¥ì€ ë°˜ë“œì‹œ íŒŒì‹± ê°€ëŠ¥í•œ **JSON ë°°ì—´** í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤ (`[{\"user_input\": \"...\", \"reference\": \"...\"}, ...]`).\n",
                "  - **ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬**: ë³¸ë¬¸ ë‚´ì˜ í°ë”°ì˜´í‘œ(\"), ë°±ìŠ¬ë˜ì‹œ(\\) ë“± íŠ¹ìˆ˜ ë¬¸ìëŠ” ë°˜ë“œì‹œ ì—­ìŠ¬ë˜ì‹œ(\\\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•˜ì‹­ì‹œì˜¤.\n",
                "  - ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì— JSON ë¬¸ë²•ì„ í•´ì¹˜ëŠ” ìš”ì†Œê°€ ìˆë”ë¼ë„, ìµœì¢… ì¶œë ¥ì€ ìœ íš¨í•œ JSONì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "---\n",
                "\n",
                "## QA ìƒì„± ì¶œë ¥ ì˜ˆì‹œ\n",
                "[\n",
                "  {\n",
                "    \"user_input\": \"íŒŒì¸íŠœë‹ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€?\",\n",
                "    \"reference\": \"íŒŒì¸íŠœë‹ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(Foundation ëª¨ë¸)ì„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë„ë©”ì¸ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ ìµœì í™” í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, íŠ¹ì • ì‘ì—…/ë„ë©”ì¸ì— ìµœì í™”, ì‚¬ìš©ì ë§ì¶¤í˜• í†¤ì•¤ë§¤ë„ˆ ì ìš©, ì•ˆì „ì„±ê³¼ ìœ¤ë¦¬ ê°•í™”ë¥¼ ìœ„í•´ í•„ìš”í•˜ë‹¤.\"\n",
                "  }\n",
                "]\n",
                "\n",
                "ìœ„ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ í•™ìŠµ ê°€ì¹˜ê°€ ë†’ì€ QA ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['[ê°•ì˜: 09_CNN_ê°œìš”]\\n\\n- Filter/Kernel',\n",
                            " '[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\\n- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\\n- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\\n- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\\n- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\\n- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.',\n",
                            " \"[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \\n- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\\n1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\\n- output layer: units=1, activation='sigmoid'\\n- loss: binary_crossentropy\\n2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\\n- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\\n- loss: categorical_crossentropy\\n- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\\n- Feature\\n- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\\n- Targetì˜ class\\n- 0 - malignant(ì•…ì„±ì¢…ì–‘)\\n- 1 - benign(ì–‘ì„±ì¢…ì–‘)\",\n",
                            " '[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤',\n",
                            " '[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\\n- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\\n- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸',\n",
                            " '[ê°•ì˜: 04_EC2_Pythonê°œë°œí™˜ê²½_êµ¬ì„±]\\n\\nVSCodeë¥¼ ì´ìš©í•´ EC2 instance ì—°ê²°  \\n- VSCodeì˜ **Remote-SSH** Extensionì„ ì´ìš©í•´ EC2 instanceì— ì—°ê²°í•´ Local í™˜ê²½ì²˜ëŸ¼ ê°œë°œ í•  ìˆ˜ìˆë‹¤.\\n- í”„ë¦¬í‹°ì–´ ì‚¬ì–‘ì—ì„œëŠ” ì ‘ì†ì´ ì›í™œí•˜ì§€ ì•Šì„ ìˆ˜ìˆë‹¤.',\n",
                            " '[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\\n- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \\n3. **ë„êµ¬ í™œìš©** (Tool Utilization)\\n- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\\n- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \\n4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\\n- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\\n- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.']"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "client = QdrantClient(host='localhost', port=ConfigDB.PORT)\n",
                "info = client.get_collection(ConfigDB.COLLECTION_NAME)\n",
                "\n",
                "results, next_id = client.scroll(\n",
                "    collection_name=ConfigDB.COLLECTION_NAME,\n",
                "    limit=info.points_count,\n",
                ")\n",
                "\n",
                "# sampling\n",
                "import random\n",
                "sample_dataset = random.sample(results, 50) # ë¦¬ìŠ¤íŠ¸ì—ì„œ ëœë¤í•˜ê²Œ Kê°œë¥¼ ì¶”ì¶œ\n",
                "\n",
                "\n",
                "# ë¬¸ì„œ ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
                "docs = [point.payload['page_content'] for point in sample_dataset if point.payload.get('metadata', {}).get('source', '') == 'lecture']\n",
                "docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/2408905276.py:8: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
                        "  generator_llm = LangchainLLMWrapper(ChatOpenAI(model=ConfigLLM.OPENAI_MODEL))\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_4612/2408905276.py:9: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
                        "  generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL))\n",
                        "Applying SummaryExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  1.22s/it]\n",
                        "Applying CustomNodeFilter:   0%|          | 0/7 [00:00<?, ?it/s]Node b78d7f90-8fa5-48b2-bf42-d301736fd3b8 does not have a summary. Skipping filtering.\n",
                        "Node d2d0e5fb-78bf-4129-a028-d1cbe340fb72 does not have a summary. Skipping filtering.\n",
                        "Node e5bac020-1f4b-4916-a80d-cff078d7bd85 does not have a summary. Skipping filtering.\n",
                        "Node 4b0abf3f-0355-4a20-9009-c6dc10827cb6 does not have a summary. Skipping filtering.\n",
                        "Node abc19e78-85dc-4ab3-9402-26d30b579718 does not have a summary. Skipping filtering.\n",
                        "Node 49a33608-df56-4dd6-81ff-fecd948f5380 does not have a summary. Skipping filtering.\n",
                        "Node 3c26cd0b-6bad-47c8-a05c-413cbd37a6f6 does not have a summary. Skipping filtering.\n",
                        "Applying CustomNodeFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 3369.69it/s]\n",
                        "Applying EmbeddingExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.60it/s]\n",
                        "Applying ThemesExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.19it/s]\n",
                        "Applying NERExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.30it/s]\n",
                        "Applying CosineSimilarityBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 340.47it/s]\n",
                        "Applying OverlapScoreBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 624.25it/s]\n",
                        "Skipping multi_hop_abstract_query_synthesizer due to unexpected error: No relationships match the provided condition. Cannot form clusters.\n",
                        "Generating personas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.07s/it]\n",
                        "Generating Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.49s/it]\n",
                        "Generating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:12<00:00,  1.22it/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Testset(samples=[TestsetSample(eval_sample=SingleTurnSample(user_input='ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\\n- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\\n- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\\n- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\\n- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\\n- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°(raw data)ì—ì„œ ë…¸ì´ì¦ˆ(noise)ë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì—ì„œëŠ” ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='MISSPELLED', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='íŠ¹ì„± ì¶”ì¶œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\\n- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\\n- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\\n- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\\n- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\\n- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='íŠ¹ì„± ì¶”ì¶œì€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì˜ ì¼í™˜ìœ¼ë¡œ, ì›ë³¸ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì • ì¤‘ì— íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='WEB_SEARCH_LIKE', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\\n- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\\n- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\\n- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\\n- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\\n- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='PERFECT_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì–´ë–¤ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆë‚˜ìš”?', retrieved_contexts=None, reference_contexts=[\"[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \\n- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\\n1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\\n- output layer: units=1, activation='sigmoid'\\n- loss: binary_crossentropy\\n2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\\n- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\\n- loss: categorical_crossentropy\\n- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\\n- Feature\\n- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\\n- Targetì˜ class\\n- 0 - malignant(ì•…ì„±ì¢…ì–‘)\\n- 1 - benign(ì–‘ì„±ì¢…ì–‘)\"], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤ê³¼ í•¨ê»˜, 0ì€ ì•…ì„±ì¢…ì–‘(malignant), 1ì€ ì–‘ì„±ì¢…ì–‘(benign)ìœ¼ë¡œ í‘œì‹œëœ íƒ€ê²Ÿ í´ë˜ìŠ¤ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='POOR_GRAMMAR', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ë­ì— ì“°ëŠ”ê±°ì•¼?', retrieved_contexts=None, reference_contexts=[\"[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \\n- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\\n1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\\n- output layer: units=1, activation='sigmoid'\\n- loss: binary_crossentropy\\n2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\\n- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\\n- loss: categorical_crossentropy\\n- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\\n- Feature\\n- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\\n- Targetì˜ class\\n- 0 - malignant(ì•…ì„±ì¢…ì–‘)\\n- 1 - benign(ì–‘ì„±ì¢…ì–‘)\"], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='POOR_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=[\"[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \\n- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\\n1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\\n- output layer: units=1, activation='sigmoid'\\n- loss: binary_crossentropy\\n2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\\n- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\\n- loss: categorical_crossentropy\\n- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\\n- Feature\\n- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\\n- Targetì˜ class\\n- 0 - malignant(ì•…ì„±ì¢…ì–‘)\\n- 1 - benign(ì–‘ì„±ì¢…ì–‘)\"], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ, ë‹¤ì–‘í•œ ì¢…ì–‘ì— ëŒ€í•œ ì¸¡ì •ê°’ë“¤ê³¼ í•¨ê»˜ 0ì€ ì•…ì„±ì¢…ì–‘(malignant), 1ì€ ì–‘ì„±ì¢…ì–‘(benign)ìœ¼ë¡œ í‘œì‹œëœ Targetì˜ classë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='PERFECT_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ê°•ì˜ì—ì„œ ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤„ ìˆ˜ ìˆë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ê°•ì˜ ìë£Œì—ì„œëŠ” ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='MISSPELLED', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ê°•ì˜ ìë£Œì—ì„œëŠ” ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='PERFECT_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ë­ì—ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ë¡œ, ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³  ìµœì í™”ë˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='POOR_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\\n- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\\n- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°ì…ë‹ˆë‹¤. ì´ë•ŒëŠ” FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ” ë° ì´ˆì ì„ ë§ì¶”ë©°, ì˜ˆë¥¼ ë“¤ì–´ ì•”í™˜ì íŒì • ëª¨ë¸ì´ë‚˜ ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='WEB_SEARCH_LIKE', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\\n- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\\n- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ” ë° ì´ˆì ì„ ë§ì¶”ì–´ì•¼ í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Scientist', query_style='PERFECT_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\\n- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\\n- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°ì…ë‹ˆë‹¤. ì´ë•ŒëŠ” FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê²Œ ë˜ë©°, ì˜ˆë¥¼ ë“¤ì–´ ì•”í™˜ì íŒì • ëª¨ë¸ì´ë‚˜ ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸ì—ì„œ ì´ëŸ¬í•œ ìƒí™©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='WEB_SEARCH_LIKE', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='LLMì´ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\\n- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \\n3. **ë„êµ¬ í™œìš©** (Tool Utilization)\\n- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\\n- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \\n4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\\n- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\\n- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='LLMì€ Agentê°€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ê³¼ ì‚¬ì „ í•™ìŠµ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ê²Œ í•¨ìœ¼ë¡œì¨, ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ê¸°ì—¬í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Computer Vision Engineer', query_style='WEB_SEARCH_LIKE', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='í–‰ë™ ì‹¤í–‰ì´ë€ ë¬´ì—‡ì¸ê°€?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\\n- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \\n3. **ë„êµ¬ í™œìš©** (Tool Utilization)\\n- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\\n- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \\n4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\\n- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\\n- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='í–‰ë™ ì‹¤í–‰ì€ Agentê°€ ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœ ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤.', rubrics=None, persona_name='Deep Learning Researcher', query_style='MISSPELLED', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ë¬¸ì œ ë¶„ì„ì´ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆê¹Œ?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\\n- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \\n3. **ë„êµ¬ í™œìš©** (Tool Utilization)\\n- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\\n- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \\n4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\\n- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\\n- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference=\"ë¬¸ì œ ë¶„ì„ì€ Agentê°€ ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ë˜ëŠ” ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. AgentëŠ” íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•˜ë©°, ì´ ê³¼ì •ì—ì„œ 'ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë¬¸ì œ ë¶„ì„ì€ Agentê°€ ëª©í‘œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ë‹¨ê³„ì…ë‹ˆë‹¤.\", rubrics=None, persona_name='Deep Learning Researcher', query_style='PERFECT_GRAMMAR', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer')])"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#################################################################\n",
                "# 2. Testset ì¤€ë¹„\n",
                "# (ê¸°ì¡´ì— ìƒì„±í•´ë‘” í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±)\n",
                "#################################################################\n",
                "import pandas as pd\n",
                "from datasets import Dataset\n",
                "\n",
                "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=ConfigLLM.OPENAI_MODEL))\n",
                "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL))\n",
                "\n",
                "generator = TestsetGenerator(\n",
                "    llm=generator_llm,\n",
                "    embedding_model=generator_embeddings,\n",
                "    llm_context=llm_context \n",
                ")\n",
                "\n",
                "# testset\n",
                "testset = generator.generate_with_chunks(\n",
                "    docs, testset_size=20  # context ë‚´ìš©, í…ŒìŠ¤íŠ¸ë°ì´í„°ì…‹ ëª‡ê°œë¥¼ ë§Œë“¤ì§€.\n",
                ")\n",
                "testset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reference ë°ì´í„° ì¬ìƒì„± ì¤‘...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_input</th>\n",
                            "      <th>reference_contexts</th>\n",
                            "      <th>reference</th>\n",
                            "      <th>persona_name</th>\n",
                            "      <th>query_style</th>\n",
                            "      <th>query_length</th>\n",
                            "      <th>synthesizer_name</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>íŠ¹ì„± ì¶”ì¶œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ì›ë³¸ ë°ì´í„°ì—ì„œ...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì–´ë–¤ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ë­ì— ì“°ëŠ”ê±°ì•¼?</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°ì´í„°...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                       user_input  \\\n",
                            "0                       ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.   \n",
                            "1                                  íŠ¹ì„± ì¶”ì¶œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "2                          ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "3  ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì–´ë–¤ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆë‚˜ìš”?   \n",
                            "4                         ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ë­ì— ì“°ëŠ”ê±°ì•¼?   \n",
                            "\n",
                            "                                  reference_contexts  \\\n",
                            "0  [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "1  [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "2  [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "3  [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "4  [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "\n",
                            "                                           reference  \\\n",
                            "0  **í•µì‹¬ ë‹µë³€**: ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±...   \n",
                            "1  **í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ì›ë³¸ ë°ì´í„°ì—ì„œ...   \n",
                            "2  **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•...   \n",
                            "3  ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ...   \n",
                            "4  ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°ì´í„°...   \n",
                            "\n",
                            "               persona_name      query_style query_length  \\\n",
                            "0  Deep Learning Researcher       MISSPELLED         LONG   \n",
                            "1  Deep Learning Researcher  WEB_SEARCH_LIKE        SHORT   \n",
                            "2  Deep Learning Researcher  PERFECT_GRAMMAR       MEDIUM   \n",
                            "3            Data Scientist     POOR_GRAMMAR         LONG   \n",
                            "4            Data Scientist     POOR_GRAMMAR        SHORT   \n",
                            "\n",
                            "                        synthesizer_name  \n",
                            "0  single_hop_specific_query_synthesizer  \n",
                            "1  single_hop_specific_query_synthesizer  \n",
                            "2  single_hop_specific_query_synthesizer  \n",
                            "3  single_hop_specific_query_synthesizer  \n",
                            "4  single_hop_specific_query_synthesizer  "
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "eval_df = testset.to_pandas()\n",
                "\n",
                "# Reference ë°ì´í„° ì¬ìƒì„± (LangGraph ì ìš©)\n",
                "from src.agent.workflow import build_reference_generation_graph\n",
                "from langchain_core.messages import HumanMessage\n",
                "\n",
                "# Graph ë¹Œë“œ\n",
                "ref_graph = build_reference_generation_graph()\n",
                "app = ref_graph.compile()\n",
                "\n",
                "print(\"Reference ë°ì´í„° ì¬ìƒì„± ì¤‘...\")\n",
                "new_references = []\n",
                "for idx, row in eval_df.iterrows():\n",
                "    if isinstance(row['reference_contexts'], list):\n",
                "        context_str = \"\\n\\n\".join(row['reference_contexts'])\n",
                "    else:\n",
                "        context_str = str(row['reference_contexts'])\n",
                "    \n",
                "    query = row['user_input']\n",
                "    \n",
                "    # Generate\n",
                "    try:\n",
                "        # LangGraph ì‹¤í–‰\n",
                "        result = app.invoke({\"context\": context_str, \"query\": query})\n",
                "        # ê²°ê³¼ ì¶”ì¶œ\n",
                "        if result.get(\"analyst_results\"):\n",
                "             new_ref = result[\"analyst_results\"][-1].content\n",
                "        else:\n",
                "             new_ref = \"\"\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating reference for query {query}: {e}\")\n",
                "        new_ref = row.get('reference', '') # Fallback to original\n",
                "        \n",
                "    new_references.append(new_ref)\n",
                "\n",
                "eval_df['reference'] = new_references\n",
                "\n",
                "# ë‹¤ìŒ ì…€ í˜¸í™˜ì„±ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
                "eval_data = eval_df.rename(columns={'user_input': 'question'}).to_dict('list')\n",
                "\n",
                "eval_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ì¤‘...\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 04_ë°ì´í„°_ì „ì²˜ë¦¬]\n",
                        "\n",
                        "Data ì „ì²˜ë¦¬(Data Preprocessing)ë€  \n",
                        "- ë°ì´í„° ë¶„ì„ì´ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë°ì´í„°ì…‹ì„ ë³€í™˜ ë˜ëŠ” ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ ë§í•œë‹¤.\n",
                        "- ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì „ì— ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì´ë‹¤.\n",
                        "- Garbage in, Garbage out.\n",
                        "- ì¢‹ì€ datasetìœ¼ë¡œ í•™ìŠµ í•´ì•¼ ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¢‹ì€ train datasetì„ ë§Œë“œëŠ” ê²ƒì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤€ë‹¤.\n",
                        "- Data ì „ì²˜ë¦¬ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì´ ìˆë‹¤.\n",
                        "- **Data Cleaning (ë°ì´í„° ì •ì œ)**\n",
                        "- ë°ì´í„°ì…‹ì— ìˆëŠ” ì˜¤ë¥˜ê°’, ë¶ˆí•„ìš”í•œ ê°’, ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’ ë“±ì„ ì œê±°í•˜ëŠ” ì‘ì—…\n",
                        "- **ì»¬ëŸ¼ ì„ íƒ ë° íŒŒìƒë³€ìˆ˜ ìƒì„±**\n",
                        "- ì»¬ëŸ¼ë“¤ ì¤‘ ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•˜ê±°ë‚˜ ê¸°ì¡´ ì»¬ëŸ¼ë“¤ì„ ê³„ì‚°í•œ ê²°ê³¼ê°’ì„ ê°€ì§€ëŠ” íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•œë‹¤.\n",
                        "- **Featureì˜ ë°ì´í„° íƒ€ì… ë³„ ë³€í™˜**\n",
                        "- ë¬¸ìì—´ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜, ë²”ì£¼í˜•ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ë“±ê³¼ ê°™ì´ ì›ë˜ ë°ì´í„°ì˜ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•˜ëŠ” ì‘ì—….\n",
                        "- **ìˆ˜ì¹˜í˜• ë°ì´í„° Feature Scaling**\n",
                        "- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì˜ scale(ì²™ë„) ë¥¼ ë§ì¶° ì£¼ëŠ” ì‘ì—….\n",
                        "- **ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©**\n",
                        "- ë¬¸ìì—´ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€ê²½í•˜ëŠ” ì‘ì—….' metadata={'source': '', 'source_file': '04_ë°ì´í„°_ì „ì²˜ë¦¬.ipynb', 'lecture_title': '04_ë°ì´í„°_ì „ì²˜ë¦¬', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.5783624}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5783624\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "3. Data Preparation\n",
                        "- ë°ì´í„° ì „ì²˜ë¦¬' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 22, 'code_snippet': '', 'chunk_index': 18, 'original_score': 0.6322969681358943}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6322969681358943\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 04_ë°ì´í„°_ì „ì²˜ë¦¬]\n",
                        "\n",
                        "ê²°ì¸¡ì¹˜(Missing Value) ì²˜ë¦¬  \n",
                        "- ê²°ì¸¡ì¹˜(Missing Value)\n",
                        "- ìˆ˜ì§‘í•˜ì§€ ëª»í•œ ê°’. ëª¨ë¥´ëŠ” ê°’. ì—†ëŠ” ê°’\n",
                        "- ê²°ì¸¡ì¹˜ ê°’ì€ `NA, NaN, None, null` ë¡œ í‘œí˜„í•œë‹¤. (ì–¸ì–´ë§ˆë‹¤ ì°¨ì´ê°€ ìˆë‹¤.)\n",
                        "- ê²°ì¸¡ì¹˜ëŠ” ë°ì´í„° ë¶„ì„ì´ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì „ì˜ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì²˜ë¦¬í•´ì¤˜ì•¼ í•œë‹¤.' metadata={'source': '', 'source_file': '04_ë°ì´í„°_ì „ì²˜ë¦¬.ipynb', 'lecture_title': '04_ë°ì´í„°_ì „ì²˜ë¦¬', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5631255621784523}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5631255621784523\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 04_ë°ì´í„°_ì „ì²˜ë¦¬]\n",
                        "\n",
                        "Data ì „ì²˜ë¦¬(Data Preprocessing)ë€  \n",
                        "- ë°ì´í„° ë¶„ì„ì´ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë°ì´í„°ì…‹ì„ ë³€í™˜ ë˜ëŠ” ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ ë§í•œë‹¤.\n",
                        "- ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì „ì— ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì´ë‹¤.\n",
                        "- Garbage in, Garbage out.\n",
                        "- ì¢‹ì€ datasetìœ¼ë¡œ í•™ìŠµ í•´ì•¼ ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¢‹ì€ train datasetì„ ë§Œë“œëŠ” ê²ƒì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤€ë‹¤.\n",
                        "- Data ì „ì²˜ë¦¬ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì´ ìˆë‹¤.\n",
                        "- **Data Cleaning (ë°ì´í„° ì •ì œ)**\n",
                        "- ë°ì´í„°ì…‹ì— ìˆëŠ” ì˜¤ë¥˜ê°’, ë¶ˆí•„ìš”í•œ ê°’, ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’ ë“±ì„ ì œê±°í•˜ëŠ” ì‘ì—…\n",
                        "- **ì»¬ëŸ¼ ì„ íƒ ë° íŒŒìƒë³€ìˆ˜ ìƒì„±**\n",
                        "- ì»¬ëŸ¼ë“¤ ì¤‘ ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•˜ê±°ë‚˜ ê¸°ì¡´ ì»¬ëŸ¼ë“¤ì„ ê³„ì‚°í•œ ê²°ê³¼ê°’ì„ ê°€ì§€ëŠ” íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•œë‹¤.\n",
                        "- **Featureì˜ ë°ì´í„° íƒ€ì… ë³„ ë³€í™˜**\n",
                        "- ë¬¸ìì—´ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜, ë²”ì£¼í˜•ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ë“±ê³¼ ê°™ì´ ì›ë˜ ë°ì´í„°ì˜ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•˜ëŠ” ì‘ì—….\n",
                        "- **ìˆ˜ì¹˜í˜• ë°ì´í„° Feature Scaling**\n",
                        "- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì˜ scale(ì²™ë„) ë¥¼ ë§ì¶° ì£¼ëŠ” ì‘ì—….\n",
                        "- **ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©**\n",
                        "- ë¬¸ìì—´ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€ê²½í•˜ëŠ” ì‘ì—….' metadata={'source': '', 'source_file': '04_ë°ì´í„°_ì „ì²˜ë¦¬.ipynb', 'lecture_title': '04_ë°ì´í„°_ì „ì²˜ë¦¬', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.57833804}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.57833804\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "3. Data Preparation\n",
                        "- ë°ì´í„° ì „ì²˜ë¦¬' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 22, 'code_snippet': '', 'chunk_index': 18, 'original_score': 0.6322331641358944}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6322331641358944\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 04_ë°ì´í„°_ì „ì²˜ë¦¬]\n",
                        "\n",
                        "ê²°ì¸¡ì¹˜(Missing Value) ì²˜ë¦¬  \n",
                        "- ê²°ì¸¡ì¹˜(Missing Value)\n",
                        "- ìˆ˜ì§‘í•˜ì§€ ëª»í•œ ê°’. ëª¨ë¥´ëŠ” ê°’. ì—†ëŠ” ê°’\n",
                        "- ê²°ì¸¡ì¹˜ ê°’ì€ `NA, NaN, None, null` ë¡œ í‘œí˜„í•œë‹¤. (ì–¸ì–´ë§ˆë‹¤ ì°¨ì´ê°€ ìˆë‹¤.)\n",
                        "- ê²°ì¸¡ì¹˜ëŠ” ë°ì´í„° ë¶„ì„ì´ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì „ì˜ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì²˜ë¦¬í•´ì¤˜ì•¼ í•œë‹¤.' metadata={'source': '', 'source_file': '04_ë°ì´í„°_ì „ì²˜ë¦¬.ipynb', 'lecture_title': '04_ë°ì´í„°_ì „ì²˜ë¦¬', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5630921781784524}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5630921781784524\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\n",
                        "- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\n",
                        "- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\n",
                        "- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\n",
                        "- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\n",
                        "- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.64501444}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.64501444\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 09_CNN_ê°œìš”]\n",
                        "\n",
                        "Feature(íŠ¹ì„±) ì¶”ì¶œê³¼ í•©ì„±ê³±\n",
                        "- Filter(Kernel)\n",
                        "- ì´ë¯¸ì§€ì™€ í•©ì„±ê³± ì—°ì‚°ì„ í†µí•´ Feature(íŒ¨í„´)ë“¤ì„ ì¶”ì¶œí•œë‹¤.' metadata={'source': '', 'source_file': '09_CNN_ê°œìš”.ipynb', 'lecture_title': '09_CNN_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 30, 'code_snippet': '', 'chunk_index': 19, 'original_score': 0.611220679578728}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.611220679578728\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 09_CNN_ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ì˜ ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´  \n",
                        "Handcrafted Feature (ì „í†µì ì¸ ì˜ìƒì²˜ë¦¬ ë°©ì‹)\n",
                        "- ë¶„ë¥˜í•˜ë ¤ê³  í•˜ëŠ” ì´ë¯¸ì§€ì˜ íŠ¹ì§•ë“¤ì„ ì‚¬ëŒì´ ì§ì ‘ ì°¾ì•„ì„œ ë§Œë“ ë‹¤. (Feature Exctraction)\n",
                        "- ê·¸ ì°¾ì•„ë‚¸ íŠ¹ì§•ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•´ Filter í–‰ë ¬ì„ ì£¼ë¡œ ì´ìš©í•¨\n",
                        "- ë¯¸ì²˜ ë°œê²¬í•˜ì§€ ëª»í•œ íŠ¹ì§•ì„ ê°€ì§„ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œëŠ” ë¶„ë¥˜ë¥¼ í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.\n",
                        "- ë‹¤ì–‘í•œ ë§ì€ ëŒ€ìƒë“¤ì— ëŒ€í•´ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì„ ì‚¬ëŒì´ ë§Œë“œëŠ” ê²ƒì´ ì–´ë µë‹¤.  \n",
                        "End to End learning (ë”¥ëŸ¬ë‹)\n",
                        "- ì´ë¯¸ì§€ì˜ **íŠ¹ì§• ì¶”ì¶œ**ë¶€í„° ì¶”ë¡ ê¹Œì§€ ìë™ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.' metadata={'source': '', 'source_file': '09_CNN_ê°œìš”.ipynb', 'lecture_title': '09_CNN_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 17, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.5590613759564861}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5590613759564861\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\n",
                        "- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\n",
                        "- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\n",
                        "- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\n",
                        "- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\n",
                        "- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.64501444}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.64501444\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 09_CNN_ê°œìš”]\n",
                        "\n",
                        "Feature(íŠ¹ì„±) ì¶”ì¶œê³¼ í•©ì„±ê³±\n",
                        "- Filter(Kernel)\n",
                        "- ì´ë¯¸ì§€ì™€ í•©ì„±ê³± ì—°ì‚°ì„ í†µí•´ Feature(íŒ¨í„´)ë“¤ì„ ì¶”ì¶œí•œë‹¤.' metadata={'source': '', 'source_file': '09_CNN_ê°œìš”.ipynb', 'lecture_title': '09_CNN_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 30, 'code_snippet': '', 'chunk_index': 19, 'original_score': 0.611220679578728}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.611220679578728\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 09_CNN_ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ì˜ ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´  \n",
                        "Handcrafted Feature (ì „í†µì ì¸ ì˜ìƒì²˜ë¦¬ ë°©ì‹)\n",
                        "- ë¶„ë¥˜í•˜ë ¤ê³  í•˜ëŠ” ì´ë¯¸ì§€ì˜ íŠ¹ì§•ë“¤ì„ ì‚¬ëŒì´ ì§ì ‘ ì°¾ì•„ì„œ ë§Œë“ ë‹¤. (Feature Exctraction)\n",
                        "- ê·¸ ì°¾ì•„ë‚¸ íŠ¹ì§•ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•´ Filter í–‰ë ¬ì„ ì£¼ë¡œ ì´ìš©í•¨\n",
                        "- ë¯¸ì²˜ ë°œê²¬í•˜ì§€ ëª»í•œ íŠ¹ì§•ì„ ê°€ì§„ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œëŠ” ë¶„ë¥˜ë¥¼ í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.\n",
                        "- ë‹¤ì–‘í•œ ë§ì€ ëŒ€ìƒë“¤ì— ëŒ€í•´ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì„ ì‚¬ëŒì´ ë§Œë“œëŠ” ê²ƒì´ ì–´ë µë‹¤.  \n",
                        "End to End learning (ë”¥ëŸ¬ë‹)\n",
                        "- ì´ë¯¸ì§€ì˜ **íŠ¹ì§• ì¶”ì¶œ**ë¶€í„° ì¶”ë¡ ê¹Œì§€ ìë™ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.' metadata={'source': '', 'source_file': '09_CNN_ê°œìš”.ipynb', 'lecture_title': '09_CNN_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 17, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.5590613759564861}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5590613759564861\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹  \n",
                        "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)\n",
                        "- ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µ ì§€ëŠ¥ ë¶„ì•¼\n",
                        "- ê¸°ê³„ì—ê²Œ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ì¼ì¼ì´ ì½”ë“œë¡œ ëª…ì‹œí•˜ì§€ ì•Šê³  ë°ì´í„°ë¥¼ ì´ìš©í•´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ìˆ ì„ ê°œë°œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œë¶„ì•¼  \n",
                        "ë”¥ëŸ¬ë‹ (Deep Learning)\n",
                        "- ì¸ê³µì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼. ë¹„ì •í˜•ë°ì´í„°(ì˜ìƒ, ìŒì„±, í…ìŠ¤íŠ¸)ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¨ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì•¼ í•œë‹¤.  \n",
                        "> - ë¹„ì •í˜• ë°ì´í„°\n",
                        "> - ì •í•´ì§„ ê·œì¹™ ì—†ì´ ì €ì¥ë˜ì–´ ê°’ì˜ ì˜ë¯¸ë‚˜ íŠ¹ì„±ì„ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°\n",
                        "> - í…ìŠ¤íŠ¸, ì˜ìƒ, ìŒì„± ë°ì´í„°ê°€ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤.\n",
                        "> - ì •í˜• ë°ì´í„°\n",
                        "> - í‘œ(table)í˜•íƒœì˜ ë¯¸ë¦¬ ì •í•´ ë†“ì€ í˜•ì‹ê³¼ êµ¬ì¡°ì— ë”°ë¼ ì €ì¥ë˜ë„ë¡ êµ¬ì„±ëœ ë°ì´í„°ë¡œ ê·¸ ì˜ë¯¸ë‚˜ íŠ¹ì„±íŒŒì•…ì´ ìš©ì´í•˜ë‹¤.\n",
                        "> - ëŒ€í‘œì ì´ ì˜ˆë¡œ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.7467641168376767}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7467641168376767\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ (Deep Learning)\n",
                        "- ì¸ê³µì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼. **ë¹„ì •í˜•ë°ì´í„°(ì˜ìƒ, ìŒì„±, í…ìŠ¤íŠ¸)ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥**ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¨ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì•¼ í•œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.6812222271629755}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6812222271629755\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´  \n",
                        "- ê¸°ì¡´ í”„ë¡œê·¸ë˜ë°ë°©ì‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜(ê·œì¹™)ì„ **ì‚¬ëŒì´** ìƒê°í•œë‹¤.\n",
                        "- ë¨¸ì‹ ëŸ¬ë‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜ì„ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ì°¾ì•„ë‚¸ë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.699795298}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.699795298\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹  \n",
                        "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)\n",
                        "- ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µ ì§€ëŠ¥ ë¶„ì•¼\n",
                        "- ê¸°ê³„ì—ê²Œ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ì¼ì¼ì´ ì½”ë“œë¡œ ëª…ì‹œí•˜ì§€ ì•Šê³  ë°ì´í„°ë¥¼ ì´ìš©í•´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ìˆ ì„ ê°œë°œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œë¶„ì•¼  \n",
                        "ë”¥ëŸ¬ë‹ (Deep Learning)\n",
                        "- ì¸ê³µì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼. ë¹„ì •í˜•ë°ì´í„°(ì˜ìƒ, ìŒì„±, í…ìŠ¤íŠ¸)ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¨ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì•¼ í•œë‹¤.  \n",
                        "> - ë¹„ì •í˜• ë°ì´í„°\n",
                        "> - ì •í•´ì§„ ê·œì¹™ ì—†ì´ ì €ì¥ë˜ì–´ ê°’ì˜ ì˜ë¯¸ë‚˜ íŠ¹ì„±ì„ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°\n",
                        "> - í…ìŠ¤íŠ¸, ì˜ìƒ, ìŒì„± ë°ì´í„°ê°€ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤.\n",
                        "> - ì •í˜• ë°ì´í„°\n",
                        "> - í‘œ(table)í˜•íƒœì˜ ë¯¸ë¦¬ ì •í•´ ë†“ì€ í˜•ì‹ê³¼ êµ¬ì¡°ì— ë”°ë¼ ì €ì¥ë˜ë„ë¡ êµ¬ì„±ëœ ë°ì´í„°ë¡œ ê·¸ ì˜ë¯¸ë‚˜ íŠ¹ì„±íŒŒì•…ì´ ìš©ì´í•˜ë‹¤.\n",
                        "> - ëŒ€í‘œì ì´ ì˜ˆë¡œ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.7467641168376767}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7467641168376767\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ (Deep Learning)\n",
                        "- ì¸ê³µì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼. **ë¹„ì •í˜•ë°ì´í„°(ì˜ìƒ, ìŒì„±, í…ìŠ¤íŠ¸)ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥**ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¨ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì•¼ í•œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.6812222271629755}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6812222271629755\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´  \n",
                        "- ê¸°ì¡´ í”„ë¡œê·¸ë˜ë°ë°©ì‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜(ê·œì¹™)ì„ **ì‚¬ëŒì´** ìƒê°í•œë‹¤.\n",
                        "- ë¨¸ì‹ ëŸ¬ë‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜ì„ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ì°¾ì•„ë‚¸ë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.699795298}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.699795298\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\n",
                        "\n",
                        "ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \n",
                        "- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\n",
                        "1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\n",
                        "- output layer: units=1, activation='sigmoid'\n",
                        "- loss: binary_crossentropy\n",
                        "2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\n",
                        "- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\n",
                        "- loss: categorical_crossentropy\n",
                        "- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\n",
                        "- Feature\n",
                        "- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\n",
                        "- Targetì˜ class\n",
                        "- 0 - malignant(ì•…ì„±ì¢…ì–‘)\n",
                        "- 1 - benign(ì–‘ì„±ì¢…ì–‘)' metadata={'source': '', 'source_file': '07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±.ipynb', 'lecture_title': '07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±', 'cell_type': 'markdown', 'cell_index': 69, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, TensorDataset\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nprint(device)\\n```\\n\\n```python\\n# Dataset\\nX, y = load_breast_cancer(return_X_y=True)\\ny = y.reshape(-1, 1)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)\\n```\\n\\n```python\\n# ì „ì²˜ë¦¬\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```\\n\\n```python\\n# Dataset\\n# ëª¨ë¸ì˜ weight, bias -> float32. X, yëŠ” weight, biasì™€ ê³„ì‚°ì„ í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— íƒ€ì…ì„ ë§ì¶°ì¤€ë‹¤.\\ntrainset = TensorDataset(\\n    torch.tensor(X_train_scaled, dtype=torch.float32),  \\n    torch.tensor(y_train, dtype=torch.float32)\\n)\\ntestset = TensorDataset(\\n    torch.tensor(X_test_scaled, dtype=torch.float32), \\n    torch.tensor(y_test, dtype=torch.float32)\\n)\\n```\\n\\n```python\\n# class name <-> class index\\nclasses = np.array([\"ì•…ì„±ì¢…ì–‘\", \"ì–‘ì„±ì¢…ì–‘\"])\\nclass_to_idx = {\"ì•…ì„±ì¢…ì–‘\":0, \"ì–‘ì„±ì¢…ì–‘\":1}\\n\\ntrainset.classes = classes\\ntrainset.class_to_idx = class_to_idx\\n```\\n\\n```python\\ntrainset.classes[[0, 1, 1, 0, 1,  1, 0, 1, 1]]\\n```\\n\\n```python\\n# DataLoader\\ntrain_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\\ntest_loader = DataLoader(testset, batch_size=100)\\n```\\n\\n```python\\n# ëª¨ë¸ ì •ì˜\\nclass BCModel(nn.Module):\\n\\n    def __init__(self):\\n        super().__init__()\\n        self.lr1 = nn.Linear(30, 16)\\n        self.lr2 = nn.Linear(16, 8)\\n        self.lr3 = nn.Linear(8, 1)  # out_features: 1 - positiveì¼ í™•ë¥ \\n        self.relu = nn.ReLU() # hidden layerì˜ í™œì„±í•¨ìˆ˜\\n        self.sigmoid = nn.Sigmoid()  # Linear ì¶œë ¥ê°’ì„ 0 ~ 1 í™•ë¥ ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” Sigmoid(Logistic)í•¨ìˆ˜.\\n    \\n    def forward(self, X):\\n        out = self.relu(self.lr1(X))\\n        out = self.relu(self.lr2(out))\\n\\n        out = self.lr3(out)\\n        out = self.sigmoid(out)\\n        return out\\n```\\n\\n```python\\nfrom torchinfo import summary\\nm = BCModel()\\nsummary(m)\\n# p = m(torch.randn(10, 30))\\n# p\\n```\\n\\n```python\\n# í•™ìŠµ\\ntrain_loss_list = []\\nvalid_loss_list = []\\nvalid_acc_list = []\\n\\nepochs = 1000\\n\\nmodel = BCModel().to(device)\\nloss_fn = nn.BCELoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.01)\\n\\n# ì¡°ê¸°ì¢…ë£Œ, best ëª¨ë¸ ì €ì¥\\nbest_score = torch.inf  # valid loss ê¸°ì¤€\\nsave_model_path = \"saved_models/bc_model.pth\"\\npatience = 10\\nstop_count = 0\\n```\\n\\n```python\\nfor epoch in range(epochs):\\n    # í•™ìŠµ\\n    model.train()\\n    train_loss = 0.0\\n    for X_train, y_train in train_loader:\\n        # 1. deviceë¡œ ì´ë™\\n        X_train, y_train = X_train.to(device), y_train.to(device)\\n        # 2. ì¶”ë¡ \\n        pred_train = model(X_train)\\n        # 3. loss\\n        loss = loss_fn(pred_train, y_train)\\n        # 4. gradientê³„ì‚°\\n        loss.backward()\\n        # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\\n        optimizer.step()\\n        # 6. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\\n        optimizer.zero_grad()\\n        # loss ëˆ„ì \\n        train_loss += loss.item()\\n    \\n    # train loss í‰ê· \\n    train_loss /= len(train_loader)\\n    train_loss_list.append(train_loss)\\n        \\n    # ê²€ì¦\\n    model.eval()\\n    valid_loss = 0\\n    valid_acc = 0\\n    with torch.no_grad():\\n        for X_valid, y_valid in test_loader:\\n            # 1. device ì´ë™\\n            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\\n            # 2. ì¶”ë¡ \\n            pred_valid = model(X_valid) # (batch, 1) 1ì¶•: positiveì¼ í™•ë¥ \\n            pred_label = (pred_valid > 0.5).type(torch.int32) # label\\n            # 3. í‰ê°€ - loss, acc\\n            valid_loss += loss_fn(pred_valid, y_valid).item()\\n            valid_acc += torch.sum(pred_label == y_valid).item()\\n        \\n        valid_loss /= len(test_loader)\\n        valid_acc /= len(testset)\\n        valid_loss_list.append(valid_loss)\\n        valid_acc_list.append(valid_acc)\\n\\n        print(f\"[{epoch+1}/{epochs}] train loss: {train_loss:.5f}, valid loss{valid_loss:.5f}, valid acc: {valid_acc:.5f}\")\\n\\n        # ì„±ëŠ¥ ê°œì„ ì‹œ ëª¨ë¸ ì €ì¥ + ì¡°ê¸°ì¢…ë£Œ\\n        if valid_loss < best_score: # ê°œì„ ë¨.\\n            # ëª¨ë¸ ì €ì¥ + stop_count ì´ˆê¸°í™”\\n            print(f\">>>>>>>> {epoch+1} Epochì—ì„œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤. {best_score}ì—ì„œ {valid_loss}ë¡œ ê°œì„ ë¨\")        \\n            torch.save(model, save_model_path)\\n            best_score = valid_loss\\n            stop_count = 0\\n        else: # ê°œì„ ì•ˆë¨\\n            stop_count += 1\\n            if patience == stop_count:\\n                print(f\"{epoch+1}ì—ì„œ ì¡°ê¸°ì¢…ë£Œ í•©ë‹ˆë‹¤. {best_score}ì—ì„œ ê°œì„ ì´ ì•ˆë¨.\")\\n                break\\n```\\n\\n```python\\n# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\\nload_bc_model = torch.load(save_model_path, weights_only=False)\\nload_bc_model\\n```\\n\\n```python\\n# ì¶”ë¡  í•¨ìˆ˜ #######\\ndef predict_bc(model, X, device=\"cpu\"):\\n    # modelë¡œ Xë¥¼ ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ë°˜í™˜\\n    # label, í™•ë¥ \\n    result = []\\n    with torch.no_grad():\\n        pred_proba = model(X) # POS í™•ë¥ \\n        pred_class = (pred_proba > 0.5).type(torch.int32)\\n        for class_index, proba in zip(pred_class, pred_proba):\\n            # print(class_index, proba if class_index.item() == 1 else 1-proba)\\n            result.append((class_index.item(), proba.item() if class_index.item() == 1 else 1-proba.item()))\\n        return result\\n```\\n\\n```python\\n@torch.no_grad\\ndef predict_bc(model, X, device=\"cpu\"):\\n    # modelë¡œ Xë¥¼ ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ë°˜í™˜\\n    # label, í™•ë¥ \\n    result = []\\n    \\n    pred_proba = model(X) # POS í™•ë¥ \\n    pred_class = (pred_proba > 0.5).type(torch.int32)\\n    for class_index, proba in zip(pred_class, pred_proba):\\n        # print(class_index, proba if class_index.item() == 1 else 1-proba)\\n        result.append((class_index.item(), proba.item() if class_index.item() == 1 else 1-proba.item()))\\n    return result\\n```\\n\\n```python\\nnew_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\\n```\\n\\n```python\\nresult = predict_bc(load_bc_model, new_data)\\nresult\\n```\\n\\n```python\\nmodel = BCModel().to(device)\\nloss_fn = nn.BCELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\\n```\\n\\n```python\\nfrom module.train import fit\\n\\nresult = fit(\\n    train_loader, test_loader, model, loss_fn, optimizer, \\n    epochs=1000, save_best_model=True, save_model_path=\"saved_models/bc_model2.pth\", \\n    device=device\\n)\\n```', 'chunk_index': 7, 'original_score': 0.5544761460824708}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5544761460824708\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- ### Binary classification (ì´ì§„ ë¶„ë¥˜)\n",
                        "- íŠ¹ì • í´ë˜ìŠ¤ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì¶”ë¡ í•˜ëŠ” ë¬¸ì œ.\n",
                        "- ëª¨ë¸ì€ Positive(ì–‘ì„±-1)ì˜ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.\n",
                        "- ëª¨ë¸ ì¶œë ¥ ê°’ì„ Threshold(ë³´í†µ 0.5)ì™€ ë¹„êµí•´ ì‘ìœ¼ë©´ 0, í¬ë©´ 1ë¡œ í›„ì²˜ë¦¬ë¥¼ í†µí•´ Labelì„ ê³„ì‚°í•œë‹¤.\n",
                        "- ex) í™˜ìì¸ì§€ ì—¬ë¶€, ìŠ¤íŒ¸ë©”ì¼ì¸ì§€ ì—¬ë¶€\n",
                        "- Output Layerì˜ unit ê°œìˆ˜ë¥¼ 1ë¡œ í•˜ê³  activation í•¨ìˆ˜ë¡œ sigmoidë¥¼ ì‚¬ìš©í•˜ì—¬ positive(1)ì˜ í™•ë¥ ( 0 ~ 1ì‚¬ì´ ê°’)ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë„ë¡ ëª¨ë¸ì„ ì •ì˜ í•œë‹¤.\n",
                        "ì´ë•Œ Lossí•¨ìˆ˜ëŠ” **binary_crossentropy**ë¥¼ loss functionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\n",
                        "- **nn.BCELoss()** loss functionì„ ì‚¬ìš©\n",
                        "\\begin{align}\n",
                        "&\\large Loss(\\hat y^{(i)} ,y^{(i)}) = - y^{(i)} log(\\hat y^{(i)} ) - (1- y^{(i)}) log (1-\\hat y^{(i)} ) \\\\\\\\\n",
                        "&y^{(i)}: \\text{ì‹¤ì œ ê°’(Ground Truth)}, \\hat y^{(i)}: \\text{ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ (Positiveì¼ í™•ë¥ )}\n",
                        "\\end{align}' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 22, 'code_snippet': '', 'chunk_index': 16, 'original_score': 0.4764769266666667}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4764769266666667\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ë¶„ë¥˜(Classification) í‰ê°€ ì§€í‘œ  \n",
                        "ì´ì§„ ë¶„ë¥˜(Binary classification)\n",
                        "- **íŠ¹ì • í´ë˜ìŠ¤ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë¶„ë¥˜í•œë‹¤.**\n",
                        "- í™˜ìì¸ê°€?\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ì¸ê°€?\n",
                        "- ì‚¬ê¸° ê±°ë˜ ì¸ê°€?\n",
                        "- ì´ì§„ ë¶„ë¥˜ ì–‘ì„±(Positive)ê³¼ ìŒì„±(Negative)\n",
                        "- **ì–‘ì„±(Positive):** ì°¾ìœ¼ë ¤ëŠ” ëŒ€ìƒì´ Trueì´ì¸ ê²ƒ. ë³´í†µ 1ë¡œ í‘œí˜„í•œë‹¤.\n",
                        "- **ìŒì„±(Negative):** ì°¾ìœ¼ë ¤ëŠ” ëŒ€ìƒì´ Falseì´ì¸ ê²ƒ. ë³´í†µ 0ë¡œ í‘œí˜„í•œë‹¤.\n",
                        "- ì˜ˆ\n",
                        "- í™˜ìì¸ê°€? (ê²€ì‚¬ê¸°ë¡ì„ í†µí•´ í™˜ìë¥¼ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°)\n",
                        "- ì–‘ì„±(Positive): í™˜ì, 1\n",
                        "- ìŒì„±(Negative): í™˜ì ì•„ë‹˜(ì •ìƒ), 0\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ì¸ê°€? (ë©”ì¼ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íŒ¸ë©”ì¼ì„ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°.)\n",
                        "- ì–‘ì„±(Positive): ìŠ¤íŒ¸ë©”ì¼, 1\n",
                        "- ìŒì„±(Negative): ìŠ¤íŒ¸ë©”ì¼ ì•„ë‹˜(ì •ìƒ ë©”ì¼), 0\n",
                        "- ì‚¬ê¸° ê±°ë˜ ì¸ê°€? (ê¸ˆìœµê±°ë˜ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ê¸ˆìœµì‚¬ê¸° ê±°ë˜ë¥¼ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°.)\n",
                        "- ì–‘ì„±(Positive): ì‚¬ê¸° ê±°ë˜, 1\n",
                        "- ìŒì„±(Negative): ì‚¬ê¸° ê±°ë˜ ì•„ë‹˜(ì •ìƒ ê±°ë˜), 0' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.3887022152745078}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3887022152745078\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\n",
                        "\n",
                        "ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ - **ì´ì§„ë¶„ë¥˜(Binary Classification) ë¬¸ì œ**  \n",
                        "- **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì²˜ë¦¬ ëª¨ë¸ì˜ ë‘ê°€ì§€ ë°©ë²•**\n",
                        "1. positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„\n",
                        "- output layer: units=1, activation='sigmoid'\n",
                        "- loss: binary_crossentropy\n",
                        "2. negative(0)ì¼ í™•ë¥ ê³¼ positive(1)ì¼ í™•ë¥ ì„ ì¶œë ¥í•˜ë„ë¡ êµ¬í˜„ => ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°\n",
                        "- output layer: units=2, activation='softmax', y(ì •ë‹µ)ì€ one hot encoding ì²˜ë¦¬\n",
                        "- loss: categorical_crossentropy\n",
                        "- ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ì¢…ì–‘ì˜ ì•…ì„±/ì–‘ì„±ì—¬ë¶€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹\n",
                        "- Feature\n",
                        "- ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •ê°’ë“¤\n",
                        "- Targetì˜ class\n",
                        "- 0 - malignant(ì•…ì„±ì¢…ì–‘)\n",
                        "- 1 - benign(ì–‘ì„±ì¢…ì–‘)' metadata={'source': '', 'source_file': '07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±.ipynb', 'lecture_title': '07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±', 'cell_type': 'markdown', 'cell_index': 69, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, TensorDataset\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nprint(device)\\n```\\n\\n```python\\n# Dataset\\nX, y = load_breast_cancer(return_X_y=True)\\ny = y.reshape(-1, 1)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)\\n```\\n\\n```python\\n# ì „ì²˜ë¦¬\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```\\n\\n```python\\n# Dataset\\n# ëª¨ë¸ì˜ weight, bias -> float32. X, yëŠ” weight, biasì™€ ê³„ì‚°ì„ í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— íƒ€ì…ì„ ë§ì¶°ì¤€ë‹¤.\\ntrainset = TensorDataset(\\n    torch.tensor(X_train_scaled, dtype=torch.float32),  \\n    torch.tensor(y_train, dtype=torch.float32)\\n)\\ntestset = TensorDataset(\\n    torch.tensor(X_test_scaled, dtype=torch.float32), \\n    torch.tensor(y_test, dtype=torch.float32)\\n)\\n```\\n\\n```python\\n# class name <-> class index\\nclasses = np.array([\"ì•…ì„±ì¢…ì–‘\", \"ì–‘ì„±ì¢…ì–‘\"])\\nclass_to_idx = {\"ì•…ì„±ì¢…ì–‘\":0, \"ì–‘ì„±ì¢…ì–‘\":1}\\n\\ntrainset.classes = classes\\ntrainset.class_to_idx = class_to_idx\\n```\\n\\n```python\\ntrainset.classes[[0, 1, 1, 0, 1,  1, 0, 1, 1]]\\n```\\n\\n```python\\n# DataLoader\\ntrain_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\\ntest_loader = DataLoader(testset, batch_size=100)\\n```\\n\\n```python\\n# ëª¨ë¸ ì •ì˜\\nclass BCModel(nn.Module):\\n\\n    def __init__(self):\\n        super().__init__()\\n        self.lr1 = nn.Linear(30, 16)\\n        self.lr2 = nn.Linear(16, 8)\\n        self.lr3 = nn.Linear(8, 1)  # out_features: 1 - positiveì¼ í™•ë¥ \\n        self.relu = nn.ReLU() # hidden layerì˜ í™œì„±í•¨ìˆ˜\\n        self.sigmoid = nn.Sigmoid()  # Linear ì¶œë ¥ê°’ì„ 0 ~ 1 í™•ë¥ ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” Sigmoid(Logistic)í•¨ìˆ˜.\\n    \\n    def forward(self, X):\\n        out = self.relu(self.lr1(X))\\n        out = self.relu(self.lr2(out))\\n\\n        out = self.lr3(out)\\n        out = self.sigmoid(out)\\n        return out\\n```\\n\\n```python\\nfrom torchinfo import summary\\nm = BCModel()\\nsummary(m)\\n# p = m(torch.randn(10, 30))\\n# p\\n```\\n\\n```python\\n# í•™ìŠµ\\ntrain_loss_list = []\\nvalid_loss_list = []\\nvalid_acc_list = []\\n\\nepochs = 1000\\n\\nmodel = BCModel().to(device)\\nloss_fn = nn.BCELoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.01)\\n\\n# ì¡°ê¸°ì¢…ë£Œ, best ëª¨ë¸ ì €ì¥\\nbest_score = torch.inf  # valid loss ê¸°ì¤€\\nsave_model_path = \"saved_models/bc_model.pth\"\\npatience = 10\\nstop_count = 0\\n```\\n\\n```python\\nfor epoch in range(epochs):\\n    # í•™ìŠµ\\n    model.train()\\n    train_loss = 0.0\\n    for X_train, y_train in train_loader:\\n        # 1. deviceë¡œ ì´ë™\\n        X_train, y_train = X_train.to(device), y_train.to(device)\\n        # 2. ì¶”ë¡ \\n        pred_train = model(X_train)\\n        # 3. loss\\n        loss = loss_fn(pred_train, y_train)\\n        # 4. gradientê³„ì‚°\\n        loss.backward()\\n        # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\\n        optimizer.step()\\n        # 6. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\\n        optimizer.zero_grad()\\n        # loss ëˆ„ì \\n        train_loss += loss.item()\\n    \\n    # train loss í‰ê· \\n    train_loss /= len(train_loader)\\n    train_loss_list.append(train_loss)\\n        \\n    # ê²€ì¦\\n    model.eval()\\n    valid_loss = 0\\n    valid_acc = 0\\n    with torch.no_grad():\\n        for X_valid, y_valid in test_loader:\\n            # 1. device ì´ë™\\n            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\\n            # 2. ì¶”ë¡ \\n            pred_valid = model(X_valid) # (batch, 1) 1ì¶•: positiveì¼ í™•ë¥ \\n            pred_label = (pred_valid > 0.5).type(torch.int32) # label\\n            # 3. í‰ê°€ - loss, acc\\n            valid_loss += loss_fn(pred_valid, y_valid).item()\\n            valid_acc += torch.sum(pred_label == y_valid).item()\\n        \\n        valid_loss /= len(test_loader)\\n        valid_acc /= len(testset)\\n        valid_loss_list.append(valid_loss)\\n        valid_acc_list.append(valid_acc)\\n\\n        print(f\"[{epoch+1}/{epochs}] train loss: {train_loss:.5f}, valid loss{valid_loss:.5f}, valid acc: {valid_acc:.5f}\")\\n\\n        # ì„±ëŠ¥ ê°œì„ ì‹œ ëª¨ë¸ ì €ì¥ + ì¡°ê¸°ì¢…ë£Œ\\n        if valid_loss < best_score: # ê°œì„ ë¨.\\n            # ëª¨ë¸ ì €ì¥ + stop_count ì´ˆê¸°í™”\\n            print(f\">>>>>>>> {epoch+1} Epochì—ì„œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤. {best_score}ì—ì„œ {valid_loss}ë¡œ ê°œì„ ë¨\")        \\n            torch.save(model, save_model_path)\\n            best_score = valid_loss\\n            stop_count = 0\\n        else: # ê°œì„ ì•ˆë¨\\n            stop_count += 1\\n            if patience == stop_count:\\n                print(f\"{epoch+1}ì—ì„œ ì¡°ê¸°ì¢…ë£Œ í•©ë‹ˆë‹¤. {best_score}ì—ì„œ ê°œì„ ì´ ì•ˆë¨.\")\\n                break\\n```\\n\\n```python\\n# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\\nload_bc_model = torch.load(save_model_path, weights_only=False)\\nload_bc_model\\n```\\n\\n```python\\n# ì¶”ë¡  í•¨ìˆ˜ #######\\ndef predict_bc(model, X, device=\"cpu\"):\\n    # modelë¡œ Xë¥¼ ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ë°˜í™˜\\n    # label, í™•ë¥ \\n    result = []\\n    with torch.no_grad():\\n        pred_proba = model(X) # POS í™•ë¥ \\n        pred_class = (pred_proba > 0.5).type(torch.int32)\\n        for class_index, proba in zip(pred_class, pred_proba):\\n            # print(class_index, proba if class_index.item() == 1 else 1-proba)\\n            result.append((class_index.item(), proba.item() if class_index.item() == 1 else 1-proba.item()))\\n        return result\\n```\\n\\n```python\\n@torch.no_grad\\ndef predict_bc(model, X, device=\"cpu\"):\\n    # modelë¡œ Xë¥¼ ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ë°˜í™˜\\n    # label, í™•ë¥ \\n    result = []\\n    \\n    pred_proba = model(X) # POS í™•ë¥ \\n    pred_class = (pred_proba > 0.5).type(torch.int32)\\n    for class_index, proba in zip(pred_class, pred_proba):\\n        # print(class_index, proba if class_index.item() == 1 else 1-proba)\\n        result.append((class_index.item(), proba.item() if class_index.item() == 1 else 1-proba.item()))\\n    return result\\n```\\n\\n```python\\nnew_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\\n```\\n\\n```python\\nresult = predict_bc(load_bc_model, new_data)\\nresult\\n```\\n\\n```python\\nmodel = BCModel().to(device)\\nloss_fn = nn.BCELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\\n```\\n\\n```python\\nfrom module.train import fit\\n\\nresult = fit(\\n    train_loader, test_loader, model, loss_fn, optimizer, \\n    epochs=1000, save_best_model=True, save_model_path=\"saved_models/bc_model2.pth\", \\n    device=device\\n)\\n```', 'chunk_index': 7, 'original_score': 0.5544761460824708}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5544761460824708\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- ### Binary classification (ì´ì§„ ë¶„ë¥˜)\n",
                        "- íŠ¹ì • í´ë˜ìŠ¤ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì¶”ë¡ í•˜ëŠ” ë¬¸ì œ.\n",
                        "- ëª¨ë¸ì€ Positive(ì–‘ì„±-1)ì˜ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.\n",
                        "- ëª¨ë¸ ì¶œë ¥ ê°’ì„ Threshold(ë³´í†µ 0.5)ì™€ ë¹„êµí•´ ì‘ìœ¼ë©´ 0, í¬ë©´ 1ë¡œ í›„ì²˜ë¦¬ë¥¼ í†µí•´ Labelì„ ê³„ì‚°í•œë‹¤.\n",
                        "- ex) í™˜ìì¸ì§€ ì—¬ë¶€, ìŠ¤íŒ¸ë©”ì¼ì¸ì§€ ì—¬ë¶€\n",
                        "- Output Layerì˜ unit ê°œìˆ˜ë¥¼ 1ë¡œ í•˜ê³  activation í•¨ìˆ˜ë¡œ sigmoidë¥¼ ì‚¬ìš©í•˜ì—¬ positive(1)ì˜ í™•ë¥ ( 0 ~ 1ì‚¬ì´ ê°’)ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë„ë¡ ëª¨ë¸ì„ ì •ì˜ í•œë‹¤.\n",
                        "ì´ë•Œ Lossí•¨ìˆ˜ëŠ” **binary_crossentropy**ë¥¼ loss functionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\n",
                        "- **nn.BCELoss()** loss functionì„ ì‚¬ìš©\n",
                        "\\begin{align}\n",
                        "&\\large Loss(\\hat y^{(i)} ,y^{(i)}) = - y^{(i)} log(\\hat y^{(i)} ) - (1- y^{(i)}) log (1-\\hat y^{(i)} ) \\\\\\\\\n",
                        "&y^{(i)}: \\text{ì‹¤ì œ ê°’(Ground Truth)}, \\hat y^{(i)}: \\text{ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ (Positiveì¼ í™•ë¥ )}\n",
                        "\\end{align}' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 22, 'code_snippet': '', 'chunk_index': 16, 'original_score': 0.4764769266666667}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4764769266666667\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ë¶„ë¥˜(Classification) í‰ê°€ ì§€í‘œ  \n",
                        "ì´ì§„ ë¶„ë¥˜(Binary classification)\n",
                        "- **íŠ¹ì • í´ë˜ìŠ¤ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë¶„ë¥˜í•œë‹¤.**\n",
                        "- í™˜ìì¸ê°€?\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ì¸ê°€?\n",
                        "- ì‚¬ê¸° ê±°ë˜ ì¸ê°€?\n",
                        "- ì´ì§„ ë¶„ë¥˜ ì–‘ì„±(Positive)ê³¼ ìŒì„±(Negative)\n",
                        "- **ì–‘ì„±(Positive):** ì°¾ìœ¼ë ¤ëŠ” ëŒ€ìƒì´ Trueì´ì¸ ê²ƒ. ë³´í†µ 1ë¡œ í‘œí˜„í•œë‹¤.\n",
                        "- **ìŒì„±(Negative):** ì°¾ìœ¼ë ¤ëŠ” ëŒ€ìƒì´ Falseì´ì¸ ê²ƒ. ë³´í†µ 0ë¡œ í‘œí˜„í•œë‹¤.\n",
                        "- ì˜ˆ\n",
                        "- í™˜ìì¸ê°€? (ê²€ì‚¬ê¸°ë¡ì„ í†µí•´ í™˜ìë¥¼ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°)\n",
                        "- ì–‘ì„±(Positive): í™˜ì, 1\n",
                        "- ìŒì„±(Negative): í™˜ì ì•„ë‹˜(ì •ìƒ), 0\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ì¸ê°€? (ë©”ì¼ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íŒ¸ë©”ì¼ì„ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°.)\n",
                        "- ì–‘ì„±(Positive): ìŠ¤íŒ¸ë©”ì¼, 1\n",
                        "- ìŒì„±(Negative): ìŠ¤íŒ¸ë©”ì¼ ì•„ë‹˜(ì •ìƒ ë©”ì¼), 0\n",
                        "- ì‚¬ê¸° ê±°ë˜ ì¸ê°€? (ê¸ˆìœµê±°ë˜ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ê¸ˆìœµì‚¬ê¸° ê±°ë˜ë¥¼ ì°¾ìœ¼ë ¤ëŠ” ê²½ìš°.)\n",
                        "- ì–‘ì„±(Positive): ì‚¬ê¸° ê±°ë˜, 1\n",
                        "- ìŒì„±(Negative): ì‚¬ê¸° ê±°ë˜ ì•„ë‹˜(ì •ìƒ ê±°ë˜), 0' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.3887022152745078}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3887022152745078\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "TODO: breast_cancer data ëª¨ë¸ë§  \n",
                        "1. breast cancer data ë¡œë”©\n",
                        "1. train/test setìœ¼ë¡œ ë¶„ë¦¬\n",
                        "1. ëª¨ë¸ë§ RandomForestClassifier(max_depth=2, n_estimators=200)\n",
                        "1. í‰ê°€ (Train/Test set)\n",
                        "- í‰ê°€ì§€í‘œ\n",
                        "- accuracy, recall, precision, f1 score, confusion matrix\n",
                        "- PR curve ê·¸ë¦¬ê³  AP ì ìˆ˜ í™•ì¸\n",
                        "- ROC curve ê·¸ë¦¬ê³  AUC ì ìˆ˜í™•ì¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 96, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset[\\'data\\'], dataset[\\'target\\']\\nX.shape, y.shape\\n```\\n\\n```python\\nimport numpy as np\\nnp.unique(y)\\n```\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```\\n\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  ëª¨ë¸ìƒì„±\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(í•™ìŠµ)\\nmodel.fit(X_train, y_train)\\n```\\n\\n```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# í‰ê°€\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# ì •í™•ë„\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(ì¬í˜„ìœ¨)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(ì •ë°€ë„)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```\\n\\n```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test setì˜ Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```\\n\\n```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # ê°ì²´ìƒì„±: í‰ê°€ì ìˆ˜ë“¤ì„ ì„¤ì •\\ndisp.plot(cmap=\"Blues\"); # ì‹œê°í™”ê´€ë ¨(matplotlib) ì„¤ì •.\\n```\\n\\n```python\\n# Precision Recall Curve + Average Precision Score => ëª¨ë¸ì˜ ì–‘ì„±ì— ëŒ€í•œ ì „ì²´ì ì¸ ì„±ëŠ¥\\npred_test_proba = model.predict_proba(X_test)[:, 1] # ì–‘ì„±ì¼ í™•ë¥ \\npred_test_proba[:10]\\n```\\n\\n```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (ì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nap_score\\n```\\n\\n```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```\\n\\n```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```\\n\\n```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```', 'chunk_index': 27, 'original_score': 0.472425204}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.472425204\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦]\n",
                        "\n",
                        "> ### Boston Housing DataSet\n",
                        "> ë¯¸êµ­ ë³´ìŠ¤í†¤ì˜ êµ¬ì—­ë³„ ì§‘ê°’ ë°ì´í„°ì…‹\n",
                        "> - CRIM: ì§€ì—­ë³„ ë²”ì£„ ë°œìƒë¥ \n",
                        "> - ZN: 25,000 í‰ë°©í”¼íŠ¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê±°ì£¼ì§€ì—­ì˜ ë¹„ìœ¨\n",
                        "> - INDUS: ë¹„ìƒì—…ì§€ì—­ í† ì§€ì˜ ë¹„ìœ¨\n",
                        "> - CHAS: ì°°ìŠ¤ê°•ì— ëŒ€í•œ ë”ë¯¸ë³€ìˆ˜(ê°•ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²½ìš°ëŠ” 1, ì•„ë‹ˆë©´ 0)\n",
                        "> - NOX: ì¼ì‚°í™”ì§ˆì†Œ ë†ë„\n",
                        "> - RM: ì£¼íƒ 1ê°€êµ¬ë‹¹ í‰ê·  ë°©ì˜ ê°œìˆ˜\n",
                        "> - AGE: 1940ë…„ ì´ì „ì— ê±´ì¶•ëœ ì†Œìœ ì£¼íƒì˜ ë¹„ìœ¨\n",
                        "> - DIS: 5ê°œì˜ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - RAD: ê³ ì†ë„ë¡œê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - TAX: 10,000 ë‹¬ëŸ¬ ë‹¹ ì¬ì‚°ì„¸ìœ¨\n",
                        "> - PTRATIO : ì§€ì—­ë³„ êµì‚¬ í•œëª…ë‹¹ í•™ìƒ ë¹„ìœ¨\n",
                        "> - B: ì§€ì—­ì˜ í‘ì¸ ê±°ì£¼ ë¹„ìœ¨\n",
                        "> - LSTAT: í•˜ìœ„ê³„ì¸µì˜ ë¹„ìœ¨(%)>\n",
                        "> - MEDV: Target. ì§€ì—­ì˜ ì£¼íƒê°€ê²© ì¤‘ì•™ê°’ (ë‹¨ìœ„: $1,000)\n",
                        ">' metadata={'source': '', 'source_file': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦.ipynb', 'lecture_title': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦', 'cell_type': 'markdown', 'cell_index': 29, 'code_snippet': '', 'chunk_index': 11, 'original_score': 0.3770527564650601}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3770527564650601\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "Dataset Loading' metadata={'source': '', 'source_file': '3_Fine_tuning.ipynb', 'lecture_title': '3_Fine_tuning', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '```python\\nimport os\\nfrom huggingface_hub import login\\nfrom dotenv import load_dotenv\\n\\nprint(load_dotenv(\"env\")) # íŒŒì¼ê²½ë¡œ ì§€ì •.\\n\\nlogin(os.getenv(\\'HUGGINGFACE_API_KEY\\'))\\n```\\n\\n```python\\nfrom datasets import load_dataset\\n# ë°ì´í„°ì…‹ ë¡œë”©\\nuser_id = \"kgmyh\"  # ë³¸ì¸ Huggingface ì‚¬ìš©ìëª… ì…ë ¥\\ndata_id = f\"{user_id}/naver_economy_news_stock_instruct_dataset-100_samples\"\\ndataset = load_dataset(data_id)\\n\\ndataset\\n```\\n\\n```python\\ntrain_set = dataset[\\'train\\']\\ntest_set = dataset[\\'test\\']\\n```\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_id = \"kakaocorp/kanana-nano-2.1b-instruct\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    device_map=\"auto\",\\n    dtype=torch.bfloat16,\\n    attn_implementation=\"eager\" \\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\n```\\n\\n```python\\nprint(model.get_memory_footprint() / 1024 / 1024 / 1024, \"GB\")\\n```\\n\\n```python\\n# #  ì…ë ¥ í”„ë¡¬í”„íŠ¸ ìƒì„±\\n#\\n#  - kanana, Llama ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹\\n#    - Instruction ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ prompt í˜•ì‹ì— ë§ì¶° ì…ë ¥ë°ì´í„°ë¥¼ ë³€í™˜ì„ í•´ì•¼ í•œë‹¤.\\n#  - `tokenizer.apply_chat_template()`: \\n#       - {\"role\":\"ì—­í• \", \"content\":\"content\"} êµ¬ì¡°ë¡œ ì…ë ¥ì„ í•˜ë©´ ì‹¤ì œ ëª¨ë¸ì˜ ëª¨ë¸ì˜ ì…ë ¥í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ëŠ” ë©”ì†Œë“œ.\\n# content = \"ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œìš”?\"\\nmessage = [  \\n        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ë‚ ì”¨ ì˜ˆë³´ê´€ì…ë‹ˆë‹¤.\"},\\n        {\"role\": \"user\", \"content\": content},\\n]\\n\\n# ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ ë³€í™˜.\\nprompt = tokenizer.apply_chat_template(\\n    message,\\n    tokenize=False, # True: í† í° idë¡œ ë³€í™˜(í† í°í™”ì²˜ë¦¬).\\n    add_generation_prompt=True # ë§¨ ë’¤ì— <|start_header_id|>assistant<|end_header_id|> ë¥¼ ì¶”ê°€í• ì§€ ì—¬ë¶€\\n)\\nprint(prompt)\\n```\\n\\n```python\\n# # í† í°í™”\\n# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\\nprint(inputs)\\n```\\n\\n```python\\n# # ë‹µë³€ ìƒì„±\\n# ìƒì„±ëª¨ë¸: model.generate() ë©”ì†Œë“œ ì‚¬ìš©\\n# import torch\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    output = model.generate(\\n        **inputs,\\n        max_new_tokens=100, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ\\n        do_sample=True,     # True: í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ(í™•ë¥  ê¸°ì¤€: top_p, temperature), False: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í°ë§Œ ì„ íƒ\\n        top_p=0.95,         # ë‹¤ìŒì— ì˜¬ í™•ë¥  ìˆœìœ¼ë¡œ í† í°ë“¤ ì •ë ¬ â†’ ëˆ„ì  â†’ 0.95(ì§€ì •í•œê°’) ë„ë‹¬ ì‹œì ê¹Œì§€ ìƒìœ„í† í°ë§Œ ë‚¨ê¸´ë‹¤. ë‚®ìœ¼ë©´(ì˜ˆ: 0.5) ì°½ì˜ì„±â†“, ë„ˆë¬´ ë†’ìœ¼ë©´(1.0) ì°½ì˜ì„±â†‘\\n        temperature=0.8,    # í† í°ì˜ ë‹¤ì–‘ì„±ì„ ì§€ì •. ë‚®ì„ìˆ˜ë¡(0ì— ê°€ê¹Œìš¸ìˆ˜ë¡) ë†’ì€ í™•ë¥ ì˜ í† í°ì„ ë”ìš± ì„ íƒ. \\n        # ë‚®ìœ¼ë©´(ì˜ˆ: 0.5) ì°½ì˜ìˆ˜ ë‚®ì•„ì§€ê³ ,  ë„ˆë¬´ ë†’ìœ¼ë©´(1.0) ì°½ì˜ì„±â†‘\\n    )\\n```\\n\\n```python\\nprint(tokenizer.decode(output[0]))\\n```\\n\\n```python\\n# ë‹µë³€ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ë””ì½”ë”©\\nresponse = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\\nprint(response)\\n```', 'chunk_index': 2, 'original_score': 0.3338130619417492}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3338130619417492\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "TODO: breast_cancer data ëª¨ë¸ë§  \n",
                        "1. breast cancer data ë¡œë”©\n",
                        "1. train/test setìœ¼ë¡œ ë¶„ë¦¬\n",
                        "1. ëª¨ë¸ë§ RandomForestClassifier(max_depth=2, n_estimators=200)\n",
                        "1. í‰ê°€ (Train/Test set)\n",
                        "- í‰ê°€ì§€í‘œ\n",
                        "- accuracy, recall, precision, f1 score, confusion matrix\n",
                        "- PR curve ê·¸ë¦¬ê³  AP ì ìˆ˜ í™•ì¸\n",
                        "- ROC curve ê·¸ë¦¬ê³  AUC ì ìˆ˜í™•ì¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 96, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset[\\'data\\'], dataset[\\'target\\']\\nX.shape, y.shape\\n```\\n\\n```python\\nimport numpy as np\\nnp.unique(y)\\n```\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```\\n\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  ëª¨ë¸ìƒì„±\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(í•™ìŠµ)\\nmodel.fit(X_train, y_train)\\n```\\n\\n```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# í‰ê°€\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# ì •í™•ë„\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(ì¬í˜„ìœ¨)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(ì •ë°€ë„)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```\\n\\n```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test setì˜ Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```\\n\\n```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # ê°ì²´ìƒì„±: í‰ê°€ì ìˆ˜ë“¤ì„ ì„¤ì •\\ndisp.plot(cmap=\"Blues\"); # ì‹œê°í™”ê´€ë ¨(matplotlib) ì„¤ì •.\\n```\\n\\n```python\\n# Precision Recall Curve + Average Precision Score => ëª¨ë¸ì˜ ì–‘ì„±ì— ëŒ€í•œ ì „ì²´ì ì¸ ì„±ëŠ¥\\npred_test_proba = model.predict_proba(X_test)[:, 1] # ì–‘ì„±ì¼ í™•ë¥ \\npred_test_proba[:10]\\n```\\n\\n```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (ì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nap_score\\n```\\n\\n```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```\\n\\n```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```\\n\\n```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```', 'chunk_index': 27, 'original_score': 0.47242764}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.47242764\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦]\n",
                        "\n",
                        "> ### Boston Housing DataSet\n",
                        "> ë¯¸êµ­ ë³´ìŠ¤í†¤ì˜ êµ¬ì—­ë³„ ì§‘ê°’ ë°ì´í„°ì…‹\n",
                        "> - CRIM: ì§€ì—­ë³„ ë²”ì£„ ë°œìƒë¥ \n",
                        "> - ZN: 25,000 í‰ë°©í”¼íŠ¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê±°ì£¼ì§€ì—­ì˜ ë¹„ìœ¨\n",
                        "> - INDUS: ë¹„ìƒì—…ì§€ì—­ í† ì§€ì˜ ë¹„ìœ¨\n",
                        "> - CHAS: ì°°ìŠ¤ê°•ì— ëŒ€í•œ ë”ë¯¸ë³€ìˆ˜(ê°•ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²½ìš°ëŠ” 1, ì•„ë‹ˆë©´ 0)\n",
                        "> - NOX: ì¼ì‚°í™”ì§ˆì†Œ ë†ë„\n",
                        "> - RM: ì£¼íƒ 1ê°€êµ¬ë‹¹ í‰ê·  ë°©ì˜ ê°œìˆ˜\n",
                        "> - AGE: 1940ë…„ ì´ì „ì— ê±´ì¶•ëœ ì†Œìœ ì£¼íƒì˜ ë¹„ìœ¨\n",
                        "> - DIS: 5ê°œì˜ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - RAD: ê³ ì†ë„ë¡œê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - TAX: 10,000 ë‹¬ëŸ¬ ë‹¹ ì¬ì‚°ì„¸ìœ¨\n",
                        "> - PTRATIO : ì§€ì—­ë³„ êµì‚¬ í•œëª…ë‹¹ í•™ìƒ ë¹„ìœ¨\n",
                        "> - B: ì§€ì—­ì˜ í‘ì¸ ê±°ì£¼ ë¹„ìœ¨\n",
                        "> - LSTAT: í•˜ìœ„ê³„ì¸µì˜ ë¹„ìœ¨(%)>\n",
                        "> - MEDV: Target. ì§€ì—­ì˜ ì£¼íƒê°€ê²© ì¤‘ì•™ê°’ (ë‹¨ìœ„: $1,000)\n",
                        ">' metadata={'source': '', 'source_file': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦.ipynb', 'lecture_title': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦', 'cell_type': 'markdown', 'cell_index': 29, 'code_snippet': '', 'chunk_index': 11, 'original_score': 0.37703583646506}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.37703583646506\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "Dataset Loading' metadata={'source': '', 'source_file': '3_Fine_tuning.ipynb', 'lecture_title': '3_Fine_tuning', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '```python\\nimport os\\nfrom huggingface_hub import login\\nfrom dotenv import load_dotenv\\n\\nprint(load_dotenv(\"env\")) # íŒŒì¼ê²½ë¡œ ì§€ì •.\\n\\nlogin(os.getenv(\\'HUGGINGFACE_API_KEY\\'))\\n```\\n\\n```python\\nfrom datasets import load_dataset\\n# ë°ì´í„°ì…‹ ë¡œë”©\\nuser_id = \"kgmyh\"  # ë³¸ì¸ Huggingface ì‚¬ìš©ìëª… ì…ë ¥\\ndata_id = f\"{user_id}/naver_economy_news_stock_instruct_dataset-100_samples\"\\ndataset = load_dataset(data_id)\\n\\ndataset\\n```\\n\\n```python\\ntrain_set = dataset[\\'train\\']\\ntest_set = dataset[\\'test\\']\\n```\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_id = \"kakaocorp/kanana-nano-2.1b-instruct\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    device_map=\"auto\",\\n    dtype=torch.bfloat16,\\n    attn_implementation=\"eager\" \\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\n```\\n\\n```python\\nprint(model.get_memory_footprint() / 1024 / 1024 / 1024, \"GB\")\\n```\\n\\n```python\\n# #  ì…ë ¥ í”„ë¡¬í”„íŠ¸ ìƒì„±\\n#\\n#  - kanana, Llama ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹\\n#    - Instruction ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ prompt í˜•ì‹ì— ë§ì¶° ì…ë ¥ë°ì´í„°ë¥¼ ë³€í™˜ì„ í•´ì•¼ í•œë‹¤.\\n#  - `tokenizer.apply_chat_template()`: \\n#       - {\"role\":\"ì—­í• \", \"content\":\"content\"} êµ¬ì¡°ë¡œ ì…ë ¥ì„ í•˜ë©´ ì‹¤ì œ ëª¨ë¸ì˜ ëª¨ë¸ì˜ ì…ë ¥í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ëŠ” ë©”ì†Œë“œ.\\n# content = \"ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œìš”?\"\\nmessage = [  \\n        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ë‚ ì”¨ ì˜ˆë³´ê´€ì…ë‹ˆë‹¤.\"},\\n        {\"role\": \"user\", \"content\": content},\\n]\\n\\n# ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ ë³€í™˜.\\nprompt = tokenizer.apply_chat_template(\\n    message,\\n    tokenize=False, # True: í† í° idë¡œ ë³€í™˜(í† í°í™”ì²˜ë¦¬).\\n    add_generation_prompt=True # ë§¨ ë’¤ì— <|start_header_id|>assistant<|end_header_id|> ë¥¼ ì¶”ê°€í• ì§€ ì—¬ë¶€\\n)\\nprint(prompt)\\n```\\n\\n```python\\n# # í† í°í™”\\n# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\\nprint(inputs)\\n```\\n\\n```python\\n# # ë‹µë³€ ìƒì„±\\n# ìƒì„±ëª¨ë¸: model.generate() ë©”ì†Œë“œ ì‚¬ìš©\\n# import torch\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    output = model.generate(\\n        **inputs,\\n        max_new_tokens=100, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ\\n        do_sample=True,     # True: í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ(í™•ë¥  ê¸°ì¤€: top_p, temperature), False: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í°ë§Œ ì„ íƒ\\n        top_p=0.95,         # ë‹¤ìŒì— ì˜¬ í™•ë¥  ìˆœìœ¼ë¡œ í† í°ë“¤ ì •ë ¬ â†’ ëˆ„ì  â†’ 0.95(ì§€ì •í•œê°’) ë„ë‹¬ ì‹œì ê¹Œì§€ ìƒìœ„í† í°ë§Œ ë‚¨ê¸´ë‹¤. ë‚®ìœ¼ë©´(ì˜ˆ: 0.5) ì°½ì˜ì„±â†“, ë„ˆë¬´ ë†’ìœ¼ë©´(1.0) ì°½ì˜ì„±â†‘\\n        temperature=0.8,    # í† í°ì˜ ë‹¤ì–‘ì„±ì„ ì§€ì •. ë‚®ì„ìˆ˜ë¡(0ì— ê°€ê¹Œìš¸ìˆ˜ë¡) ë†’ì€ í™•ë¥ ì˜ í† í°ì„ ë”ìš± ì„ íƒ. \\n        # ë‚®ìœ¼ë©´(ì˜ˆ: 0.5) ì°½ì˜ìˆ˜ ë‚®ì•„ì§€ê³ ,  ë„ˆë¬´ ë†’ìœ¼ë©´(1.0) ì°½ì˜ì„±â†‘\\n    )\\n```\\n\\n```python\\nprint(tokenizer.decode(output[0]))\\n```\\n\\n```python\\n# ë‹µë³€ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ë””ì½”ë”©\\nresponse = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\\nprint(response)\\n```', 'chunk_index': 2, 'original_score': 0.33379582394174917}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.33379582394174917\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "TODO: breast_cancer data ëª¨ë¸ë§  \n",
                        "1. breast cancer data ë¡œë”©\n",
                        "1. train/test setìœ¼ë¡œ ë¶„ë¦¬\n",
                        "1. ëª¨ë¸ë§ RandomForestClassifier(max_depth=2, n_estimators=200)\n",
                        "1. í‰ê°€ (Train/Test set)\n",
                        "- í‰ê°€ì§€í‘œ\n",
                        "- accuracy, recall, precision, f1 score, confusion matrix\n",
                        "- PR curve ê·¸ë¦¬ê³  AP ì ìˆ˜ í™•ì¸\n",
                        "- ROC curve ê·¸ë¦¬ê³  AUC ì ìˆ˜í™•ì¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 96, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset[\\'data\\'], dataset[\\'target\\']\\nX.shape, y.shape\\n```\\n\\n```python\\nimport numpy as np\\nnp.unique(y)\\n```\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```\\n\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  ëª¨ë¸ìƒì„±\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(í•™ìŠµ)\\nmodel.fit(X_train, y_train)\\n```\\n\\n```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# í‰ê°€\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# ì •í™•ë„\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(ì¬í˜„ìœ¨)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(ì •ë°€ë„)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```\\n\\n```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test setì˜ Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```\\n\\n```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # ê°ì²´ìƒì„±: í‰ê°€ì ìˆ˜ë“¤ì„ ì„¤ì •\\ndisp.plot(cmap=\"Blues\"); # ì‹œê°í™”ê´€ë ¨(matplotlib) ì„¤ì •.\\n```\\n\\n```python\\n# Precision Recall Curve + Average Precision Score => ëª¨ë¸ì˜ ì–‘ì„±ì— ëŒ€í•œ ì „ì²´ì ì¸ ì„±ëŠ¥\\npred_test_proba = model.predict_proba(X_test)[:, 1] # ì–‘ì„±ì¼ í™•ë¥ \\npred_test_proba[:10]\\n```\\n\\n```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (ì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nap_score\\n```\\n\\n```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```\\n\\n```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```\\n\\n```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```', 'chunk_index': 27, 'original_score': 0.45530386}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.45530386\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦]\n",
                        "\n",
                        "> ### Boston Housing DataSet\n",
                        "> ë¯¸êµ­ ë³´ìŠ¤í†¤ì˜ êµ¬ì—­ë³„ ì§‘ê°’ ë°ì´í„°ì…‹\n",
                        "> - CRIM: ì§€ì—­ë³„ ë²”ì£„ ë°œìƒë¥ \n",
                        "> - ZN: 25,000 í‰ë°©í”¼íŠ¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê±°ì£¼ì§€ì—­ì˜ ë¹„ìœ¨\n",
                        "> - INDUS: ë¹„ìƒì—…ì§€ì—­ í† ì§€ì˜ ë¹„ìœ¨\n",
                        "> - CHAS: ì°°ìŠ¤ê°•ì— ëŒ€í•œ ë”ë¯¸ë³€ìˆ˜(ê°•ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²½ìš°ëŠ” 1, ì•„ë‹ˆë©´ 0)\n",
                        "> - NOX: ì¼ì‚°í™”ì§ˆì†Œ ë†ë„\n",
                        "> - RM: ì£¼íƒ 1ê°€êµ¬ë‹¹ í‰ê·  ë°©ì˜ ê°œìˆ˜\n",
                        "> - AGE: 1940ë…„ ì´ì „ì— ê±´ì¶•ëœ ì†Œìœ ì£¼íƒì˜ ë¹„ìœ¨\n",
                        "> - DIS: 5ê°œì˜ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - RAD: ê³ ì†ë„ë¡œê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - TAX: 10,000 ë‹¬ëŸ¬ ë‹¹ ì¬ì‚°ì„¸ìœ¨\n",
                        "> - PTRATIO : ì§€ì—­ë³„ êµì‚¬ í•œëª…ë‹¹ í•™ìƒ ë¹„ìœ¨\n",
                        "> - B: ì§€ì—­ì˜ í‘ì¸ ê±°ì£¼ ë¹„ìœ¨\n",
                        "> - LSTAT: í•˜ìœ„ê³„ì¸µì˜ ë¹„ìœ¨(%)>\n",
                        "> - MEDV: Target. ì§€ì—­ì˜ ì£¼íƒê°€ê²© ì¤‘ì•™ê°’ (ë‹¨ìœ„: $1,000)\n",
                        ">' metadata={'source': '', 'source_file': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦.ipynb', 'lecture_title': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦', 'cell_type': 'markdown', 'cell_index': 29, 'code_snippet': '', 'chunk_index': 11, 'original_score': 0.35672037246506005}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.35672037246506005\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„]\n",
                        "\n",
                        "scikit-learn ë‚´ì¥ ë°ì´í„°ì…‹ì˜ êµ¬ì„±\n",
                        "- scikit-learnì˜ datasetì€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì˜ Bunch í´ë˜ìŠ¤ ê°ì²´ì´ë‹¤.\n",
                        "- keys() í•¨ìˆ˜ë¡œ keyê°’ë“¤ì„ ì¡°íšŒ\n",
                        "- êµ¬ì„±\n",
                        "- **target_names**: ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’(class)ì„ ê°€ì§„ ë¬¸ìì—´ ë°°ì—´\n",
                        "- **target**: Label(ì¶œë ¥ë°ì´í„°)\n",
                        "- **data**: Feature(ì…ë ¥ë³€ìˆ˜)\n",
                        "- **feature_names**: ì…ë ¥ë³€ìˆ˜ ê° í•­ëª©ì˜ ì´ë¦„\n",
                        "- **DESCR**: ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„¤ëª…' metadata={'source': '', 'source_file': '02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„.ipynb', 'lecture_title': '02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '```python\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nprint(type(iris))\\n```\\n\\n```python\\niris.keys() # Dataset êµ¬ì„± keyê°’ë“¤ ì¡°íšŒ\\n```\\n\\n```python\\n# # ì…ë ¥ë³€ìˆ˜ ì¡°íšŒ\\nprint(iris.data.shape)\\niris.data[:3]\\n# iris[\\'data\\']\\n```\\n\\n```python\\n# ì…ë ¥ë³€ìˆ˜ëª… ì¡°íšŒ\\niris[\\'feature_names\\']\\n```\\n\\n```python\\n# ì¶œë ¥ë³€ìˆ˜ ì¡°íšŒ\\nprint(iris[\\'target\\'].shape)\\niris[\\'target\\']#[:3]\\n```\\n\\n```python\\n# ì¶œë ¥ ë³€ìˆ˜ì˜ classì˜ ì˜ë¯¸ ì¡°íšŒ\\niris[\\'target_names\\']\\n```\\n\\n```python\\nprint(\"--- IRIS Dataset ì„¤ëª… ---\")\\nprint(iris[\\'DESCR\\'])\\n```', 'chunk_index': 3, 'original_score': 0.33266681081267974}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.33266681081267974\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "TODO: breast_cancer data ëª¨ë¸ë§  \n",
                        "1. breast cancer data ë¡œë”©\n",
                        "1. train/test setìœ¼ë¡œ ë¶„ë¦¬\n",
                        "1. ëª¨ë¸ë§ RandomForestClassifier(max_depth=2, n_estimators=200)\n",
                        "1. í‰ê°€ (Train/Test set)\n",
                        "- í‰ê°€ì§€í‘œ\n",
                        "- accuracy, recall, precision, f1 score, confusion matrix\n",
                        "- PR curve ê·¸ë¦¬ê³  AP ì ìˆ˜ í™•ì¸\n",
                        "- ROC curve ê·¸ë¦¬ê³  AUC ì ìˆ˜í™•ì¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 96, 'code_snippet': '```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset[\\'data\\'], dataset[\\'target\\']\\nX.shape, y.shape\\n```\\n\\n```python\\nimport numpy as np\\nnp.unique(y)\\n```\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```\\n\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  ëª¨ë¸ìƒì„±\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(í•™ìŠµ)\\nmodel.fit(X_train, y_train)\\n```\\n\\n```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# í‰ê°€\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# ì •í™•ë„\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(ì¬í˜„ìœ¨)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(ì •ë°€ë„)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```\\n\\n```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test setì˜ Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```\\n\\n```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # ê°ì²´ìƒì„±: í‰ê°€ì ìˆ˜ë“¤ì„ ì„¤ì •\\ndisp.plot(cmap=\"Blues\"); # ì‹œê°í™”ê´€ë ¨(matplotlib) ì„¤ì •.\\n```\\n\\n```python\\n# Precision Recall Curve + Average Precision Score => ëª¨ë¸ì˜ ì–‘ì„±ì— ëŒ€í•œ ì „ì²´ì ì¸ ì„±ëŠ¥\\npred_test_proba = model.predict_proba(X_test)[:, 1] # ì–‘ì„±ì¼ í™•ë¥ \\npred_test_proba[:10]\\n```\\n\\n```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (ì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nap_score\\n```\\n\\n```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```\\n\\n```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```\\n\\n```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```', 'chunk_index': 27, 'original_score': 0.45530386}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.45530386\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦]\n",
                        "\n",
                        "> ### Boston Housing DataSet\n",
                        "> ë¯¸êµ­ ë³´ìŠ¤í†¤ì˜ êµ¬ì—­ë³„ ì§‘ê°’ ë°ì´í„°ì…‹\n",
                        "> - CRIM: ì§€ì—­ë³„ ë²”ì£„ ë°œìƒë¥ \n",
                        "> - ZN: 25,000 í‰ë°©í”¼íŠ¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê±°ì£¼ì§€ì—­ì˜ ë¹„ìœ¨\n",
                        "> - INDUS: ë¹„ìƒì—…ì§€ì—­ í† ì§€ì˜ ë¹„ìœ¨\n",
                        "> - CHAS: ì°°ìŠ¤ê°•ì— ëŒ€í•œ ë”ë¯¸ë³€ìˆ˜(ê°•ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²½ìš°ëŠ” 1, ì•„ë‹ˆë©´ 0)\n",
                        "> - NOX: ì¼ì‚°í™”ì§ˆì†Œ ë†ë„\n",
                        "> - RM: ì£¼íƒ 1ê°€êµ¬ë‹¹ í‰ê·  ë°©ì˜ ê°œìˆ˜\n",
                        "> - AGE: 1940ë…„ ì´ì „ì— ê±´ì¶•ëœ ì†Œìœ ì£¼íƒì˜ ë¹„ìœ¨\n",
                        "> - DIS: 5ê°œì˜ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - RAD: ê³ ì†ë„ë¡œê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜\n",
                        "> - TAX: 10,000 ë‹¬ëŸ¬ ë‹¹ ì¬ì‚°ì„¸ìœ¨\n",
                        "> - PTRATIO : ì§€ì—­ë³„ êµì‚¬ í•œëª…ë‹¹ í•™ìƒ ë¹„ìœ¨\n",
                        "> - B: ì§€ì—­ì˜ í‘ì¸ ê±°ì£¼ ë¹„ìœ¨\n",
                        "> - LSTAT: í•˜ìœ„ê³„ì¸µì˜ ë¹„ìœ¨(%)>\n",
                        "> - MEDV: Target. ì§€ì—­ì˜ ì£¼íƒê°€ê²© ì¤‘ì•™ê°’ (ë‹¨ìœ„: $1,000)\n",
                        ">' metadata={'source': '', 'source_file': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦.ipynb', 'lecture_title': '03_ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°ì™€ ëª¨ë¸ê²€ì¦', 'cell_type': 'markdown', 'cell_index': 29, 'code_snippet': '', 'chunk_index': 11, 'original_score': 0.35672037246506005}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.35672037246506005\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„]\n",
                        "\n",
                        "scikit-learn ë‚´ì¥ ë°ì´í„°ì…‹ì˜ êµ¬ì„±\n",
                        "- scikit-learnì˜ datasetì€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì˜ Bunch í´ë˜ìŠ¤ ê°ì²´ì´ë‹¤.\n",
                        "- keys() í•¨ìˆ˜ë¡œ keyê°’ë“¤ì„ ì¡°íšŒ\n",
                        "- êµ¬ì„±\n",
                        "- **target_names**: ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’(class)ì„ ê°€ì§„ ë¬¸ìì—´ ë°°ì—´\n",
                        "- **target**: Label(ì¶œë ¥ë°ì´í„°)\n",
                        "- **data**: Feature(ì…ë ¥ë³€ìˆ˜)\n",
                        "- **feature_names**: ì…ë ¥ë³€ìˆ˜ ê° í•­ëª©ì˜ ì´ë¦„\n",
                        "- **DESCR**: ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„¤ëª…' metadata={'source': '', 'source_file': '02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„.ipynb', 'lecture_title': '02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '```python\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nprint(type(iris))\\n```\\n\\n```python\\niris.keys() # Dataset êµ¬ì„± keyê°’ë“¤ ì¡°íšŒ\\n```\\n\\n```python\\n# # ì…ë ¥ë³€ìˆ˜ ì¡°íšŒ\\nprint(iris.data.shape)\\niris.data[:3]\\n# iris[\\'data\\']\\n```\\n\\n```python\\n# ì…ë ¥ë³€ìˆ˜ëª… ì¡°íšŒ\\niris[\\'feature_names\\']\\n```\\n\\n```python\\n# ì¶œë ¥ë³€ìˆ˜ ì¡°íšŒ\\nprint(iris[\\'target\\'].shape)\\niris[\\'target\\']#[:3]\\n```\\n\\n```python\\n# ì¶œë ¥ ë³€ìˆ˜ì˜ classì˜ ì˜ë¯¸ ì¡°íšŒ\\niris[\\'target_names\\']\\n```\\n\\n```python\\nprint(\"--- IRIS Dataset ì„¤ëª… ---\")\\nprint(iris[\\'DESCR\\'])\\n```', 'chunk_index': 3, 'original_score': 0.33266681081267974}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.33266681081267974\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.76134488}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.76134488\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "ì—­í• \n",
                        "1. **Model/Network**\n",
                        "- ê¸°ì¡´ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜. ë”¥ëŸ¬ë‹(Neural Network) ëª¨ë¸ì€ Network (model) ì´ë¼ê³  í•œë‹¤.\n",
                        "2. **Loss Function (ì†ì‹¤ í•¨ìˆ˜)**\n",
                        "- ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ì¶”ë¡ í•œ ê²°ê³¼ì™€ ì •ë‹µ(Ground truth)ê°„ì˜ Loss(ì†ì‹¤/ì˜¤ì°¨)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
                        "3. **Optimizer(ì˜µí‹°ë§ˆì´ì €)**\n",
                        "- ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ loss functionì´ ê³„ì‚°í•œ Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ Lossê°€ ì¤„ì–´ë“¤ë„ë¡ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤(weight)ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.45840213658891327}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.45840213658891327\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Optimizer (ìµœì í™” ë°©ë²•)  \n",
                        "- í•™ìŠµí•  ë•Œ ëª¨ë¸ì´ ì •ë‹µì— ê°€ê¹Œìš´ ì¶”ë¡ ì„ í•˜ë„ë¡ parameter(weight, bias)ë¥¼ ìµœì í™” í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜.\n",
                        "- Deep Learningì€ ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì™€ ì˜¤ì°¨ ì—­ì „íŒŒ(back propagation) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë“¤ì„ ìµœì í™”í•œë‹¤.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 28, 'code_snippet': '', 'chunk_index': 21, 'original_score': 0.4312646424510205}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4312646424510205\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- **ìˆœì „íŒŒ(forward propagation): ì¶”ë¡ **\n",
                        "- **ì—­ì „íŒŒ(back propagation): í•™ìŠµì‹œ íŒŒë¼ë¯¸í„°(weight) ì—…ë°ì´íŠ¸**' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 44, 'code_snippet': '', 'chunk_index': 34, 'original_score': 0.4963733394694026}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4963733394694026\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤  \n",
                        "- **í•™ìŠµë‹¨ê³„**' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 14, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.5069719302446043}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5069719302446043\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.7621628}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7621628\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "ì—­í• \n",
                        "1. **Model/Network**\n",
                        "- ê¸°ì¡´ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜. ë”¥ëŸ¬ë‹(Neural Network) ëª¨ë¸ì€ Network (model) ì´ë¼ê³  í•œë‹¤.\n",
                        "2. **Loss Function (ì†ì‹¤ í•¨ìˆ˜)**\n",
                        "- ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ì¶”ë¡ í•œ ê²°ê³¼ì™€ ì •ë‹µ(Ground truth)ê°„ì˜ Loss(ì†ì‹¤/ì˜¤ì°¨)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
                        "3. **Optimizer(ì˜µí‹°ë§ˆì´ì €)**\n",
                        "- ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ loss functionì´ ê³„ì‚°í•œ Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ Lossê°€ ì¤„ì–´ë“¤ë„ë¡ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤(weight)ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.4589237405889132}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4589237405889132\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Optimizer (ìµœì í™” ë°©ë²•)  \n",
                        "- í•™ìŠµí•  ë•Œ ëª¨ë¸ì´ ì •ë‹µì— ê°€ê¹Œìš´ ì¶”ë¡ ì„ í•˜ë„ë¡ parameter(weight, bias)ë¥¼ ìµœì í™” í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜.\n",
                        "- Deep Learningì€ ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì™€ ì˜¤ì°¨ ì—­ì „íŒŒ(back propagation) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë“¤ì„ ìµœì í™”í•œë‹¤.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 28, 'code_snippet': '', 'chunk_index': 21, 'original_score': 0.4328049984510205}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4328049984510205\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- **ìˆœì „íŒŒ(forward propagation): ì¶”ë¡ **\n",
                        "- **ì—­ì „íŒŒ(back propagation): í•™ìŠµì‹œ íŒŒë¼ë¯¸í„°(weight) ì—…ë°ì´íŠ¸**' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 44, 'code_snippet': '', 'chunk_index': 34, 'original_score': 0.4979722194694026}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4979722194694026\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤  \n",
                        "- **í•™ìŠµë‹¨ê³„**' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 14, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.5084477742446043}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5084477742446043\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.74174618}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.74174618\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Optimizer (ìµœì í™” ë°©ë²•)  \n",
                        "- í•™ìŠµí•  ë•Œ ëª¨ë¸ì´ ì •ë‹µì— ê°€ê¹Œìš´ ì¶”ë¡ ì„ í•˜ë„ë¡ parameter(weight, bias)ë¥¼ ìµœì í™” í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜.\n",
                        "- Deep Learningì€ ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì™€ ì˜¤ì°¨ ì—­ì „íŒŒ(back propagation) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë“¤ì„ ìµœì í™”í•œë‹¤.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 28, 'code_snippet': '', 'chunk_index': 21, 'original_score': 0.4154927544510205}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4154927544510205\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- **ìˆœì „íŒŒ(forward propagation): ì¶”ë¡ **\n",
                        "- **ì—­ì „íŒŒ(back propagation): í•™ìŠµì‹œ íŒŒë¼ë¯¸í„°(weight) ì—…ë°ì´íŠ¸**' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 44, 'code_snippet': '', 'chunk_index': 34, 'original_score': 0.47193205146940265}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.47193205146940265\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤  \n",
                        "- **í•™ìŠµë‹¨ê³„**' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 14, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.5171437062446043}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5171437062446043\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ…]\n",
                        "\n",
                        "GradientBoosting í•™ìŠµ ë° ì¶”ë¡  í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ….ipynb', 'lecture_title': '10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ…', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.5043134363709829}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5043134363709829\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.7417320439999999}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7417320439999999\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Optimizer (ìµœì í™” ë°©ë²•)  \n",
                        "- í•™ìŠµí•  ë•Œ ëª¨ë¸ì´ ì •ë‹µì— ê°€ê¹Œìš´ ì¶”ë¡ ì„ í•˜ë„ë¡ parameter(weight, bias)ë¥¼ ìµœì í™” í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜.\n",
                        "- Deep Learningì€ ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì™€ ì˜¤ì°¨ ì—­ì „íŒŒ(back propagation) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë“¤ì„ ìµœì í™”í•œë‹¤.' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 28, 'code_snippet': '', 'chunk_index': 21, 'original_score': 0.4154800944510205}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4154800944510205\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "- **ìˆœì „íŒŒ(forward propagation): ì¶”ë¡ **\n",
                        "- **ì—­ì „íŒŒ(back propagation): í•™ìŠµì‹œ íŒŒë¼ë¯¸í„°(weight) ì—…ë°ì´íŠ¸**' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 44, 'code_snippet': '', 'chunk_index': 34, 'original_score': 0.47191150746940264}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.47191150746940264\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤  \n",
                        "- **í•™ìŠµë‹¨ê³„**' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 14, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.5171265582446043}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5171265582446043\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ…]\n",
                        "\n",
                        "GradientBoosting í•™ìŠµ ë° ì¶”ë¡  í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ….ipynb', 'lecture_title': '10_ì•™ìƒë¸”_ë¶€ìŠ¤íŒ…', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.504325202370983}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.504325202370983\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.8558167000000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.8558167000000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„]\n",
                        "\n",
                        "3. **train**\n",
                        "- train í•¨ìˆ˜, test í•¨ìˆ˜ ì •ì˜' metadata={'source': '', 'source_file': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„.ipynb', 'lecture_title': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.5536572790438248}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5536572790438248\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤  \n",
                        "- **í•™ìŠµë‹¨ê³„**' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 14, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.5476949}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5476949\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\n",
                        "\n",
                        "Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤' metadata={'source': '', 'source_file': '05_ì‹ ê²½ë§ êµ¬ì¡°.ipynb', 'lecture_title': '05_ì‹ ê²½ë§ êµ¬ì¡°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.7298201662045061}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7298201662045061\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„]\n",
                        "\n",
                        "í•™ìŠµ(í›ˆë ¨-train) ë° ê²€ì¦' metadata={'source': '', 'source_file': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„.ipynb', 'lecture_title': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„', 'cell_type': 'markdown', 'cell_index': 44, 'code_snippet': '```python\\n# í•™ìŠµì‹œ ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ê°’ë“¤ì€ ê°™ì€ device(cpu or cuda or mps)ì— ìˆì–´ì•¼ í•œë‹¤\\n# deviceë¡œ ì´ë™í•  ëŒ€ìƒ: Modelê°ì²´, X(input), y(output)\\n```\\n\\n```python\\nmodel = MNISTModel().to(device)\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n```\\n\\n```python\\nfor e in range(epochs):\\n    # ëª¨ë¸ í•™ìŠµ\\n    model.train()\\n\\n    # ëª¨ë¸ ê²€ì¦\\n    model.eval()\\n```\\n\\n```python\\nfor a in train_loader:\\n    print(type(a))\\n    break\\n```\\n\\n```python\\nimport time\\n# í•™ìŠµ\\n# í•œ Epoch í•™ìŠµì´ ëë‚ ë•Œ ë§ˆë‹¤ trainset/testsetìœ¼ë¡œ ê²€ì¦ì„ ì§„í–‰.\\n# ì—í­ë³„ ê²€ì¦ê²°ê³¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\\ntrain_loss_list = []\\nvalid_loss_list = []\\nvalid_acc_list = []\\ns = time.time()\\n\\n# í•™ìŠµ-ì¤‘ì²© ë°˜ë³µë¬¸: epoch ë°˜ë³µ -> step(batch_size) ì— ëŒ€í•œ ë°˜ë³µ\\nfor epoch in range(epochs):\\n    # 1 ì—í­í•™ìŠµ = Train + Validataion  ë‘ë‹¨ê³„ë¡œ\\n    # # ëª¨ë¸ Train - 1 epoch : Trainset\\n    # model.train()  # ëª¨ë¸ì„ train ëª¨ë“œë¡œ ë³€í™˜.\\n    train_loss = 0 # í˜„ì¬ epochì˜ train lossë¥¼ ì €ì¥í•  ë³€ìˆ˜.\\n\\n    # batch ë‹¨ìœ„ë¡œ í•™ìŠµ: 1 step - 1ê°œ batchì˜ ë°ì´í„°ë¡œ í•™ìŠµ.\\n    for X_train, y_train  in train_loader: # í•œë²ˆ ë°˜ë³µí•  ë•Œë§ˆë‹¤ 1ê°œ batch ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì œê³µ. (X, y) ë¥¼ ë¬¶ì–´ì„œ ì œê³µí•œë‹¤.\\n        # 1. X, yë¥¼ devcieë¡œ ì´ë™\\n        X_train, y_train = X_train.to(device), y_train.to(device)\\n        # 2. ëª¨ë¸ì„ ì´ìš©í•´ ì¶”ë¡ \\n        pred = model(X_train) # Model.forward(X_train) ë©”ì†Œë“œ í˜¸ì¶œ\\n        # 3. loss ê³„ì‚°\\n        loss = loss_fn(pred, y_train)\\n        # 4. gradient ê³„ì‚°\\n        loss.backward()\\n        # 5. ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤(weight, bias) update\\n        optimizer.step()\\n        # 6. gradient ì´ˆê¸°í™”\\n        optimizer.zero_grad()\\n        # í•™ìŠµ ê²°ê³¼ ì €ì¥ë° ì¶œë ¥ì„ ìœ„í•´ loss ì €ì¥.\\n        train_loss = train_loss + loss.item() \\n\\n    \\n    train_loss = train_loss / len(train_loader)  # í•œ ì—í­ì—ì„œ í•™ìŠµí•œ stepë³„ lossì˜ í‰ê· ê³„ì‚°.\\n    train_loss_list.append(train_loss)\\n    \\n    # # 1 epoch í•™ìŠµí•œ ê²°ê³¼ ê²€ì¦: Testset\\n    # model.eval()  # ëª¨ë¸ì„ evaluation() (ì¶”ë¡ , ê²€ì¦) ëª¨ë“œë¡œ ë³€í™˜.\\n    valid_loss = 0\\n    valid_acc = 0\\n    with torch.no_grad(): # ì¶”ë¡ ë§Œ í•¨ -> gradient ê³„ì‚°í•  í•„ìš” ì—†ìŒ. -> grad_fn êµ¬í•  í•„ìš”ì—†ë‹¤.\\n        for X_valid, y_valid in test_loader:\\n            # 1. device ë¡œ ì´ë™\\n            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\\n            # 2. ì¶”ë¡ \\n            pred_valid = model(X_valid)\\n            # 3-1. ê²€ì¦ -> loss ê³„ì‚°\\n            valid_loss = valid_loss + loss_fn(pred_valid, y_valid).item()\\n            # 3-2. ê²€ì¦ -> accuracy ê³„ì‚°\\n            # pred_valid shape: (256, 10: classë³„ í™•ë¥ ) -> ì •ë‹µ class ì¶”ì¶œ\\n            pred_valid_class = pred_valid.argmax(dim=-1)\\n            valid_acc = valid_acc + torch.sum(y_valid == pred_valid_class).item()\\n        \\n        # ê²€ì¦ê²°ê³¼ ëˆ„ì ê°’ì˜ í‰ê· \\n        valid_loss = valid_loss / len(test_loader)  # lossëŠ” stepìˆ˜ ë‚˜ëˆ”.\\n        valid_acc = valid_acc / len(testset)        # accuracyëŠ” ë°ì´í„° ê°œìˆ˜ë¡œ ë‚˜ëˆ”.\\n        valid_loss_list.append(valid_loss)\\n        valid_acc_list.append(valid_acc)\\n\\n        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥\\n        print(f\"[{epoch+1:02d}/{epochs}] train_loss: {train_loss}, valid_loss: {valid_loss}, valid_acc: {valid_acc}\")\\n\\ne = time.time()\\nprint(\\'í•™ìŠµì— ê±¸ë¦° ì‹œê°„(ì´ˆ):\\', e-s)\\n```\\n\\n```python\\n# train loss, valid loss, valid acc ë¥¼ epoch ë³„ë¡œ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œê°í™”.\\nplt.figure(figsize=(10, 5))\\nplt.subplot(1, 2, 1)\\nplt.plot(range(1, epochs+1), train_loss_list, label=\"train loss\")\\nplt.plot(range(1, epochs+1), valid_loss_list, label=\"valid loss\")\\nplt.title(\"Loss\")\\nplt.legend()\\nplt.ylim(0, 0.3)\\nplt.grid(True, linestyle=\":\")\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(range(1, epochs+1), valid_acc_list)\\nplt.title(\"valid accuracy\")\\nplt.grid(True, linestyle=\":\")\\n\\nplt.tight_layout()\\nplt.show()\\n```', 'chunk_index': 8, 'original_score': 0.6049481157983194}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6049481157983194\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„]\n",
                        "\n",
                        "3. **train**\n",
                        "- train í•¨ìˆ˜, test í•¨ìˆ˜ ì •ì˜' metadata={'source': '', 'source_file': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„.ipynb', 'lecture_title': '04_ì²«ë²ˆì§¸ ë”¥ëŸ¬ë‹-MLP êµ¬í˜„', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.63871704}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.63871704\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.30882516}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.30882516\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„  \n",
                        "**ë¶„ë¥˜ì˜ ê²½ìš° Precision(ì •ë°€ë„)ê°€ ì¤‘ìš”í•œ ê²½ìš°ì™€ Recall(ì¬í˜„ìœ¨) ì¤‘ìš”í•œ ì—…ë¬´ê°€ ìˆë‹¤.**' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 55, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.6466254520000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6466254520000001\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "PR Curve(Precision Recall Curve-ì •ë°€ë„ ì¬í˜„ìœ¨ ê³¡ì„ )ì™€ AP Score(Average Precision Score)\n",
                        "- ì´ì§„ë¶„ë¥˜ì˜ í‰ê°€ì§€í‘œ.\n",
                        "- Positive í™•ë¥ ì„ ì´ìš©í•´ class(0, 1)ì„ ê²°ì •í•  ë•Œ ì„ê³„ê°’ì´ ë³€í™”ì— ë”°ë¥¸ ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ë³€í™”ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì¬í˜„ìœ¨ì´ ë³€í™”í•  ë•Œ ì •ë°€ë„ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í‰ê°€í•œë‹¤.\n",
                        "- Precisionê³¼ Recall ê°’ë“¤ì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì˜ Positiveì— ëŒ€í•œ ì„±ëŠ¥ì˜ ê°•ê±´í•¨(robust)ë¥¼ í‰ê°€í•œë‹¤.\n",
                        "- **Xì¶•ì— ì¬í˜„ìœ¨, Yì¶•ì— ì •ë°€ë„ë¥¼** ë†“ê³  ì„ê³„ê°’ì´ 1 â†’ 0 ë³€í™”í• ë•Œ ë‘ ê°’ì˜ ë³€í™”ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦°ë‹¤.\n",
                        "- AP Score\n",
                        "- PR Curveì˜ ì„±ëŠ¥í‰ê°€ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ì ìˆ˜(ìˆ«ì)ë¡œ í‰ê°€í•œê²ƒ.\n",
                        "- PR Curveì˜ ì„ ì•„ë˜ ë©´ì ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ ë†’ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 75, 'code_snippet': '```python\\n# DecisionTreeì˜ PrecisionRecall ì»¤ë¸Œ ê·¸ë¦¬ê¸° + AP Score ê³„ì‚°.\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nimport matplotlib.pyplot as plt\\n\\n# ëª¨ë¸ì´ ì¶”ì •í•œ positive í™•ë¥ ì„ ì¡°íšŒ\\ntest_proba_tree = tree.predict_proba(X_test)[:, 1]  # DecisionTree\\ntest_proba_rfc = rfc.predict_proba(X_test)[:, 1]    # RandomForest\\n```\\n\\n```python\\n# ap score ë¡œ ëª¨ë¸ì„ í‰ê°€\\ntree_ap = average_precision_score(y_test, test_proba_tree)  # (yì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nrfc_ap = average_precision_score(y_test, test_proba_rfc)\\nprint(\"DecisionTree Average Precision Score:\", tree_ap)\\nprint(\"RandomForest Average Precision Score:\", rfc_ap)\\n```\\n\\n```python\\n# ì‹œê°í™”\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\n# í•˜ë‚˜ì˜ Figure ë‘ê°œ subplotìœ¼ë¡œ ê·¸ë¦¬ê¸°.\\nfig = plt.figure(figsize=(12, 6))\\nax1 = fig.add_subplot(1, 2, 1) # DecisionTree\\nax2 = fig.add_subplot(1, 2, 2) # RandomForest\\n\\ndisp_tree = PrecisionRecallDisplay(  #PrecisionRecall Curveë¥¼ ì‹œê°í™”í•˜ëŠ” í´ìŠ¤ìŠ¤\\n    precisions1, # precisionê°’ë“¤\\n    recalls1,    # recallê°’ë“¤\\n    average_precision=tree_ap  # AP score\\n)\\ndisp_tree.plot(ax=ax1) # ì‹œê°í™”\\n\\ndisp_rfc = PrecisionRecallDisplay(precisions2, recalls2, average_precision=rfc_ap)\\ndisp_rfc.plot(ax=ax2)\\n\\nax1.set_title(\"DecisionTree\")\\nax2.set_title(\"Random Forest\")\\nplt.show()\\n```\\n\\n```python\\n# í•˜ë‚˜ì˜ subplotì— ê°™ì´ ê·¸ë¦¬ê¸°.\\n\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\nax = plt.gca()\\n\\ndisp_tree = PrecisionRecallDisplay(\\n    precisions1, \\n    recalls1, \\n    average_precision=tree_ap, \\n    estimator_name=\"DecisionTree\" # label ì§€ì •\\n)\\n\\ndisp_tree.plot(ax=ax)\\n\\ndisp_rfc = PrecisionRecallDisplay(\\n    precisions2, \\n    recalls2, \\n    average_precision=rfc_ap, \\n    estimator_name=\"Random Forest\"\\n)\\ndisp_rfc.plot(ax=ax)\\n\\nplt.title(\"Precision Recall Curve\")\\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\\nplt.show()\\n```', 'chunk_index': 22, 'original_score': 0.5475746591332848}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5475746591332848\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.30882516}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.30882516\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„  \n",
                        "**ë¶„ë¥˜ì˜ ê²½ìš° Precision(ì •ë°€ë„)ê°€ ì¤‘ìš”í•œ ê²½ìš°ì™€ Recall(ì¬í˜„ìœ¨) ì¤‘ìš”í•œ ì—…ë¬´ê°€ ìˆë‹¤.**' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 55, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.6466254520000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6466254520000001\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "PR Curve(Precision Recall Curve-ì •ë°€ë„ ì¬í˜„ìœ¨ ê³¡ì„ )ì™€ AP Score(Average Precision Score)\n",
                        "- ì´ì§„ë¶„ë¥˜ì˜ í‰ê°€ì§€í‘œ.\n",
                        "- Positive í™•ë¥ ì„ ì´ìš©í•´ class(0, 1)ì„ ê²°ì •í•  ë•Œ ì„ê³„ê°’ì´ ë³€í™”ì— ë”°ë¥¸ ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ë³€í™”ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì¬í˜„ìœ¨ì´ ë³€í™”í•  ë•Œ ì •ë°€ë„ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í‰ê°€í•œë‹¤.\n",
                        "- Precisionê³¼ Recall ê°’ë“¤ì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì˜ Positiveì— ëŒ€í•œ ì„±ëŠ¥ì˜ ê°•ê±´í•¨(robust)ë¥¼ í‰ê°€í•œë‹¤.\n",
                        "- **Xì¶•ì— ì¬í˜„ìœ¨, Yì¶•ì— ì •ë°€ë„ë¥¼** ë†“ê³  ì„ê³„ê°’ì´ 1 â†’ 0 ë³€í™”í• ë•Œ ë‘ ê°’ì˜ ë³€í™”ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦°ë‹¤.\n",
                        "- AP Score\n",
                        "- PR Curveì˜ ì„±ëŠ¥í‰ê°€ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ì ìˆ˜(ìˆ«ì)ë¡œ í‰ê°€í•œê²ƒ.\n",
                        "- PR Curveì˜ ì„ ì•„ë˜ ë©´ì ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ ë†’ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 75, 'code_snippet': '```python\\n# DecisionTreeì˜ PrecisionRecall ì»¤ë¸Œ ê·¸ë¦¬ê¸° + AP Score ê³„ì‚°.\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nimport matplotlib.pyplot as plt\\n\\n# ëª¨ë¸ì´ ì¶”ì •í•œ positive í™•ë¥ ì„ ì¡°íšŒ\\ntest_proba_tree = tree.predict_proba(X_test)[:, 1]  # DecisionTree\\ntest_proba_rfc = rfc.predict_proba(X_test)[:, 1]    # RandomForest\\n```\\n\\n```python\\n# ap score ë¡œ ëª¨ë¸ì„ í‰ê°€\\ntree_ap = average_precision_score(y_test, test_proba_tree)  # (yì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nrfc_ap = average_precision_score(y_test, test_proba_rfc)\\nprint(\"DecisionTree Average Precision Score:\", tree_ap)\\nprint(\"RandomForest Average Precision Score:\", rfc_ap)\\n```\\n\\n```python\\n# ì‹œê°í™”\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\n# í•˜ë‚˜ì˜ Figure ë‘ê°œ subplotìœ¼ë¡œ ê·¸ë¦¬ê¸°.\\nfig = plt.figure(figsize=(12, 6))\\nax1 = fig.add_subplot(1, 2, 1) # DecisionTree\\nax2 = fig.add_subplot(1, 2, 2) # RandomForest\\n\\ndisp_tree = PrecisionRecallDisplay(  #PrecisionRecall Curveë¥¼ ì‹œê°í™”í•˜ëŠ” í´ìŠ¤ìŠ¤\\n    precisions1, # precisionê°’ë“¤\\n    recalls1,    # recallê°’ë“¤\\n    average_precision=tree_ap  # AP score\\n)\\ndisp_tree.plot(ax=ax1) # ì‹œê°í™”\\n\\ndisp_rfc = PrecisionRecallDisplay(precisions2, recalls2, average_precision=rfc_ap)\\ndisp_rfc.plot(ax=ax2)\\n\\nax1.set_title(\"DecisionTree\")\\nax2.set_title(\"Random Forest\")\\nplt.show()\\n```\\n\\n```python\\n# í•˜ë‚˜ì˜ subplotì— ê°™ì´ ê·¸ë¦¬ê¸°.\\n\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\nax = plt.gca()\\n\\ndisp_tree = PrecisionRecallDisplay(\\n    precisions1, \\n    recalls1, \\n    average_precision=tree_ap, \\n    estimator_name=\"DecisionTree\" # label ì§€ì •\\n)\\n\\ndisp_tree.plot(ax=ax)\\n\\ndisp_rfc = PrecisionRecallDisplay(\\n    precisions2, \\n    recalls2, \\n    average_precision=rfc_ap, \\n    estimator_name=\"Random Forest\"\\n)\\ndisp_rfc.plot(ax=ax)\\n\\nplt.title(\"Precision Recall Curve\")\\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\\nplt.show()\\n```', 'chunk_index': 22, 'original_score': 0.5475746591332848}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5475746591332848\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.71814398}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.71814398\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ê²°ê³¼ í›„ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì¬í˜„ìœ¨ ë˜ëŠ” ì •ë°€ë„ ì„±ëŠ¥ ì˜¬ë¦¬ê¸°  \n",
                        "- Positive(1)ì¼ í™•ë¥ ì— ëŒ€í•œ ì„ê³„ê°’(Threshold) ë³€ê²½ì„ í†µí•œ ì¬í˜„ìœ¨, ì •ë°€ë„ë¥¼ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤.\n",
                        "- **ê²°ê³¼ í›„ì²˜ë¦¬ì‹œ ì„ê³„ê°’(Threshold) ë³€ê²½**\n",
                        "- ë¶„ë¥˜ ëª¨ë¸ì€ ì…ë ¥ê°’ì— ëŒ€í•´ classë³„ í™•ë¥ ì„ ì˜ˆì¸¡ í•œë‹¤. ê·¸ ì¶œë ¥ëœ í™•ë¥ ê°’ì´ ë†’ì€ classë¥¼ ì •ë‹µ classë¡œ ì²˜ë¦¬í•œë‹¤.\n",
                        "- **ì´ì§„ ë¶„ë¥˜**ì˜ ê²½ìš° ëª¨ë¸ì€ ì–‘ì„±(Positive)ì¼ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.\n",
                        "- **ê²°ê³¼ í›„ì²˜ë¦¬**\n",
                        "- ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì´ ì¶œë ¥í•œ ì–‘ì„±ì¼ í™•ë¥ ì—ì„œ ì–‘ì„±ê³¼ ìŒì„±ì„ ë‚˜ëˆ„ëŠ” ì„ê³„ê°’(Threshold)ì„ ì •í•˜ê³  ê·¸ ì„ê³„ê°’ ì´í•˜ì¼ ê²½ìš° ìŒì„±, ì´ˆê³¼ì¼ ê²½ìš° ì–‘ì„±ìœ¼ë¡œ classë¥¼ ì •í•œë‹¤. ì´ ì‘ì—…ì€ ê²°ê³¼ í›„ì²˜ë¦¬ì—ì„œ ì§„í–‰í•œë‹¤.\n",
                        "- ê·¸ ì„ê³„ê°’ì„ ë¬´ì—‡ìœ¼ë¡œ í•˜ëŠëƒì— ë”°ë¼ ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ê°€ ë³€ê²½ëœë‹¤. (ê¸°ë³¸: 0.5)\n",
                        "- ëª¨ë¸ì˜ ì¬í˜„ìœ¨ì´ë‚˜ ì •ë°€ë„ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ **í›„ì²˜ë¦¬ ì‘ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ê³„ê°’(threshold)ë¥¼ ë³€ê²½í•œë‹¤.**\n",
                        "- ë‹¨ ì„ê³„ê°’(threshold)ë¥¼ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ì„±ëŠ¥ì„ ì˜¬ë¼ê°€ë©´ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë–¨ì–´ì§„ë‹¤. ì¦‰ **ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ì„ê³„ê°’ ë³€ê²½ì— ë”°ë¥¸ ì„±ëŠ¥ë³€í™”ëŠ” ë°˜ë¹„ë¡€í•œë‹¤.**\n",
                        "- ê·¸ë˜ì„œ ê·¹ë‹¨ì ìœ¼ë¡œ ì„ê³„ì ì„ ë³€ê²½í•´ì„œ í•œìª½ì˜ ì ìˆ˜ë¥¼ ë†’ì´ë©´ ì•ˆëœë‹¤.\n",
                        "- ì˜ˆ: í™˜ì ì—¬ë¶€ ì˜ˆì¸¡ì‹œ ì¬í˜„ìœ¨ì„ ë„ˆë¬´ ë†’ì´ë©´ ì •ë°€ë„ê°€ ë‚®ì•„ì ¸ ê±¸í•í•˜ë©´ ì •ìƒì¸ì„ í™˜ìë¡œ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 58, 'code_snippet': '', 'chunk_index': 16, 'original_score': 0.4421693465378995}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4421693465378995\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ROC curve(Receiver Operating Characteristic Curve)ì™€ AUC(Area Under the Curve) score  \n",
                        "- **FPR(False Positive Rate-ìœ„ì–‘ì„±ìœ¨)**\n",
                        "- ìœ„ì–‘ì„±ìœ¨ (fall-out)\n",
                        "- 1-íŠ¹ì´ë„(TNR)\n",
                        "- ì‹¤ì œ ìŒì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡ í•œ ë¹„ìœ¨\n",
                        "- ë‚®ì„ ìˆ˜ë¡ ì¢‹ë‹¤.  \n",
                        "\\cfrac{FP}{TN+FP}  \n",
                        "- **TPR(True Positive Rate-ì¬í˜„ìœ¨/ë¯¼ê°ë„)**\n",
                        "- ì¬í˜„ìœ¨(recall)\n",
                        "- ì‹¤ì œ ì–‘ì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\n",
                        "- ë†’ì„ ìˆ˜ë¡ ì¢‹ë‹¤.  \n",
                        "\\frac{TP}{FN+TP}  \n",
                        "- Positiveì˜ ì„ê³„ê°’ì„ ë³€ê²½í•  ê²½ìš° **FPRê³¼ TPR(recall)ì€ ë¹„ë¡€í•´ì„œ ë³€í™”í•œë‹¤.**\n",
                        "- ROC Curve\n",
                        "- ì´ì§„ ë¶„ë¥˜ì˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ\n",
                        "- Positive í™•ë¥ ì„ ì´ìš©í•´ class(0, 1)ì„ ê²°ì •í•  ë•Œ ì„ê³„ê°’ì´ ë³€í™”ì— ë”°ë¥¸ ì¬í˜„ìœ¨(TPR)ê³¼ ìœ„ì–‘ì„±ìœ¨(FPR)ì˜ ë³€í™”ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- FPR ë³€í™”í•  ë•Œ TPRì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ” ì§€ë¥¼ í‰ê°€í•œë‹¤.\n",
                        "- FPRì„ Xì¶•, TPRì„ Yì¶•ìœ¼ë¡œ ë†“ê³  ë†“ê³  ì„ê³„ê°’ì´ 1 â†’ 0 ë³€í™”í• ë•Œ ë‘ ê°’ì˜ ë³€í™”ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦°ë‹¤.\n",
                        "- Positive(ì–‘ì„±), Negative(ìŒì„±) ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì˜ ê°•ê±´í•¨(robust)ì„ í‰ê°€í•œë‹¤.  \n",
                        "- **AUC Score**\n",
                        "- ROC Curveì˜ ê²°ê³¼ë¥¼ ì ìˆ˜í™”(ìˆ˜ì¹˜í™”) í•˜ëŠ” í•¨ìˆ˜ë¡œ ROC Curve ì•„ë˜ìª½ ë©´ì ì„ ê³„ì‚°í•œë‹¤.\n",
                        "- 0 ~ 1 ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ë‚˜ì˜¤ë©° í´ìˆ˜ë¡ ì¢‹ë‹¤.\n",
                        "- AUC Scoreê°’ì´ í¬ë ¤ë©´(1ì— ê°€ê¹Œìš´ ê°’) ì„ê³„ê°’ì´ í´ ë•Œ FPRì€ ì‘ê³ , TPRì˜ ê°’ì€ ì»¤ì•¼ í•œë‹¤. FPRì´ ì‘ë‹¤ëŠ” ê²ƒì€ Negative ì˜ ë¶„ë¥˜í–ˆë‹¤ëŠ” ê²ƒì´ê³  TPRì´ í¬ë‹¤ëŠ” ê²ƒì€ Positiveë¥¼ ì˜ ë¶„ë¥˜ í–ˆë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ë‘˜ì— ëŒ€í•œ ë¶„ë¥˜ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n",
                        "- **AUC ì ìˆ˜ê¸°ì¤€**\n",
                        "- 1.0 ~ 0.9 : ì•„ì£¼ ì¢‹ìŒ\n",
                        "- 0.9 ~ 0.8 : ì¢‹ìŒ\n",
                        "- 0.8 ~ 0.7 : ê´œì°®ì€ ëª¨ë¸\n",
                        "- 0.7 ~ 0.6 : ì˜ë¯¸ëŠ” ìˆìœ¼ë‚˜ ì¢‹ì€ ëª¨ë¸ì€ ì•„ë‹˜\n",
                        "- 0.6 ~ 0.5 : ì¢‹ì§€ ì•Šì€ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 83, 'code_snippet': '', 'chunk_index': 23, 'original_score': 0.43756637332793713}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.43756637332793713\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.72316837}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.72316837\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ê²°ê³¼ í›„ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì¬í˜„ìœ¨ ë˜ëŠ” ì •ë°€ë„ ì„±ëŠ¥ ì˜¬ë¦¬ê¸°  \n",
                        "- Positive(1)ì¼ í™•ë¥ ì— ëŒ€í•œ ì„ê³„ê°’(Threshold) ë³€ê²½ì„ í†µí•œ ì¬í˜„ìœ¨, ì •ë°€ë„ë¥¼ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤.\n",
                        "- **ê²°ê³¼ í›„ì²˜ë¦¬ì‹œ ì„ê³„ê°’(Threshold) ë³€ê²½**\n",
                        "- ë¶„ë¥˜ ëª¨ë¸ì€ ì…ë ¥ê°’ì— ëŒ€í•´ classë³„ í™•ë¥ ì„ ì˜ˆì¸¡ í•œë‹¤. ê·¸ ì¶œë ¥ëœ í™•ë¥ ê°’ì´ ë†’ì€ classë¥¼ ì •ë‹µ classë¡œ ì²˜ë¦¬í•œë‹¤.\n",
                        "- **ì´ì§„ ë¶„ë¥˜**ì˜ ê²½ìš° ëª¨ë¸ì€ ì–‘ì„±(Positive)ì¼ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.\n",
                        "- **ê²°ê³¼ í›„ì²˜ë¦¬**\n",
                        "- ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì´ ì¶œë ¥í•œ ì–‘ì„±ì¼ í™•ë¥ ì—ì„œ ì–‘ì„±ê³¼ ìŒì„±ì„ ë‚˜ëˆ„ëŠ” ì„ê³„ê°’(Threshold)ì„ ì •í•˜ê³  ê·¸ ì„ê³„ê°’ ì´í•˜ì¼ ê²½ìš° ìŒì„±, ì´ˆê³¼ì¼ ê²½ìš° ì–‘ì„±ìœ¼ë¡œ classë¥¼ ì •í•œë‹¤. ì´ ì‘ì—…ì€ ê²°ê³¼ í›„ì²˜ë¦¬ì—ì„œ ì§„í–‰í•œë‹¤.\n",
                        "- ê·¸ ì„ê³„ê°’ì„ ë¬´ì—‡ìœ¼ë¡œ í•˜ëŠëƒì— ë”°ë¼ ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ê°€ ë³€ê²½ëœë‹¤. (ê¸°ë³¸: 0.5)\n",
                        "- ëª¨ë¸ì˜ ì¬í˜„ìœ¨ì´ë‚˜ ì •ë°€ë„ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ **í›„ì²˜ë¦¬ ì‘ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ê³„ê°’(threshold)ë¥¼ ë³€ê²½í•œë‹¤.**\n",
                        "- ë‹¨ ì„ê³„ê°’(threshold)ë¥¼ ë³€ê²½í•´ì„œ í•˜ë‚˜ì˜ ì„±ëŠ¥ì„ ì˜¬ë¼ê°€ë©´ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë–¨ì–´ì§„ë‹¤. ì¦‰ **ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ì„ê³„ê°’ ë³€ê²½ì— ë”°ë¥¸ ì„±ëŠ¥ë³€í™”ëŠ” ë°˜ë¹„ë¡€í•œë‹¤.**\n",
                        "- ê·¸ë˜ì„œ ê·¹ë‹¨ì ìœ¼ë¡œ ì„ê³„ì ì„ ë³€ê²½í•´ì„œ í•œìª½ì˜ ì ìˆ˜ë¥¼ ë†’ì´ë©´ ì•ˆëœë‹¤.\n",
                        "- ì˜ˆ: í™˜ì ì—¬ë¶€ ì˜ˆì¸¡ì‹œ ì¬í˜„ìœ¨ì„ ë„ˆë¬´ ë†’ì´ë©´ ì •ë°€ë„ê°€ ë‚®ì•„ì ¸ ê±¸í•í•˜ë©´ ì •ìƒì¸ì„ í™˜ìë¡œ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 58, 'code_snippet': '', 'chunk_index': 16, 'original_score': 0.3795590510071779}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3795590510071779\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ROC curve(Receiver Operating Characteristic Curve)ì™€ AUC(Area Under the Curve) score  \n",
                        "- **FPR(False Positive Rate-ìœ„ì–‘ì„±ìœ¨)**\n",
                        "- ìœ„ì–‘ì„±ìœ¨ (fall-out)\n",
                        "- 1-íŠ¹ì´ë„(TNR)\n",
                        "- ì‹¤ì œ ìŒì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡ í•œ ë¹„ìœ¨\n",
                        "- ë‚®ì„ ìˆ˜ë¡ ì¢‹ë‹¤.  \n",
                        "\\cfrac{FP}{TN+FP}  \n",
                        "- **TPR(True Positive Rate-ì¬í˜„ìœ¨/ë¯¼ê°ë„)**\n",
                        "- ì¬í˜„ìœ¨(recall)\n",
                        "- ì‹¤ì œ ì–‘ì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\n",
                        "- ë†’ì„ ìˆ˜ë¡ ì¢‹ë‹¤.  \n",
                        "\\frac{TP}{FN+TP}  \n",
                        "- Positiveì˜ ì„ê³„ê°’ì„ ë³€ê²½í•  ê²½ìš° **FPRê³¼ TPR(recall)ì€ ë¹„ë¡€í•´ì„œ ë³€í™”í•œë‹¤.**\n",
                        "- ROC Curve\n",
                        "- ì´ì§„ ë¶„ë¥˜ì˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ\n",
                        "- Positive í™•ë¥ ì„ ì´ìš©í•´ class(0, 1)ì„ ê²°ì •í•  ë•Œ ì„ê³„ê°’ì´ ë³€í™”ì— ë”°ë¥¸ ì¬í˜„ìœ¨(TPR)ê³¼ ìœ„ì–‘ì„±ìœ¨(FPR)ì˜ ë³€í™”ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- FPR ë³€í™”í•  ë•Œ TPRì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ” ì§€ë¥¼ í‰ê°€í•œë‹¤.\n",
                        "- FPRì„ Xì¶•, TPRì„ Yì¶•ìœ¼ë¡œ ë†“ê³  ë†“ê³  ì„ê³„ê°’ì´ 1 â†’ 0 ë³€í™”í• ë•Œ ë‘ ê°’ì˜ ë³€í™”ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦°ë‹¤.\n",
                        "- Positive(ì–‘ì„±), Negative(ìŒì„±) ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì˜ ê°•ê±´í•¨(robust)ì„ í‰ê°€í•œë‹¤.  \n",
                        "- **AUC Score**\n",
                        "- ROC Curveì˜ ê²°ê³¼ë¥¼ ì ìˆ˜í™”(ìˆ˜ì¹˜í™”) í•˜ëŠ” í•¨ìˆ˜ë¡œ ROC Curve ì•„ë˜ìª½ ë©´ì ì„ ê³„ì‚°í•œë‹¤.\n",
                        "- 0 ~ 1 ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ë‚˜ì˜¤ë©° í´ìˆ˜ë¡ ì¢‹ë‹¤.\n",
                        "- AUC Scoreê°’ì´ í¬ë ¤ë©´(1ì— ê°€ê¹Œìš´ ê°’) ì„ê³„ê°’ì´ í´ ë•Œ FPRì€ ì‘ê³ , TPRì˜ ê°’ì€ ì»¤ì•¼ í•œë‹¤. FPRì´ ì‘ë‹¤ëŠ” ê²ƒì€ Negative ì˜ ë¶„ë¥˜í–ˆë‹¤ëŠ” ê²ƒì´ê³  TPRì´ í¬ë‹¤ëŠ” ê²ƒì€ Positiveë¥¼ ì˜ ë¶„ë¥˜ í–ˆë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ë‘˜ì— ëŒ€í•œ ë¶„ë¥˜ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n",
                        "- **AUC ì ìˆ˜ê¸°ì¤€**\n",
                        "- 1.0 ~ 0.9 : ì•„ì£¼ ì¢‹ìŒ\n",
                        "- 0.9 ~ 0.8 : ì¢‹ìŒ\n",
                        "- 0.8 ~ 0.7 : ê´œì°®ì€ ëª¨ë¸\n",
                        "- 0.7 ~ 0.6 : ì˜ë¯¸ëŠ” ìˆìœ¼ë‚˜ ì¢‹ì€ ëª¨ë¸ì€ ì•„ë‹˜\n",
                        "- 0.6 ~ 0.5 : ì¢‹ì§€ ì•Šì€ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 83, 'code_snippet': '', 'chunk_index': 23, 'original_score': 0.37777752910214546}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.37777752910214546\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„  \n",
                        "**ë¶„ë¥˜ì˜ ê²½ìš° Precision(ì •ë°€ë„)ê°€ ì¤‘ìš”í•œ ê²½ìš°ì™€ Recall(ì¬í˜„ìœ¨) ì¤‘ìš”í•œ ì—…ë¬´ê°€ ìˆë‹¤.**' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 55, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.3447059532424226}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3447059532424226\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "PR Curve(Precision Recall Curve-ì •ë°€ë„ ì¬í˜„ìœ¨ ê³¡ì„ )ì™€ AP Score(Average Precision Score)\n",
                        "- ì´ì§„ë¶„ë¥˜ì˜ í‰ê°€ì§€í‘œ.\n",
                        "- Positive í™•ë¥ ì„ ì´ìš©í•´ class(0, 1)ì„ ê²°ì •í•  ë•Œ ì„ê³„ê°’ì´ ë³€í™”ì— ë”°ë¥¸ ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ë³€í™”ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì¬í˜„ìœ¨ì´ ë³€í™”í•  ë•Œ ì •ë°€ë„ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í‰ê°€í•œë‹¤.\n",
                        "- Precisionê³¼ Recall ê°’ë“¤ì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì˜ Positiveì— ëŒ€í•œ ì„±ëŠ¥ì˜ ê°•ê±´í•¨(robust)ë¥¼ í‰ê°€í•œë‹¤.\n",
                        "- **Xì¶•ì— ì¬í˜„ìœ¨, Yì¶•ì— ì •ë°€ë„ë¥¼** ë†“ê³  ì„ê³„ê°’ì´ 1 â†’ 0 ë³€í™”í• ë•Œ ë‘ ê°’ì˜ ë³€í™”ë¥¼ ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦°ë‹¤.\n",
                        "- AP Score\n",
                        "- PR Curveì˜ ì„±ëŠ¥í‰ê°€ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ì ìˆ˜(ìˆ«ì)ë¡œ í‰ê°€í•œê²ƒ.\n",
                        "- PR Curveì˜ ì„ ì•„ë˜ ë©´ì ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ ë†’ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 75, 'code_snippet': '```python\\n# DecisionTreeì˜ PrecisionRecall ì»¤ë¸Œ ê·¸ë¦¬ê¸° + AP Score ê³„ì‚°.\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nimport matplotlib.pyplot as plt\\n\\n# ëª¨ë¸ì´ ì¶”ì •í•œ positive í™•ë¥ ì„ ì¡°íšŒ\\ntest_proba_tree = tree.predict_proba(X_test)[:, 1]  # DecisionTree\\ntest_proba_rfc = rfc.predict_proba(X_test)[:, 1]    # RandomForest\\n```\\n\\n```python\\n# ap score ë¡œ ëª¨ë¸ì„ í‰ê°€\\ntree_ap = average_precision_score(y_test, test_proba_tree)  # (yì •ë‹µ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì–‘ì„±ì¼ í™•ë¥ )\\nrfc_ap = average_precision_score(y_test, test_proba_rfc)\\nprint(\"DecisionTree Average Precision Score:\", tree_ap)\\nprint(\"RandomForest Average Precision Score:\", rfc_ap)\\n```\\n\\n```python\\n# ì‹œê°í™”\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\n# í•˜ë‚˜ì˜ Figure ë‘ê°œ subplotìœ¼ë¡œ ê·¸ë¦¬ê¸°.\\nfig = plt.figure(figsize=(12, 6))\\nax1 = fig.add_subplot(1, 2, 1) # DecisionTree\\nax2 = fig.add_subplot(1, 2, 2) # RandomForest\\n\\ndisp_tree = PrecisionRecallDisplay(  #PrecisionRecall Curveë¥¼ ì‹œê°í™”í•˜ëŠ” í´ìŠ¤ìŠ¤\\n    precisions1, # precisionê°’ë“¤\\n    recalls1,    # recallê°’ë“¤\\n    average_precision=tree_ap  # AP score\\n)\\ndisp_tree.plot(ax=ax1) # ì‹œê°í™”\\n\\ndisp_rfc = PrecisionRecallDisplay(precisions2, recalls2, average_precision=rfc_ap)\\ndisp_rfc.plot(ax=ax2)\\n\\nax1.set_title(\"DecisionTree\")\\nax2.set_title(\"Random Forest\")\\nplt.show()\\n```\\n\\n```python\\n# í•˜ë‚˜ì˜ subplotì— ê°™ì´ ê·¸ë¦¬ê¸°.\\n\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\nax = plt.gca()\\n\\ndisp_tree = PrecisionRecallDisplay(\\n    precisions1, \\n    recalls1, \\n    average_precision=tree_ap, \\n    estimator_name=\"DecisionTree\" # label ì§€ì •\\n)\\n\\ndisp_tree.plot(ax=ax)\\n\\ndisp_rfc = PrecisionRecallDisplay(\\n    precisions2, \\n    recalls2, \\n    average_precision=rfc_ap, \\n    estimator_name=\"Random Forest\"\\n)\\ndisp_rfc.plot(ax=ax)\\n\\nplt.title(\"Precision Recall Curve\")\\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\\nplt.show()\\n```', 'chunk_index': 22, 'original_score': 0.37599788996533384}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.37599788996533384\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì •ë°€ë„ê°€ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Negative ë°ì´í„°ë¥¼ Positive ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FP(False Positive)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´ˆì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ íŒì •' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 57, 'code_snippet': '', 'chunk_index': 15, 'original_score': 0.6471182531407298}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6471182531407298\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.7035706297235023}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7035706297235023\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ìŒì„±(Negative) ì˜ˆì¸¡ë ¥ ì¸¡ì • í‰ê°€ì§€í‘œ\n",
                        "- **Specificity(íŠ¹ì´ë„)**\n",
                        "- ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Negative(ìŒì„±)ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡ í•œ ê²ƒì˜ ë¹„ìœ¨\n",
                        "- **TNR**(True Negative Rate) ë¼ê³ ë„ í•œë‹¤.\n",
                        "- **Fall out(ìœ„ì–‘ì„±ë¥ )**\n",
                        "- ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Positive(ì–‘ì„±)ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨. `1 - íŠ¹ì´ë„`\n",
                        "- **FPR** (False Positive Rate) ë¼ê³ ë„ í•œë‹¤.\n",
                        "- Fall Out(FPR) = \\cfrac{FP}{TN+FP}' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 23, 'code_snippet': '', 'chunk_index': 10, 'original_score': 0.6423409120000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6423409120000001\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì •ë°€ë„ê°€ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Negative ë°ì´í„°ë¥¼ Positive ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FP(False Positive)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´ˆì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ìŠ¤íŒ¸ë©”ì¼ íŒì •' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 57, 'code_snippet': '', 'chunk_index': 15, 'original_score': 0.6471182531407298}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6471182531407298\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\n",
                        "- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.\n",
                        "- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.\n",
                        "- ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸°ì ë°œ ëª¨ë¸' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 56, 'code_snippet': '', 'chunk_index': 14, 'original_score': 0.7035706297235023}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7035706297235023\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ìŒì„±(Negative) ì˜ˆì¸¡ë ¥ ì¸¡ì • í‰ê°€ì§€í‘œ\n",
                        "- **Specificity(íŠ¹ì´ë„)**\n",
                        "- ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Negative(ìŒì„±)ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡ í•œ ê²ƒì˜ ë¹„ìœ¨\n",
                        "- **TNR**(True Negative Rate) ë¼ê³ ë„ í•œë‹¤.\n",
                        "- **Fall out(ìœ„ì–‘ì„±ë¥ )**\n",
                        "- ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Positive(ì–‘ì„±)ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨. `1 - íŠ¹ì´ë„`\n",
                        "- **FPR** (False Positive Rate) ë¼ê³ ë„ í•œë‹¤.\n",
                        "- Fall Out(FPR) = \\cfrac{FP}{TN+FP}' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 23, 'code_snippet': '', 'chunk_index': 10, 'original_score': 0.6423409120000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6423409120000001\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_LLM_LangChain_ê°œìš”]\n",
                        "\n",
                        "Agents  \n",
                        "- ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥(tool)ë“¤ì„ ì¤€ë¹„í•˜ê³  í•˜ê³  ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°›ìœ¼ë©´ LLM ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì ë‹¹í•œ toolë“¤ì„ ì„ íƒí•´ ì²˜ë¦¬í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ Chainê³¼ í•¨ê»˜ Langchainì˜ ì¤‘ìš”í•œ ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ë‹¤.\n",
                        "- Langchainì— ë‹¤ì–‘í•œ toolë“¤ì´ ì œê³µë˜ê³  ìˆê³  ê°œë°œìê°€ í•„ìš”í•œ toolë“¤ì„ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.\n",
                        "- ê¸°ë³¸ì ì¸ ì‘ì—…ë“¤ì´ textë¥¼ ì´ìš©í•œ LLMê³¼ì˜ ìƒí˜¸ ì‘ìš©ì´ì—ˆë‹¤ë©´, agentëŠ” ì™¸ë¶€ ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ì™€ ìƒí˜¸ì‘ìš©ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.' metadata={'source': '', 'source_file': '01_LLM_LangChain_ê°œìš”.ipynb', 'lecture_title': '01_LLM_LangChain_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 12, 'code_snippet': '', 'chunk_index': 10, 'original_score': 0.68838132}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.68838132\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "Agent ê°œìš”\n",
                        "> **AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë‘ë‡Œë¡œ ì‚¬ìš©í•˜ì—¬, ëª©í‘œë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  ì™¸ë¶€ ë„êµ¬ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° ì‹¤ì œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**  \n",
                        "- AgentëŠ” **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ê³¼ ë‹¤ì–‘í•œ ë„êµ¬(Tool)ë¥¼ ê²°í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”ì²­ì„ ìŠ¤ìŠ¤ë¡œ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ëœ ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**\n",
                        "ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ì±—ë´‡ì´ â€œì§ˆë¬¸ â†’ ë‹µë³€â€ êµ¬ì¡°ë¡œ ë™ì‘í•œë‹¤ë©´,\n",
                        "- AgentëŠ” **ëª©í‘œ ì„¤ì • â†’ íŒë‹¨ â†’ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜**ì˜ ì „ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n",
                        "- AgentëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ììœ¨ì ìœ¼ë¡œ **ì™¸ë¶€ í™˜ê²½(ë„êµ¬, API, ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ ë“±)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ê³  ì‹¤ì œ í–‰ë™ì„ ìˆ˜í–‰í•œë‹¤.**\n",
                        "ì´ë•Œ Agentì˜ **í•µì‹¬ì ì¸ ì˜ì‚¬ ê²°ì •ê³¼ ì¶”ë¡  ê³¼ì •ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹´ë‹¹**í•˜ë©°, **ì‚¬ëŒì˜ ê°œì…ì€ ìµœì†Œí™”**í•œë‹¤.  \n",
                        "- ì¦‰, AgentëŠ” ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë§í•´ì£¼ëŠ” ì¡´ì¬ê°€ ì•„ë‹ˆë¼, **\"ë¬¸ì œë¥¼ ì´í•´í•˜ê³ , í•´ê²° ë°©ë²•ì„ ê³„íší•˜ê³ , ì§ì ‘ ì‹¤í–‰ê¹Œì§€ ë‹´ë‹¹í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì‘ì—… ìˆ˜í–‰ì\"ì´ë‹¤.**  \n",
                        "- AI ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ë•Œ, **ì›Œí¬í”Œë¡œìš°**(**Workflow**)ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì‹¤í–‰ ë‹¨ê³„ê°€ ê³ ì •ì ì¸ êµ¬í˜„ ë°©ì‹ì´ë¼ë©´, **ì—ì´ì „íŠ¸**(**Agent**)ëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ìƒí™©ì„ íŒë‹¨í•˜ì—¬ í–‰ë™ì„ ê²°ì •Â·ì‹¤í–‰í•˜ëŠ” ììœ¨ì ì¸ ë°©ì‹ì´ë‹¤.  \n",
                        "Agentì˜ ì£¼ìš” íŠ¹ì§•  \n",
                        "1. **ììœ¨ì„±** (Autonomy)\n",
                        "- AgentëŠ” **ì‚¬ì „ ì •ì˜ëœ ê·œì¹™**(Rule-based)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , LLMì„ í†µí•´ í˜„ì¬ ìƒí™©ì„ ì¸ì§€í•˜ê³  ì¶”ë¡ í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ê²°ì •ì„ ë‚´ë¦¬ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íší•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¦‰, ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìˆ˜í–‰ ë‹¨ê³„ë¥¼ ì§€ì‹œí•˜ì§€ ì•Šì•„ë„, AgentëŠ” **í˜„ì¬ ìƒí™©ì„ í•´ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•œë‹¤.**  \n",
                        "2. **ëª©í‘œ ì§€í–¥ì„±** (Goal-Oriented)\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6174378560687982}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6174378560687982\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)\n",
                        "- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\n",
                        "- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \n",
                        "4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\n",
                        "- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\n",
                        "- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.6271469745191735}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6271469745191735\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_LLM_LangChain_ê°œìš”]\n",
                        "\n",
                        "Agents  \n",
                        "- ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥(tool)ë“¤ì„ ì¤€ë¹„í•˜ê³  í•˜ê³  ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°›ìœ¼ë©´ LLM ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì ë‹¹í•œ toolë“¤ì„ ì„ íƒí•´ ì²˜ë¦¬í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ Chainê³¼ í•¨ê»˜ Langchainì˜ ì¤‘ìš”í•œ ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ë‹¤.\n",
                        "- Langchainì— ë‹¤ì–‘í•œ toolë“¤ì´ ì œê³µë˜ê³  ìˆê³  ê°œë°œìê°€ í•„ìš”í•œ toolë“¤ì„ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.\n",
                        "- ê¸°ë³¸ì ì¸ ì‘ì—…ë“¤ì´ textë¥¼ ì´ìš©í•œ LLMê³¼ì˜ ìƒí˜¸ ì‘ìš©ì´ì—ˆë‹¤ë©´, agentëŠ” ì™¸ë¶€ ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ì™€ ìƒí˜¸ì‘ìš©ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.' metadata={'source': '', 'source_file': '01_LLM_LangChain_ê°œìš”.ipynb', 'lecture_title': '01_LLM_LangChain_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 12, 'code_snippet': '', 'chunk_index': 10, 'original_score': 0.68833638}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.68833638\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "Agent ê°œìš”\n",
                        "> **AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë‘ë‡Œë¡œ ì‚¬ìš©í•˜ì—¬, ëª©í‘œë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  ì™¸ë¶€ ë„êµ¬ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° ì‹¤ì œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**  \n",
                        "- AgentëŠ” **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ê³¼ ë‹¤ì–‘í•œ ë„êµ¬(Tool)ë¥¼ ê²°í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”ì²­ì„ ìŠ¤ìŠ¤ë¡œ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ëœ ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**\n",
                        "ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ì±—ë´‡ì´ â€œì§ˆë¬¸ â†’ ë‹µë³€â€ êµ¬ì¡°ë¡œ ë™ì‘í•œë‹¤ë©´,\n",
                        "- AgentëŠ” **ëª©í‘œ ì„¤ì • â†’ íŒë‹¨ â†’ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜**ì˜ ì „ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n",
                        "- AgentëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ììœ¨ì ìœ¼ë¡œ **ì™¸ë¶€ í™˜ê²½(ë„êµ¬, API, ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ ë“±)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ê³  ì‹¤ì œ í–‰ë™ì„ ìˆ˜í–‰í•œë‹¤.**\n",
                        "ì´ë•Œ Agentì˜ **í•µì‹¬ì ì¸ ì˜ì‚¬ ê²°ì •ê³¼ ì¶”ë¡  ê³¼ì •ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹´ë‹¹**í•˜ë©°, **ì‚¬ëŒì˜ ê°œì…ì€ ìµœì†Œí™”**í•œë‹¤.  \n",
                        "- ì¦‰, AgentëŠ” ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë§í•´ì£¼ëŠ” ì¡´ì¬ê°€ ì•„ë‹ˆë¼, **\"ë¬¸ì œë¥¼ ì´í•´í•˜ê³ , í•´ê²° ë°©ë²•ì„ ê³„íší•˜ê³ , ì§ì ‘ ì‹¤í–‰ê¹Œì§€ ë‹´ë‹¹í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì‘ì—… ìˆ˜í–‰ì\"ì´ë‹¤.**  \n",
                        "- AI ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ë•Œ, **ì›Œí¬í”Œë¡œìš°**(**Workflow**)ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì‹¤í–‰ ë‹¨ê³„ê°€ ê³ ì •ì ì¸ êµ¬í˜„ ë°©ì‹ì´ë¼ë©´, **ì—ì´ì „íŠ¸**(**Agent**)ëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ìƒí™©ì„ íŒë‹¨í•˜ì—¬ í–‰ë™ì„ ê²°ì •Â·ì‹¤í–‰í•˜ëŠ” ììœ¨ì ì¸ ë°©ì‹ì´ë‹¤.  \n",
                        "Agentì˜ ì£¼ìš” íŠ¹ì§•  \n",
                        "1. **ììœ¨ì„±** (Autonomy)\n",
                        "- AgentëŠ” **ì‚¬ì „ ì •ì˜ëœ ê·œì¹™**(Rule-based)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , LLMì„ í†µí•´ í˜„ì¬ ìƒí™©ì„ ì¸ì§€í•˜ê³  ì¶”ë¡ í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ê²°ì •ì„ ë‚´ë¦¬ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íší•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¦‰, ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìˆ˜í–‰ ë‹¨ê³„ë¥¼ ì§€ì‹œí•˜ì§€ ì•Šì•„ë„, AgentëŠ” **í˜„ì¬ ìƒí™©ì„ í•´ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•œë‹¤.**  \n",
                        "2. **ëª©í‘œ ì§€í–¥ì„±** (Goal-Oriented)\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6172888160687982}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6172888160687982\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)\n",
                        "- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\n",
                        "- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \n",
                        "4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\n",
                        "- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\n",
                        "- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.6270613605191735}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6270613605191735\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ReAct íŒ¨í„´\n",
                        "- Agent êµ¬í˜„ì˜ ë°”íƒ•ì´ ë˜ëŠ” ì´ë¡ .  \n",
                        "1. ê°œìš”  \n",
                        "ReActëŠ” **Reasoning**(ì¶”ë¡ )ê³¼ **Acting**(í–‰ë™)ì„ ê²°í•©í•œ AI Agentì˜ í”„ë¡¬í”„íŒ… íŒ¨í„´ì´ë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¨ìˆœíˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ (Reasoning)í•˜ê³  ê·¸ì— ë”°ë¼ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ë©°(Acting), ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³ (Observation) ë‹¤ìŒ ì¶”ë¡ ì— ë°˜ì˜í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” Agent ì„¤ê³„ ë°©ì‹ì´ë‹¤.  \n",
                        "ReActì˜ í•µì‹¬ ê°œë…  \n",
                        "ì „í†µì ì¸ í”„ë¡¬í”„íŒ… ë°©ì‹ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì¦‰ì‹œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. í•˜ì§€ë§Œ **ReAct íŒ¨í„´**ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤:  \n",
                        "```\n",
                        "ì§ˆë¬¸ â†’ ìƒê°(Thought) â†’ í–‰ë™(Action) â†’ ê´€ì°°(Observation) â†’ ìƒê° â†’ í–‰ë™ â†’ ê´€ì°° â†’ ... â†’ ë‹µë³€\n",
                        "```  \n",
                        "- ì˜ˆ)\n",
                        "- ì§ˆë¬¸: \"í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ì™€ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ì•Œë ¤ì¤˜\":  \n",
                        "- **Thought(ìƒê°)**: ë‚ ì”¨ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë‚ ì”¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ê² ë‹¤\n",
                        "- **Action(í–‰ë™)**: WeatherAPI.get_weather(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: ê¸°ì˜¨ 15ë„, ë§‘ìŒ\n",
                        "- **Thought(ìƒê°)**: ì´ì œ ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë„ í•„ìš”í•˜ë‹¤\n",
                        "- **Action(í–‰ë™)**: AirQualityAPI.get_pm(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: PM10 ë†ë„ 20ã/ã¥, ì¢‹ìŒ\n",
                        "- **Answer(ë‹µë³€)**: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ê¸°ì˜¨ 15ë„ë¡œ ë§‘ê³ , ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 30ã/ã¥ë¡œ ì¢‹ì€ ìƒíƒœì´ë‹¤  \n",
                        "ì´ì²˜ëŸ¼ ReActëŠ” Agentê°€ **ì‚¬ê³  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ë©´ì„œ** ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.  \n",
                        "ReActì˜ ì¥ì   \n",
                        "1. **íˆ¬ëª…ì„±**: Agentì˜ ì‚¬ê³  ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ì´ ìš©ì´í•˜ë‹¤\n",
                        "2. **ì •í™•ì„±**: ì™¸ë¶€ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ í™˜ê°(hallucination)ì„ ì¤„ì¸ë‹¤\n",
                        "3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤\n",
                        "4. **í•´ì„ ê°€ëŠ¥ì„±**: ê° ë‹¨ê³„ì˜ ì¶”ë¡  ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.394449101934236}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.394449101934236\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 13_Agent_langgraph]\n",
                        "\n",
                        "3. **ë°ì´í„° ë³´ì •(ì¤‘ê°„ ê²°ê³¼ë¬¼ ìˆ˜ì •)**\n",
                        "- AIê°€ ì¶”ì¶œí•œ ì •ë³´ê°€ ì• ë§¤í•˜ê±°ë‚˜ ì¶”ê°€ ë°ì´í„°ê°€ í•„ìš”í•œ ê²½ìš° ì‚¬ëŒì´ ì§ì ‘ ìˆ˜ì •\n",
                        "- ì˜ˆ: AIê°€ ì‘ì„±í•œ ë¸”ë¡œê·¸ ì´ˆì•ˆì„ ì‚¬ëŒì´ ê²€í† í•˜ê³  ìˆ˜ì •í•œ ë’¤, ë‹¤ì‹œ AIì—ê²Œ ë°œí–‰ì„ ë§¡ê¸¸ ë•Œ.\n",
                        "4. **ì •ì±…/ê·œì œ ì¤€ìˆ˜ ê²€ì¦**\n",
                        "- ì˜ˆ: ê¸ˆìœµ, ì˜ë£Œ, ë²•ë¥  ë“± ë„ë©”ì¸ì—ì„œ ì‚¬ëŒ ê²€í†  í•„ìˆ˜\n",
                        "5. **ë„êµ¬ ì‹¤í–‰ ì „ í™•ì¸**\n",
                        "- ì˜ˆ: ì´ë©”ì¼ ë°œì†¡, DB ì‚­ì œ, ê²°ì œ ì‹¤í–‰ ë“± AIê°€ ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ê¸° ì „ ì‚¬ëŒ ìŠ¹ì¸\n",
                        "6. **í•™ìŠµ í”¼ë“œë°±**\n",
                        "- ì˜ˆ: ëª¨ë¸ì´ í‹€ë ¸ì„ ê²½ìš° ì‚¬ëŒì´ ì§ì ‘ í”¼ë“œë°±ì„ ì£¼ê³  íë¦„ ì¬ì¡°ì •  \n",
                        "ê´€ë ¨ ì£¼ìš” API  \n",
                        "| API | ì—­í•  | ì‚¬ìš© ëª©ì  |\n",
                        "| ---------------------------- | ------------------------------------------- | ----------------------------------------- |\n",
                        "| `interrupt()` | í˜„ì¬ ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì™¸ë¶€ ì…ë ¥ì„ ëŒ€ê¸°| ì‚¬ëŒì˜ ê²€í† /ì…ë ¥ì„ ë°›ê¸° ìœ„í•´ ê·¸ë˜í”„ë¥¼ ë©ˆì¶¤ |\n",
                        "| `MemorySaver / Checkpointer` | ì‹¤í–‰ ì§€ì ì„ ì €ì¥í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼ í•œë‹¤. | ì´ì „ ìƒíƒœë¡œ ë˜ëŒë¦¬ê±°ë‚˜ ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ì‹¤í–‰ í•˜ê¸° ìœ„í•´ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.|\n",
                        "| `invoke()`/`stream()` | ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘ ë˜ëŠ” ì¬ê°œ | HITL ì…ë ¥ í›„ ë‹¤ì‹œ ì‹¤í–‰ |\n",
                        "| `Command(resume=...)` | ì¬ê°œ ëª…ë ¹ | interruptë¡œ ë©ˆì¶˜ ê³³ì— ì‚¬ëŒì˜ ì…ë ¥ê°’ì„ ì „ë‹¬í•˜ë©° ì‹¤í–‰ì„ ë‹¤ì‹œ ì‹œì‘í•¨ |\n",
                        "| `graph.update_state()` | ìƒíƒœ(State) ìˆ˜ì • | interrupt ë˜ì–´ ìˆëŠ” ë™ì•ˆ ì‚¬ëŒì´ ì§ì ‘ stateë¥¼ ìˆ˜ì • í•  ìˆ˜ìˆê²Œ í•œë‹¤.|  \n",
                        "ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ëŠ” ë‘ê°€ì§€ ë°©ì‹\n",
                        "- ë…¸ë“œì—ì„œ `interrupt()` ë¥¼ í˜¸ì¶œ í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ ë©ˆì¶˜ë‹¤. (Dynamic Interrupt)\n",
                        "- Graph ì»´íŒŒì¼ì‹œ `interrupt_before` ì„¤ì •ìœ¼ë¡œ ë…¸ë“œë¥¼ ì§€ì •í•˜ì—¬ íŠ¹ì • ë…¸ë“œê°€ ì‹¤í–‰í•˜ê¸° ì§ì „ì— ë¬´ì¡°ê±´ ë©ˆì¶”ê²Œ í•œë‹¤. (Static Interrupt)\n",
                        "- **ìµœì‹  ë²„ì „ì—ì„œëŠ” Dynamic Interrupt ë°©ì‹ì„ ì„ í˜¸í•œë‹¤.**' metadata={'source': '', 'source_file': '13_Agent_langgraph.ipynb', 'lecture_title': '13_Agent_langgraph', 'cell_type': 'markdown', 'cell_index': 51, 'code_snippet': '```python\\n# # ì •ë³´ ì…ë ¥ ë°›ê¸°\\n# from typing import Annotated\\nfrom langchain_openai import ChatOpenAI\\nfrom typing_extensions import TypedDict\\n\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.types import Command, interrupt\\n\\nfrom IPython.display import Image, display\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\nllm = ChatOpenAI(model=\"gpt-5-mini\")\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n    name: str\\n    age: int\\n    tall: float\\n\\ndef check_info(state: State):\\n    \"\"\"ëˆ„ë½ëœ ì •ë³´ í™•ì¸ í›„ ì…ë ¥ë°›ëŠ”ë‹¤.\"\"\"\\n    name = state.get(\"name\", None)\\n    age = state.get(\"age\", None)\\n    tall = state.get(\"tall\", None)\\n\\n    if name is None or name == \"\":\\n        name = interrupt({\"query\":\"ì´ë¦„ ì…ë ¥í•˜ì„¸ìš”\"})   \\n    if age is None:\\n        age = interrupt({\"query\":\"ë‚˜ì´ ì…ë ¥í•˜ì„¸ìš”\"})\\n    if tall is None:\\n        tall = interrupt({\"query\":\"í‚¤ ì…ë ¥í•˜ì„¸ìš”\"})\\n    return {\"name\": name, \"age\": age, \"tall\": tall}\\n\\ndef write_info(state: State):\\n    print(state)\\n    res = llm.invoke(f\"ì´ë¦„: {state[\"name\"]}, ë‚˜ì´: {state[\"age\"]}, í‚¤: {state[\"tall\"]}\\\\n ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ë§ì„ 100ê¸€ì ë‚´ë¡œ ì‘ì„±í•´ì¤˜.\")\\n\\n    return {\"messages\": [res], \"name\": None, \"age\":None, \"tall\":None }\\n```\\n\\n```python\\ngraph_builder = StateGraph(State)\\ngraph_builder.add_node(\"check_info\", check_info)\\ngraph_builder.add_node(\"write_info\", write_info)\\n\\ngraph_builder.add_edge(START, \"check_info\")\\ngraph_builder.add_edge(\"check_info\", \"write_info\")\\ngraph_builder.add_edge(\"write_info\", END)\\n\\nmemory = InMemorySaver()\\ngraph = graph_builder.compile(checkpointer=memory) # HITL ì‚¬ìš©í•˜ë ¤ë©´ ì²´í¬í¬ì¸í„°(ë©”ëª¨ë¦¬) ì‚¬ìš©í•´ì•¼ í•œë‹¤.\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"chat-1\"}}\\nres = graph.invoke({\"name\":\"í™ê¸¸ë™\"}, config=config) \\n# interrupt() ë¡œ ì¤‘ë‹¨ë˜ì„œ ëë‚˜ë©´ ê·¸ ì‹œì ì˜ state + \"__interrupt__ item\" ë¥¼ ë°˜í™˜í•œë‹¤.\\n# resì— \"__interrupt__\"  keyê°€ ìˆìœ¼ë©´ ì¸í„°ëŸ½íŠ¸ëœ ìƒíƒœ, ì—†ìœ¼ë©´ ì¢…ë£Œë˜ì„œ ëë‚œ ê²ƒ.\\nres\\n```\\n\\n```python\\nif res[\\'__interrupt__\\']:\\n    print(res[\\'__interrupt__\\'][-1].value[\"query\"])\\n```\\n\\n```python\\nprint(graph.get_state(config))\\nprint(graph.get_state(config).next)  # ë‹¤ìŒì— ì¼í•  ë…¸ë“œì˜ ì´ë¦„. check_infoì—ì„œ ì¤‘ë‹¨ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¬ê°œë˜ì—ˆì„ ë•Œ  \\n                                     # ì¼í•  ë…¸ë“œëŠ” check_info.\\n```\\n\\n```python\\ncmd = Command(resume=30) # interrupt ë¥¼ ì¬ê°œ. resume=valueê°€ ì „ë‹¬ëœë‹¤.\\nres = graph.invoke(cmd, config=config)\\nres\\n```\\n\\n```python\\n# # ì „ì²´ flow ì‘ì„±\\n# config = {\"configurable\": {\"thread_id\": \"chat-2\"}} \\nres = graph.invoke({}, config=config)\\n\\nwhile \\'__interrupt__\\' in res: # dictì—ì„œ interrupt í‚¤ê°€ ìˆëŠ”ì§€\\n    human_input = input(res[\\'__interrupt__\\'][-1].value[\"query\"]+\":\")\\n    # print(human_input)\\n    human_command = Command(resume=human_input)\\n    res = graph.invoke(human_command, config=config)\\n\\n# # ì™„ë£Œí›„\\nprint(res[\\'messages\\'][-1].content)\\n```\\n\\n```python\\ngraph.get_state(config).next  # ì¢…ë£Œ ë˜ì—ˆìœ¼ë¯€ë¡œ nextê°€ ì—†ë‹¤.\\n```\\n\\n```python\\n# # ì‚¬ìš©ì ìŠ¹ì¸ ì˜ˆì œ\\n# í˜¸í…” ì˜ˆì•½ ì˜ˆì œ (HOTEL)\\n# from langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import interrupt, Command\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom typing_extensions import TypedDict\\n\\nfrom IPython.display import Image, display\\n\\n# ìƒíƒœ ì •ì˜\\nclass AgentState(TypedDict):\\n    hotel: str           # ê²€ìƒ‰í•œ í˜¸í…”\\n    booked: str | None   # ì˜ˆì•½ ì™„ë£Œ í˜¸í…”\\n    decision: str | None # ì˜ˆì•½ ê²°ì • ì—¬ë¶€. approve / reject\\n    is_decision: bool    # ì˜ˆì•½ ê²°ì • í–ˆëŠ”ì§€ ì—¬ë¶€\\n\\n# 1) í˜¸í…” ê²€ìƒ‰ ë…¸ë“œ (í† ì´: í•­ìƒ 5ê°œ ì¤‘ ëœë¤ 1ê°œ ì„ íƒ)\\ndef search_hotels(state: AgentState):\\n    import random\\n    hotel_list = [\\n        \"í•˜ì•¼íŠ¸ í˜¸í…”\",\\n        \"ë¡¯ë° í˜¸í…”\",\\n        \"ì‹ ë¼ í˜¸í…”\",\\n        \"íŒŒë¼ë‹¤ì´ìŠ¤ í˜¸í…”\",\\n        \"ì¡°ì„  í˜¸í…”\"\\n    ]\\n    picked = random.choice(hotel_list)  # í˜¸í…” ê²€ìƒ‰ tool ì´ìš©\\n\\n    print(\"\\\\n[í˜¸í…” ê²€ìƒ‰ ê²°ê³¼]\")\\n    print(f\"- {picked}\")\\n    return {\"hotel\": picked, \"booked\": None, \"is_decision\": False}\\n\\n# 2) ìŠ¹ì¸ ìš”ì²­ ë…¸ë“œ ê°€ì´ë“œ í”„ë¦°íŠ¸\\ndef ask_approval_print(state: AgentState):\\n    is_decision = state.get(\"is_decision\", None)\\n    if not is_decision:\\n        searched_hotel = state[\"hotel\"]\\n        print(f\"\\\\ní˜¸í…” \\'{searched_hotel}\\' ì˜ˆì•½ì„ ì§„í–‰í• ê¹Œìš”?\")\\n        print(\"ì‚¬ìš©ì ì…ë ¥: approve / reject\")\\n\\n# 3) ì‚¬ìš©ìì—ê²Œ ì˜ˆì•½ ìŠ¹ì¸ ìš”ì²­ ë…¸ë“œ\\ndef ask_approval(state: AgentState):\\n    # HITL ì¤‘ë‹¨ â†’ ì¸ê°„ ê²°ì • ëŒ€ê¸°\\n    decision = interrupt(f\"{state[\"hotel\"]} ì˜ˆì•½ ìŠ¹ì¸ ëŒ€ê¸°\")\\n    return {\"decision\": decision, \"is_decision\": True}  # ê²°ì • í–ˆìŒì„ í‘œì‹œ\\n\\n# 3) ì˜ˆì•½ ì²˜ë¦¬ ë…¸ë“œ\\ndef book_hotel(state: AgentState):\\n    current = state[\"hotel\"]\\n    state[\"booked\"] = current\\n    print(f\"\\\\n[ì˜ˆì•½ ì™„ë£Œ]\")\\n    print(f\"í˜¸í…”: {current}\")\\n    return state\\n\\n# 4) ì¡°ê±´ ë¶„ê¸° ì—£ì§€ í•¨ìˆ˜ - ë¶„ê¸° ì¡°ê±´ ë¼ìš°íŒ… í•¨ìˆ˜\\ndef route_after_approval(state: AgentState):\\n    if state.get(\"decision\") == \"approve\":\\n        return \"book_hotel\"  # ì˜ˆì•½ ë…¸ë“œ\\n    return \"search_hotels\"   # í˜¸í…” ê²€ìƒ‰ ë…¸ë“œ\\n```\\n\\n```python\\n# ê·¸ë˜í”„ ë¹Œë”\\nbuilder = StateGraph(AgentState)\\n\\n# ë…¸ë“œ ì„¤ì •\\nbuilder.add_node(\"search_hotels\", search_hotels)\\nbuilder.add_node(\"ask_approval\", ask_approval)\\nbuilder.add_node(\"ask_approval_print\", ask_approval_print)\\nbuilder.add_node(\"book_hotel\", book_hotel)\\n\\n# ì—£ì§€ ì •ì˜\\nbuilder.add_edge(START, \"search_hotels\")\\nbuilder.add_edge(\"search_hotels\", \"ask_approval_print\")\\nbuilder.add_edge(\"ask_approval_print\", \"ask_approval\") # ì¸í„°ëŸ½íŠ¸\\nbuilder.add_conditional_edges(\"ask_approval\", route_after_approval, {\\n    \"search_hotels\": \"search_hotels\",\\n    \"book_hotel\": \"book_hotel\"\\n})\\nbuilder.add_edge(\"book_hotel\", END)\\n\\nmemory = InMemorySaver()\\ngraph = builder.compile(checkpointer=memory)\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"reservation-1\"}}\\nres = graph.invoke({}, config=config)\\n\\nwhile \\'__interrupt__\\' in res: # dictì—ì„œ interrupt í‚¤ê°€ ìˆëŠ”ì§€\\n    user_input = input(res[\\'__interrupt__\\'][-1].value)\\n    human_command = Command(resume=user_input)  # ìŠ¹ì¸\\n    res = graph.invoke(human_command, config=config)\\n```', 'chunk_index': 13, 'original_score': 0.3579542625632212}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3579542625632212\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ê°œìš”]\n",
                        "\n",
                        "íŒŒì´ì¬ ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰  \n",
                        "REPL ë°©ì‹\n",
                        "- (Read-Eval-Print loop)\n",
                        "- í•œ ëª…ë ¹ë¬¸ ë‹¨ìœ„ë¡œ ì‹¤í–‰í•œë‹¤.\n",
                        "- ëª…ë ¹ë¬¸ì˜ ì²˜ë¦¬ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶œë ¥í•´ì¤€ë‹¤.\n",
                        "- Python shell (íŒŒì´ì¬ ê¸°ë³¸ shell) ë˜ëŠ” ipython shell ì„ ì´ìš©í•œë‹¤.\n",
                        "- í„°ë¯¸ë„(ëª…ë ¹í”„ë¡¬í”„íŠ¸)ì—ì„œ `python` ì‹¤í–‰' metadata={'source': '', 'source_file': '01_ê°œìš”.ipynb', 'lecture_title': '01_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 9, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.380265116}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.380265116\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ReAct íŒ¨í„´\n",
                        "- Agent êµ¬í˜„ì˜ ë°”íƒ•ì´ ë˜ëŠ” ì´ë¡ .  \n",
                        "1. ê°œìš”  \n",
                        "ReActëŠ” **Reasoning**(ì¶”ë¡ )ê³¼ **Acting**(í–‰ë™)ì„ ê²°í•©í•œ AI Agentì˜ í”„ë¡¬í”„íŒ… íŒ¨í„´ì´ë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¨ìˆœíˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ (Reasoning)í•˜ê³  ê·¸ì— ë”°ë¼ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ë©°(Acting), ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³ (Observation) ë‹¤ìŒ ì¶”ë¡ ì— ë°˜ì˜í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” Agent ì„¤ê³„ ë°©ì‹ì´ë‹¤.  \n",
                        "ReActì˜ í•µì‹¬ ê°œë…  \n",
                        "ì „í†µì ì¸ í”„ë¡¬í”„íŒ… ë°©ì‹ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì¦‰ì‹œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. í•˜ì§€ë§Œ **ReAct íŒ¨í„´**ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤:  \n",
                        "```\n",
                        "ì§ˆë¬¸ â†’ ìƒê°(Thought) â†’ í–‰ë™(Action) â†’ ê´€ì°°(Observation) â†’ ìƒê° â†’ í–‰ë™ â†’ ê´€ì°° â†’ ... â†’ ë‹µë³€\n",
                        "```  \n",
                        "- ì˜ˆ)\n",
                        "- ì§ˆë¬¸: \"í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ì™€ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ì•Œë ¤ì¤˜\":  \n",
                        "- **Thought(ìƒê°)**: ë‚ ì”¨ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë‚ ì”¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ê² ë‹¤\n",
                        "- **Action(í–‰ë™)**: WeatherAPI.get_weather(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: ê¸°ì˜¨ 15ë„, ë§‘ìŒ\n",
                        "- **Thought(ìƒê°)**: ì´ì œ ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë„ í•„ìš”í•˜ë‹¤\n",
                        "- **Action(í–‰ë™)**: AirQualityAPI.get_pm(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: PM10 ë†ë„ 20ã/ã¥, ì¢‹ìŒ\n",
                        "- **Answer(ë‹µë³€)**: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ê¸°ì˜¨ 15ë„ë¡œ ë§‘ê³ , ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 30ã/ã¥ë¡œ ì¢‹ì€ ìƒíƒœì´ë‹¤  \n",
                        "ì´ì²˜ëŸ¼ ReActëŠ” Agentê°€ **ì‚¬ê³  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ë©´ì„œ** ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.  \n",
                        "ReActì˜ ì¥ì   \n",
                        "1. **íˆ¬ëª…ì„±**: Agentì˜ ì‚¬ê³  ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ì´ ìš©ì´í•˜ë‹¤\n",
                        "2. **ì •í™•ì„±**: ì™¸ë¶€ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ í™˜ê°(hallucination)ì„ ì¤„ì¸ë‹¤\n",
                        "3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤\n",
                        "4. **í•´ì„ ê°€ëŠ¥ì„±**: ê° ë‹¨ê³„ì˜ ì¶”ë¡  ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.39463945793423605}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.39463945793423605\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 13_Agent_langgraph]\n",
                        "\n",
                        "3. **ë°ì´í„° ë³´ì •(ì¤‘ê°„ ê²°ê³¼ë¬¼ ìˆ˜ì •)**\n",
                        "- AIê°€ ì¶”ì¶œí•œ ì •ë³´ê°€ ì• ë§¤í•˜ê±°ë‚˜ ì¶”ê°€ ë°ì´í„°ê°€ í•„ìš”í•œ ê²½ìš° ì‚¬ëŒì´ ì§ì ‘ ìˆ˜ì •\n",
                        "- ì˜ˆ: AIê°€ ì‘ì„±í•œ ë¸”ë¡œê·¸ ì´ˆì•ˆì„ ì‚¬ëŒì´ ê²€í† í•˜ê³  ìˆ˜ì •í•œ ë’¤, ë‹¤ì‹œ AIì—ê²Œ ë°œí–‰ì„ ë§¡ê¸¸ ë•Œ.\n",
                        "4. **ì •ì±…/ê·œì œ ì¤€ìˆ˜ ê²€ì¦**\n",
                        "- ì˜ˆ: ê¸ˆìœµ, ì˜ë£Œ, ë²•ë¥  ë“± ë„ë©”ì¸ì—ì„œ ì‚¬ëŒ ê²€í†  í•„ìˆ˜\n",
                        "5. **ë„êµ¬ ì‹¤í–‰ ì „ í™•ì¸**\n",
                        "- ì˜ˆ: ì´ë©”ì¼ ë°œì†¡, DB ì‚­ì œ, ê²°ì œ ì‹¤í–‰ ë“± AIê°€ ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ê¸° ì „ ì‚¬ëŒ ìŠ¹ì¸\n",
                        "6. **í•™ìŠµ í”¼ë“œë°±**\n",
                        "- ì˜ˆ: ëª¨ë¸ì´ í‹€ë ¸ì„ ê²½ìš° ì‚¬ëŒì´ ì§ì ‘ í”¼ë“œë°±ì„ ì£¼ê³  íë¦„ ì¬ì¡°ì •  \n",
                        "ê´€ë ¨ ì£¼ìš” API  \n",
                        "| API | ì—­í•  | ì‚¬ìš© ëª©ì  |\n",
                        "| ---------------------------- | ------------------------------------------- | ----------------------------------------- |\n",
                        "| `interrupt()` | í˜„ì¬ ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì™¸ë¶€ ì…ë ¥ì„ ëŒ€ê¸°| ì‚¬ëŒì˜ ê²€í† /ì…ë ¥ì„ ë°›ê¸° ìœ„í•´ ê·¸ë˜í”„ë¥¼ ë©ˆì¶¤ |\n",
                        "| `MemorySaver / Checkpointer` | ì‹¤í–‰ ì§€ì ì„ ì €ì¥í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼ í•œë‹¤. | ì´ì „ ìƒíƒœë¡œ ë˜ëŒë¦¬ê±°ë‚˜ ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ì‹¤í–‰ í•˜ê¸° ìœ„í•´ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.|\n",
                        "| `invoke()`/`stream()` | ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘ ë˜ëŠ” ì¬ê°œ | HITL ì…ë ¥ í›„ ë‹¤ì‹œ ì‹¤í–‰ |\n",
                        "| `Command(resume=...)` | ì¬ê°œ ëª…ë ¹ | interruptë¡œ ë©ˆì¶˜ ê³³ì— ì‚¬ëŒì˜ ì…ë ¥ê°’ì„ ì „ë‹¬í•˜ë©° ì‹¤í–‰ì„ ë‹¤ì‹œ ì‹œì‘í•¨ |\n",
                        "| `graph.update_state()` | ìƒíƒœ(State) ìˆ˜ì • | interrupt ë˜ì–´ ìˆëŠ” ë™ì•ˆ ì‚¬ëŒì´ ì§ì ‘ stateë¥¼ ìˆ˜ì • í•  ìˆ˜ìˆê²Œ í•œë‹¤.|  \n",
                        "ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ëŠ” ë‘ê°€ì§€ ë°©ì‹\n",
                        "- ë…¸ë“œì—ì„œ `interrupt()` ë¥¼ í˜¸ì¶œ í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ ë©ˆì¶˜ë‹¤. (Dynamic Interrupt)\n",
                        "- Graph ì»´íŒŒì¼ì‹œ `interrupt_before` ì„¤ì •ìœ¼ë¡œ ë…¸ë“œë¥¼ ì§€ì •í•˜ì—¬ íŠ¹ì • ë…¸ë“œê°€ ì‹¤í–‰í•˜ê¸° ì§ì „ì— ë¬´ì¡°ê±´ ë©ˆì¶”ê²Œ í•œë‹¤. (Static Interrupt)\n",
                        "- **ìµœì‹  ë²„ì „ì—ì„œëŠ” Dynamic Interrupt ë°©ì‹ì„ ì„ í˜¸í•œë‹¤.**' metadata={'source': '', 'source_file': '13_Agent_langgraph.ipynb', 'lecture_title': '13_Agent_langgraph', 'cell_type': 'markdown', 'cell_index': 51, 'code_snippet': '```python\\n# # ì •ë³´ ì…ë ¥ ë°›ê¸°\\n# from typing import Annotated\\nfrom langchain_openai import ChatOpenAI\\nfrom typing_extensions import TypedDict\\n\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.types import Command, interrupt\\n\\nfrom IPython.display import Image, display\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\nllm = ChatOpenAI(model=\"gpt-5-mini\")\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n    name: str\\n    age: int\\n    tall: float\\n\\ndef check_info(state: State):\\n    \"\"\"ëˆ„ë½ëœ ì •ë³´ í™•ì¸ í›„ ì…ë ¥ë°›ëŠ”ë‹¤.\"\"\"\\n    name = state.get(\"name\", None)\\n    age = state.get(\"age\", None)\\n    tall = state.get(\"tall\", None)\\n\\n    if name is None or name == \"\":\\n        name = interrupt({\"query\":\"ì´ë¦„ ì…ë ¥í•˜ì„¸ìš”\"})   \\n    if age is None:\\n        age = interrupt({\"query\":\"ë‚˜ì´ ì…ë ¥í•˜ì„¸ìš”\"})\\n    if tall is None:\\n        tall = interrupt({\"query\":\"í‚¤ ì…ë ¥í•˜ì„¸ìš”\"})\\n    return {\"name\": name, \"age\": age, \"tall\": tall}\\n\\ndef write_info(state: State):\\n    print(state)\\n    res = llm.invoke(f\"ì´ë¦„: {state[\"name\"]}, ë‚˜ì´: {state[\"age\"]}, í‚¤: {state[\"tall\"]}\\\\n ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ë§ì„ 100ê¸€ì ë‚´ë¡œ ì‘ì„±í•´ì¤˜.\")\\n\\n    return {\"messages\": [res], \"name\": None, \"age\":None, \"tall\":None }\\n```\\n\\n```python\\ngraph_builder = StateGraph(State)\\ngraph_builder.add_node(\"check_info\", check_info)\\ngraph_builder.add_node(\"write_info\", write_info)\\n\\ngraph_builder.add_edge(START, \"check_info\")\\ngraph_builder.add_edge(\"check_info\", \"write_info\")\\ngraph_builder.add_edge(\"write_info\", END)\\n\\nmemory = InMemorySaver()\\ngraph = graph_builder.compile(checkpointer=memory) # HITL ì‚¬ìš©í•˜ë ¤ë©´ ì²´í¬í¬ì¸í„°(ë©”ëª¨ë¦¬) ì‚¬ìš©í•´ì•¼ í•œë‹¤.\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"chat-1\"}}\\nres = graph.invoke({\"name\":\"í™ê¸¸ë™\"}, config=config) \\n# interrupt() ë¡œ ì¤‘ë‹¨ë˜ì„œ ëë‚˜ë©´ ê·¸ ì‹œì ì˜ state + \"__interrupt__ item\" ë¥¼ ë°˜í™˜í•œë‹¤.\\n# resì— \"__interrupt__\"  keyê°€ ìˆìœ¼ë©´ ì¸í„°ëŸ½íŠ¸ëœ ìƒíƒœ, ì—†ìœ¼ë©´ ì¢…ë£Œë˜ì„œ ëë‚œ ê²ƒ.\\nres\\n```\\n\\n```python\\nif res[\\'__interrupt__\\']:\\n    print(res[\\'__interrupt__\\'][-1].value[\"query\"])\\n```\\n\\n```python\\nprint(graph.get_state(config))\\nprint(graph.get_state(config).next)  # ë‹¤ìŒì— ì¼í•  ë…¸ë“œì˜ ì´ë¦„. check_infoì—ì„œ ì¤‘ë‹¨ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¬ê°œë˜ì—ˆì„ ë•Œ  \\n                                     # ì¼í•  ë…¸ë“œëŠ” check_info.\\n```\\n\\n```python\\ncmd = Command(resume=30) # interrupt ë¥¼ ì¬ê°œ. resume=valueê°€ ì „ë‹¬ëœë‹¤.\\nres = graph.invoke(cmd, config=config)\\nres\\n```\\n\\n```python\\n# # ì „ì²´ flow ì‘ì„±\\n# config = {\"configurable\": {\"thread_id\": \"chat-2\"}} \\nres = graph.invoke({}, config=config)\\n\\nwhile \\'__interrupt__\\' in res: # dictì—ì„œ interrupt í‚¤ê°€ ìˆëŠ”ì§€\\n    human_input = input(res[\\'__interrupt__\\'][-1].value[\"query\"]+\":\")\\n    # print(human_input)\\n    human_command = Command(resume=human_input)\\n    res = graph.invoke(human_command, config=config)\\n\\n# # ì™„ë£Œí›„\\nprint(res[\\'messages\\'][-1].content)\\n```\\n\\n```python\\ngraph.get_state(config).next  # ì¢…ë£Œ ë˜ì—ˆìœ¼ë¯€ë¡œ nextê°€ ì—†ë‹¤.\\n```\\n\\n```python\\n# # ì‚¬ìš©ì ìŠ¹ì¸ ì˜ˆì œ\\n# í˜¸í…” ì˜ˆì•½ ì˜ˆì œ (HOTEL)\\n# from langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import interrupt, Command\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom typing_extensions import TypedDict\\n\\nfrom IPython.display import Image, display\\n\\n# ìƒíƒœ ì •ì˜\\nclass AgentState(TypedDict):\\n    hotel: str           # ê²€ìƒ‰í•œ í˜¸í…”\\n    booked: str | None   # ì˜ˆì•½ ì™„ë£Œ í˜¸í…”\\n    decision: str | None # ì˜ˆì•½ ê²°ì • ì—¬ë¶€. approve / reject\\n    is_decision: bool    # ì˜ˆì•½ ê²°ì • í–ˆëŠ”ì§€ ì—¬ë¶€\\n\\n# 1) í˜¸í…” ê²€ìƒ‰ ë…¸ë“œ (í† ì´: í•­ìƒ 5ê°œ ì¤‘ ëœë¤ 1ê°œ ì„ íƒ)\\ndef search_hotels(state: AgentState):\\n    import random\\n    hotel_list = [\\n        \"í•˜ì•¼íŠ¸ í˜¸í…”\",\\n        \"ë¡¯ë° í˜¸í…”\",\\n        \"ì‹ ë¼ í˜¸í…”\",\\n        \"íŒŒë¼ë‹¤ì´ìŠ¤ í˜¸í…”\",\\n        \"ì¡°ì„  í˜¸í…”\"\\n    ]\\n    picked = random.choice(hotel_list)  # í˜¸í…” ê²€ìƒ‰ tool ì´ìš©\\n\\n    print(\"\\\\n[í˜¸í…” ê²€ìƒ‰ ê²°ê³¼]\")\\n    print(f\"- {picked}\")\\n    return {\"hotel\": picked, \"booked\": None, \"is_decision\": False}\\n\\n# 2) ìŠ¹ì¸ ìš”ì²­ ë…¸ë“œ ê°€ì´ë“œ í”„ë¦°íŠ¸\\ndef ask_approval_print(state: AgentState):\\n    is_decision = state.get(\"is_decision\", None)\\n    if not is_decision:\\n        searched_hotel = state[\"hotel\"]\\n        print(f\"\\\\ní˜¸í…” \\'{searched_hotel}\\' ì˜ˆì•½ì„ ì§„í–‰í• ê¹Œìš”?\")\\n        print(\"ì‚¬ìš©ì ì…ë ¥: approve / reject\")\\n\\n# 3) ì‚¬ìš©ìì—ê²Œ ì˜ˆì•½ ìŠ¹ì¸ ìš”ì²­ ë…¸ë“œ\\ndef ask_approval(state: AgentState):\\n    # HITL ì¤‘ë‹¨ â†’ ì¸ê°„ ê²°ì • ëŒ€ê¸°\\n    decision = interrupt(f\"{state[\"hotel\"]} ì˜ˆì•½ ìŠ¹ì¸ ëŒ€ê¸°\")\\n    return {\"decision\": decision, \"is_decision\": True}  # ê²°ì • í–ˆìŒì„ í‘œì‹œ\\n\\n# 3) ì˜ˆì•½ ì²˜ë¦¬ ë…¸ë“œ\\ndef book_hotel(state: AgentState):\\n    current = state[\"hotel\"]\\n    state[\"booked\"] = current\\n    print(f\"\\\\n[ì˜ˆì•½ ì™„ë£Œ]\")\\n    print(f\"í˜¸í…”: {current}\")\\n    return state\\n\\n# 4) ì¡°ê±´ ë¶„ê¸° ì—£ì§€ í•¨ìˆ˜ - ë¶„ê¸° ì¡°ê±´ ë¼ìš°íŒ… í•¨ìˆ˜\\ndef route_after_approval(state: AgentState):\\n    if state.get(\"decision\") == \"approve\":\\n        return \"book_hotel\"  # ì˜ˆì•½ ë…¸ë“œ\\n    return \"search_hotels\"   # í˜¸í…” ê²€ìƒ‰ ë…¸ë“œ\\n```\\n\\n```python\\n# ê·¸ë˜í”„ ë¹Œë”\\nbuilder = StateGraph(AgentState)\\n\\n# ë…¸ë“œ ì„¤ì •\\nbuilder.add_node(\"search_hotels\", search_hotels)\\nbuilder.add_node(\"ask_approval\", ask_approval)\\nbuilder.add_node(\"ask_approval_print\", ask_approval_print)\\nbuilder.add_node(\"book_hotel\", book_hotel)\\n\\n# ì—£ì§€ ì •ì˜\\nbuilder.add_edge(START, \"search_hotels\")\\nbuilder.add_edge(\"search_hotels\", \"ask_approval_print\")\\nbuilder.add_edge(\"ask_approval_print\", \"ask_approval\") # ì¸í„°ëŸ½íŠ¸\\nbuilder.add_conditional_edges(\"ask_approval\", route_after_approval, {\\n    \"search_hotels\": \"search_hotels\",\\n    \"book_hotel\": \"book_hotel\"\\n})\\nbuilder.add_edge(\"book_hotel\", END)\\n\\nmemory = InMemorySaver()\\ngraph = builder.compile(checkpointer=memory)\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"reservation-1\"}}\\nres = graph.invoke({}, config=config)\\n\\nwhile \\'__interrupt__\\' in res: # dictì—ì„œ interrupt í‚¤ê°€ ìˆëŠ”ì§€\\n    user_input = input(res[\\'__interrupt__\\'][-1].value)\\n    human_command = Command(resume=user_input)  # ìŠ¹ì¸\\n    res = graph.invoke(human_command, config=config)\\n```', 'chunk_index': 13, 'original_score': 0.35986383456322124}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.35986383456322124\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ê°œìš”]\n",
                        "\n",
                        "íŒŒì´ì¬ ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰  \n",
                        "REPL ë°©ì‹\n",
                        "- (Read-Eval-Print loop)\n",
                        "- í•œ ëª…ë ¹ë¬¸ ë‹¨ìœ„ë¡œ ì‹¤í–‰í•œë‹¤.\n",
                        "- ëª…ë ¹ë¬¸ì˜ ì²˜ë¦¬ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶œë ¥í•´ì¤€ë‹¤.\n",
                        "- Python shell (íŒŒì´ì¬ ê¸°ë³¸ shell) ë˜ëŠ” ipython shell ì„ ì´ìš©í•œë‹¤.\n",
                        "- í„°ë¯¸ë„(ëª…ë ¹í”„ë¡¬í”„íŠ¸)ì—ì„œ `python` ì‹¤í–‰' metadata={'source': '', 'source_file': '01_ê°œìš”.ipynb', 'lecture_title': '01_ê°œìš”', 'cell_type': 'markdown', 'cell_index': 9, 'code_snippet': '', 'chunk_index': 8, 'original_score': 0.382996232}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.382996232\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)\n",
                        "- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\n",
                        "- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \n",
                        "4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\n",
                        "- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\n",
                        "- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.4820141805499232}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4820141805499232\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "Agent ê°œìš”\n",
                        "> **AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë‘ë‡Œë¡œ ì‚¬ìš©í•˜ì—¬, ëª©í‘œë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  ì™¸ë¶€ ë„êµ¬ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° ì‹¤ì œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**  \n",
                        "- AgentëŠ” **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ê³¼ ë‹¤ì–‘í•œ ë„êµ¬(Tool)ë¥¼ ê²°í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”ì²­ì„ ìŠ¤ìŠ¤ë¡œ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ëœ ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**\n",
                        "ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ì±—ë´‡ì´ â€œì§ˆë¬¸ â†’ ë‹µë³€â€ êµ¬ì¡°ë¡œ ë™ì‘í•œë‹¤ë©´,\n",
                        "- AgentëŠ” **ëª©í‘œ ì„¤ì • â†’ íŒë‹¨ â†’ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜**ì˜ ì „ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n",
                        "- AgentëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ììœ¨ì ìœ¼ë¡œ **ì™¸ë¶€ í™˜ê²½(ë„êµ¬, API, ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ ë“±)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ê³  ì‹¤ì œ í–‰ë™ì„ ìˆ˜í–‰í•œë‹¤.**\n",
                        "ì´ë•Œ Agentì˜ **í•µì‹¬ì ì¸ ì˜ì‚¬ ê²°ì •ê³¼ ì¶”ë¡  ê³¼ì •ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹´ë‹¹**í•˜ë©°, **ì‚¬ëŒì˜ ê°œì…ì€ ìµœì†Œí™”**í•œë‹¤.  \n",
                        "- ì¦‰, AgentëŠ” ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë§í•´ì£¼ëŠ” ì¡´ì¬ê°€ ì•„ë‹ˆë¼, **\"ë¬¸ì œë¥¼ ì´í•´í•˜ê³ , í•´ê²° ë°©ë²•ì„ ê³„íší•˜ê³ , ì§ì ‘ ì‹¤í–‰ê¹Œì§€ ë‹´ë‹¹í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì‘ì—… ìˆ˜í–‰ì\"ì´ë‹¤.**  \n",
                        "- AI ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ë•Œ, **ì›Œí¬í”Œë¡œìš°**(**Workflow**)ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì‹¤í–‰ ë‹¨ê³„ê°€ ê³ ì •ì ì¸ êµ¬í˜„ ë°©ì‹ì´ë¼ë©´, **ì—ì´ì „íŠ¸**(**Agent**)ëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ìƒí™©ì„ íŒë‹¨í•˜ì—¬ í–‰ë™ì„ ê²°ì •Â·ì‹¤í–‰í•˜ëŠ” ììœ¨ì ì¸ ë°©ì‹ì´ë‹¤.  \n",
                        "Agentì˜ ì£¼ìš” íŠ¹ì§•  \n",
                        "1. **ììœ¨ì„±** (Autonomy)\n",
                        "- AgentëŠ” **ì‚¬ì „ ì •ì˜ëœ ê·œì¹™**(Rule-based)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , LLMì„ í†µí•´ í˜„ì¬ ìƒí™©ì„ ì¸ì§€í•˜ê³  ì¶”ë¡ í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ê²°ì •ì„ ë‚´ë¦¬ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íší•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¦‰, ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìˆ˜í–‰ ë‹¨ê³„ë¥¼ ì§€ì‹œí•˜ì§€ ì•Šì•„ë„, AgentëŠ” **í˜„ì¬ ìƒí™©ì„ í•´ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•œë‹¤.**  \n",
                        "2. **ëª©í‘œ ì§€í–¥ì„±** (Goal-Oriented)\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4817651561681796}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4817651561681796\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ReAct íŒ¨í„´\n",
                        "- Agent êµ¬í˜„ì˜ ë°”íƒ•ì´ ë˜ëŠ” ì´ë¡ .  \n",
                        "1. ê°œìš”  \n",
                        "ReActëŠ” **Reasoning**(ì¶”ë¡ )ê³¼ **Acting**(í–‰ë™)ì„ ê²°í•©í•œ AI Agentì˜ í”„ë¡¬í”„íŒ… íŒ¨í„´ì´ë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¨ìˆœíˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ (Reasoning)í•˜ê³  ê·¸ì— ë”°ë¼ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ë©°(Acting), ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³ (Observation) ë‹¤ìŒ ì¶”ë¡ ì— ë°˜ì˜í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” Agent ì„¤ê³„ ë°©ì‹ì´ë‹¤.  \n",
                        "ReActì˜ í•µì‹¬ ê°œë…  \n",
                        "ì „í†µì ì¸ í”„ë¡¬í”„íŒ… ë°©ì‹ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì¦‰ì‹œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. í•˜ì§€ë§Œ **ReAct íŒ¨í„´**ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤:  \n",
                        "```\n",
                        "ì§ˆë¬¸ â†’ ìƒê°(Thought) â†’ í–‰ë™(Action) â†’ ê´€ì°°(Observation) â†’ ìƒê° â†’ í–‰ë™ â†’ ê´€ì°° â†’ ... â†’ ë‹µë³€\n",
                        "```  \n",
                        "- ì˜ˆ)\n",
                        "- ì§ˆë¬¸: \"í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ì™€ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ì•Œë ¤ì¤˜\":  \n",
                        "- **Thought(ìƒê°)**: ë‚ ì”¨ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë‚ ì”¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ê² ë‹¤\n",
                        "- **Action(í–‰ë™)**: WeatherAPI.get_weather(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: ê¸°ì˜¨ 15ë„, ë§‘ìŒ\n",
                        "- **Thought(ìƒê°)**: ì´ì œ ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë„ í•„ìš”í•˜ë‹¤\n",
                        "- **Action(í–‰ë™)**: AirQualityAPI.get_pm(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: PM10 ë†ë„ 20ã/ã¥, ì¢‹ìŒ\n",
                        "- **Answer(ë‹µë³€)**: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ê¸°ì˜¨ 15ë„ë¡œ ë§‘ê³ , ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 30ã/ã¥ë¡œ ì¢‹ì€ ìƒíƒœì´ë‹¤  \n",
                        "ì´ì²˜ëŸ¼ ReActëŠ” Agentê°€ **ì‚¬ê³  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ë©´ì„œ** ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.  \n",
                        "ReActì˜ ì¥ì   \n",
                        "1. **íˆ¬ëª…ì„±**: Agentì˜ ì‚¬ê³  ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ì´ ìš©ì´í•˜ë‹¤\n",
                        "2. **ì •í™•ì„±**: ì™¸ë¶€ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ í™˜ê°(hallucination)ì„ ì¤„ì¸ë‹¤\n",
                        "3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤\n",
                        "4. **í•´ì„ ê°€ëŠ¥ì„±**: ê° ë‹¨ê³„ì˜ ì¶”ë¡  ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.4204311008993576}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4204311008993576\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ë‹¤ì–‘í•œ Agent í™œìš© ì‚¬ë¡€  \n",
                        "- **ë¯¸êµ­ êµ­ì„¸ì²­(IRS) ì„¸ë¬´ì²˜ë¦¬ì— ë„ì…ëœ AI ì—ì´ì „íŠ¸**\n",
                        "- ë¯¸êµ­ êµ­ì„¸ì²­(IRS)ì€ Salesforce Agentforce ê¸°ë°˜ AI ì—ì´ì „íŠ¸ë¥¼ ë„ì…í•´ ì„¸ë¬´ ì—…ë¬´ì—ì„œì˜ ë³´ì¡°Â·ë¶„ì„ ê¸°ëŠ¥ì„ ê°•í™”í•˜ê³  ì—…ë¬´ íš¨ìœ¨ì„ ë†’ì´ê³  ìˆë‹¤.  \n",
                        "- **ê³ ê° ì¶”ì²œ ë° ê°œì¸í™”ëœ ì½˜í…ì¸  ì œê³µ**\n",
                        "- Netflix, Amazon ê°™ì€ OTT í”Œë«í¼ì€ ì‚¬ìš©ìì˜ ì·¨í–¥ ë³€í™”ì— ë”°ë¼ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì ì‘ì‹œí‚¤ëŠ” í•™ìŠµí˜• ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•´ ì½˜í…ì¸ /ì œí’ˆ ì¶”ì²œì„ ê°œì„ í•´ ë‚˜ê°„ë‹¤.  \n",
                        "- **ì˜ì—… ì§€ì›ìš© AI ì—ì´ì „íŠ¸**\n",
                        "- Oracleì€ ì˜ì—… ì „ë¬¸ê°€ë¥¼ ìœ„í•œ AI ì—ì´ì „íŠ¸ë¥¼ ë„ì…í•´ ê³ ê° ë¯¸íŒ… í›„ CRM ì—…ë°ì´íŠ¸, ê³ ê° ë°ì´í„° ë¶„ì„Â·ë³´ê³ ì„œ ìƒì„± ê°™ì€ ë°˜ë³µ ì‘ì—…ì„ ìë™í™”í•˜ê³  ìˆë‹¤.  \n",
                        "- **ë¬¼ë¥˜ ìµœì í™” ë° ë°°ì†¡ ê³„íš**\n",
                        "- Uber Freight, J.B. Hunt ë“±ì—ì„œëŠ” AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•´ ì‹¤ì‹œê°„ êµí†µÂ·ë‚ ì”¨ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°°ì†¡ ê²½ë¡œë¥¼ ê³„ì‚°í•œë‹¤.  \n",
                        "- **AWSì˜ í´ë¼ìš°ë“œ ìë™í™” ì—ì´ì „íŠ¸**\n",
                        "- Amazon Web ServicesëŠ” ë³´ì•ˆ, DevOps, ì¼ë°˜ ì‘ì—…ì„ ìë™í™”í•˜ëŠ” AI ì—ì´ì „íŠ¸ë“¤ì„ ì¶œì‹œí•˜ë©° í´ë¼ìš°ë“œ ì¸í”„ë¼ ìš´ì˜ê³¼ ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ê³  ìˆë‹¤.  \n",
                        "- **ì‚¼ì„±SDS AI ì—ì´ì „íŠ¸ í”Œë«í¼**\n",
                        "- ì‚¼ì„±SDSëŠ” **ì‚¬ìš©ì ê°œì… ì—†ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨Â·ë¬¸ì œ í•´ê²°ì´ ê°€ëŠ¥í•œ AI ì—ì´ì „íŠ¸ í”Œë«í¼ â€˜íŒ¨ë¸Œë¦­ìŠ¤(Fabrics)â€™**ë¥¼ ê³µê°œí–ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì—… ë‚´ë¶€ì—ì„œ ìë™í™”ëœ ì˜ì‚¬ê²°ì •ê³¼ ì‘ì—… ì‹¤í–‰ì„ ì§€ì›í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.45973011173610356}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.45973011173610356\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸]\n",
                        "\n",
                        "ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree )' metadata={'source': '', 'source_file': '09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸.ipynb', 'lecture_title': '09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.47639081194107447}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.47639081194107447\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)\n",
                        "- AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥, ì‚¬ì „ í•™ìŠµ ì •ë³´ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ **ì™¸ë¶€ ë„êµ¬ì™€ APIë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰**í•œë‹¤.\n",
                        "- ì´ë¥¼ í†µí•´ LLMì´ ì²˜ë¦¬í•  ìˆ˜ì—†ëŠ” **ìµœì‹  ì •ë³´ ê²€ìƒ‰, ë³µì¡í•œ ê³„ì‚°, ì™¸ë¶€ ì‹œìŠ¤í…œ ì œì–´**ë“±ê³¼ ê°™ì€ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ AgentëŠ” **\"ë§ë§Œ í•˜ëŠ” AI\"ê°€ ì•„ë‹ˆë¼ \"ì‹¤ì œë¡œ ì¼ì„ ì²˜ë¦¬í•˜ëŠ” AI\"ë¡œ ê¸°ëŠ¥í•œë‹¤.**  \n",
                        "4. **ì¸ê°„ ê°œì… ìµœì†Œí™”** (Human-in-the-loop ìµœì†Œí™”)\n",
                        "- AgentëŠ” **ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ ê²°ì •, í–‰ë™ ì‹¤í–‰ì˜ ëŒ€ë¶€ë¶„ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœë‹¤.**\n",
                        "- ì‚¬ëŒì€ **ì´ˆê¸° ëª©í‘œ ì œì‹œ, ê²°ê³¼ í™•ì¸ ë° ì‹¤í–‰ ìµœì¢… ìŠ¹ì¸** ì •ë„ì— ë§Œ ê°œì…í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.4820125425499232}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4820125425499232\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "Agent ê°œìš”\n",
                        "> **AgentëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ë‘ë‡Œë¡œ ì‚¬ìš©í•˜ì—¬, ëª©í‘œë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  ì™¸ë¶€ ë„êµ¬ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° ì‹¤ì œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**  \n",
                        "- AgentëŠ” **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ê³¼ ë‹¤ì–‘í•œ ë„êµ¬(Tool)ë¥¼ ê²°í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”ì²­ì„ ìŠ¤ìŠ¤ë¡œ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ëœ ì§€ëŠ¥í˜• ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.**\n",
                        "ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ì±—ë´‡ì´ â€œì§ˆë¬¸ â†’ ë‹µë³€â€ êµ¬ì¡°ë¡œ ë™ì‘í•œë‹¤ë©´,\n",
                        "- AgentëŠ” **ëª©í‘œ ì„¤ì • â†’ íŒë‹¨ â†’ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜**ì˜ ì „ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  \n",
                        "- AgentëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ììœ¨ì ìœ¼ë¡œ **ì™¸ë¶€ í™˜ê²½(ë„êµ¬, API, ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ ë“±)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ê³  ì‹¤ì œ í–‰ë™ì„ ìˆ˜í–‰í•œë‹¤.**\n",
                        "ì´ë•Œ Agentì˜ **í•µì‹¬ì ì¸ ì˜ì‚¬ ê²°ì •ê³¼ ì¶”ë¡  ê³¼ì •ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹´ë‹¹**í•˜ë©°, **ì‚¬ëŒì˜ ê°œì…ì€ ìµœì†Œí™”**í•œë‹¤.  \n",
                        "- ì¦‰, AgentëŠ” ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë§í•´ì£¼ëŠ” ì¡´ì¬ê°€ ì•„ë‹ˆë¼, **\"ë¬¸ì œë¥¼ ì´í•´í•˜ê³ , í•´ê²° ë°©ë²•ì„ ê³„íší•˜ê³ , ì§ì ‘ ì‹¤í–‰ê¹Œì§€ ë‹´ë‹¹í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì‘ì—… ìˆ˜í–‰ì\"ì´ë‹¤.**  \n",
                        "- AI ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ë•Œ, **ì›Œí¬í”Œë¡œìš°**(**Workflow**)ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì‹¤í–‰ ë‹¨ê³„ê°€ ê³ ì •ì ì¸ êµ¬í˜„ ë°©ì‹ì´ë¼ë©´, **ì—ì´ì „íŠ¸**(**Agent**)ëŠ” ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ìƒí™©ì„ íŒë‹¨í•˜ì—¬ í–‰ë™ì„ ê²°ì •Â·ì‹¤í–‰í•˜ëŠ” ììœ¨ì ì¸ ë°©ì‹ì´ë‹¤.  \n",
                        "Agentì˜ ì£¼ìš” íŠ¹ì§•  \n",
                        "1. **ììœ¨ì„±** (Autonomy)\n",
                        "- AgentëŠ” **ì‚¬ì „ ì •ì˜ëœ ê·œì¹™**(Rule-based)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , LLMì„ í†µí•´ í˜„ì¬ ìƒí™©ì„ ì¸ì§€í•˜ê³  ì¶”ë¡ í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ê²°ì •ì„ ë‚´ë¦¬ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íší•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì¦‰, ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìˆ˜í–‰ ë‹¨ê³„ë¥¼ ì§€ì‹œí•˜ì§€ ì•Šì•„ë„, AgentëŠ” **í˜„ì¬ ìƒí™©ì„ í•´ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•œë‹¤.**  \n",
                        "2. **ëª©í‘œ ì§€í–¥ì„±** (Goal-Oriented)\n",
                        "- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” **ë‹¨ìˆœí•œ ëŒ€í™” ìƒëŒ€ê°€ ì•„ë‹ˆë¼, íŠ¹ì • ëª©í‘œì™€ ë³µì¡í•œ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìµœì¢… ëª©í‘œì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ í–‰ë™í•œë‹¤.\n",
                        "- AgentëŠ” ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•  ë•Œ í•­ìƒ **\"ì´ í–‰ë™ì´ ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë˜ëŠ”ê°€\"ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ íŒë‹¨í•œë‹¤.  \n",
                        "3. **ë„êµ¬ í™œìš©** (Tool Utilization)' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4817701361681796}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4817701361681796\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ReAct íŒ¨í„´\n",
                        "- Agent êµ¬í˜„ì˜ ë°”íƒ•ì´ ë˜ëŠ” ì´ë¡ .  \n",
                        "1. ê°œìš”  \n",
                        "ReActëŠ” **Reasoning**(ì¶”ë¡ )ê³¼ **Acting**(í–‰ë™)ì„ ê²°í•©í•œ AI Agentì˜ í”„ë¡¬í”„íŒ… íŒ¨í„´ì´ë‹¤. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë‹¨ìˆœíˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ (Reasoning)í•˜ê³  ê·¸ì— ë”°ë¼ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ë©°(Acting), ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³ (Observation) ë‹¤ìŒ ì¶”ë¡ ì— ë°˜ì˜í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” Agent ì„¤ê³„ ë°©ì‹ì´ë‹¤.  \n",
                        "ReActì˜ í•µì‹¬ ê°œë…  \n",
                        "ì „í†µì ì¸ í”„ë¡¬í”„íŒ… ë°©ì‹ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì¦‰ì‹œ ë‹µë³€ì„ ìƒì„±í•œë‹¤. í•˜ì§€ë§Œ **ReAct íŒ¨í„´**ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤:  \n",
                        "```\n",
                        "ì§ˆë¬¸ â†’ ìƒê°(Thought) â†’ í–‰ë™(Action) â†’ ê´€ì°°(Observation) â†’ ìƒê° â†’ í–‰ë™ â†’ ê´€ì°° â†’ ... â†’ ë‹µë³€\n",
                        "```  \n",
                        "- ì˜ˆ)\n",
                        "- ì§ˆë¬¸: \"í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ì™€ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ì•Œë ¤ì¤˜\":  \n",
                        "- **Thought(ìƒê°)**: ë‚ ì”¨ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë‚ ì”¨ APIë¥¼ í˜¸ì¶œí•´ì•¼ê² ë‹¤\n",
                        "- **Action(í–‰ë™)**: WeatherAPI.get_weather(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: ê¸°ì˜¨ 15ë„, ë§‘ìŒ\n",
                        "- **Thought(ìƒê°)**: ì´ì œ ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë„ í•„ìš”í•˜ë‹¤\n",
                        "- **Action(í–‰ë™)**: AirQualityAPI.get_pm(\"ì„œìš¸\")\n",
                        "- **Observation(ê´€ì°°)**: PM10 ë†ë„ 20ã/ã¥, ì¢‹ìŒ\n",
                        "- **Answer(ë‹µë³€)**: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ê¸°ì˜¨ 15ë„ë¡œ ë§‘ê³ , ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 30ã/ã¥ë¡œ ì¢‹ì€ ìƒíƒœì´ë‹¤  \n",
                        "ì´ì²˜ëŸ¼ ReActëŠ” Agentê°€ **ì‚¬ê³  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ë©´ì„œ** ì™¸ë¶€ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.  \n",
                        "ReActì˜ ì¥ì   \n",
                        "1. **íˆ¬ëª…ì„±**: Agentì˜ ì‚¬ê³  ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ì´ ìš©ì´í•˜ë‹¤\n",
                        "2. **ì •í™•ì„±**: ì™¸ë¶€ ì§€ì‹ê³¼ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ í™˜ê°(hallucination)ì„ ì¤„ì¸ë‹¤\n",
                        "3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤\n",
                        "4. **í•´ì„ ê°€ëŠ¥ì„±**: ê° ë‹¨ê³„ì˜ ì¶”ë¡  ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.4204162208993576}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4204162208993576\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 12_Agent_ToolCalling]\n",
                        "\n",
                        "ë‹¤ì–‘í•œ Agent í™œìš© ì‚¬ë¡€  \n",
                        "- **ë¯¸êµ­ êµ­ì„¸ì²­(IRS) ì„¸ë¬´ì²˜ë¦¬ì— ë„ì…ëœ AI ì—ì´ì „íŠ¸**\n",
                        "- ë¯¸êµ­ êµ­ì„¸ì²­(IRS)ì€ Salesforce Agentforce ê¸°ë°˜ AI ì—ì´ì „íŠ¸ë¥¼ ë„ì…í•´ ì„¸ë¬´ ì—…ë¬´ì—ì„œì˜ ë³´ì¡°Â·ë¶„ì„ ê¸°ëŠ¥ì„ ê°•í™”í•˜ê³  ì—…ë¬´ íš¨ìœ¨ì„ ë†’ì´ê³  ìˆë‹¤.  \n",
                        "- **ê³ ê° ì¶”ì²œ ë° ê°œì¸í™”ëœ ì½˜í…ì¸  ì œê³µ**\n",
                        "- Netflix, Amazon ê°™ì€ OTT í”Œë«í¼ì€ ì‚¬ìš©ìì˜ ì·¨í–¥ ë³€í™”ì— ë”°ë¼ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì ì‘ì‹œí‚¤ëŠ” í•™ìŠµí˜• ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•´ ì½˜í…ì¸ /ì œí’ˆ ì¶”ì²œì„ ê°œì„ í•´ ë‚˜ê°„ë‹¤.  \n",
                        "- **ì˜ì—… ì§€ì›ìš© AI ì—ì´ì „íŠ¸**\n",
                        "- Oracleì€ ì˜ì—… ì „ë¬¸ê°€ë¥¼ ìœ„í•œ AI ì—ì´ì „íŠ¸ë¥¼ ë„ì…í•´ ê³ ê° ë¯¸íŒ… í›„ CRM ì—…ë°ì´íŠ¸, ê³ ê° ë°ì´í„° ë¶„ì„Â·ë³´ê³ ì„œ ìƒì„± ê°™ì€ ë°˜ë³µ ì‘ì—…ì„ ìë™í™”í•˜ê³  ìˆë‹¤.  \n",
                        "- **ë¬¼ë¥˜ ìµœì í™” ë° ë°°ì†¡ ê³„íš**\n",
                        "- Uber Freight, J.B. Hunt ë“±ì—ì„œëŠ” AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•´ ì‹¤ì‹œê°„ êµí†µÂ·ë‚ ì”¨ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°°ì†¡ ê²½ë¡œë¥¼ ê³„ì‚°í•œë‹¤.  \n",
                        "- **AWSì˜ í´ë¼ìš°ë“œ ìë™í™” ì—ì´ì „íŠ¸**\n",
                        "- Amazon Web ServicesëŠ” ë³´ì•ˆ, DevOps, ì¼ë°˜ ì‘ì—…ì„ ìë™í™”í•˜ëŠ” AI ì—ì´ì „íŠ¸ë“¤ì„ ì¶œì‹œí•˜ë©° í´ë¼ìš°ë“œ ì¸í”„ë¼ ìš´ì˜ê³¼ ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ê³  ìˆë‹¤.  \n",
                        "- **ì‚¼ì„±SDS AI ì—ì´ì „íŠ¸ í”Œë«í¼**\n",
                        "- ì‚¼ì„±SDSëŠ” **ì‚¬ìš©ì ê°œì… ì—†ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨Â·ë¬¸ì œ í•´ê²°ì´ ê°€ëŠ¥í•œ AI ì—ì´ì „íŠ¸ í”Œë«í¼ â€˜íŒ¨ë¸Œë¦­ìŠ¤(Fabrics)â€™**ë¥¼ ê³µê°œí–ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì—… ë‚´ë¶€ì—ì„œ ìë™í™”ëœ ì˜ì‚¬ê²°ì •ê³¼ ì‘ì—… ì‹¤í–‰ì„ ì§€ì›í•œë‹¤.' metadata={'source': '', 'source_file': '12_Agent_ToolCalling.ipynb', 'lecture_title': '12_Agent_ToolCalling', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.45968056373610355}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.45968056373610355\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸]\n",
                        "\n",
                        "ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree )' metadata={'source': '', 'source_file': '09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸.ipynb', 'lecture_title': '09_ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4764512019410745}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4764512019410745\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n"
                    ]
                }
            ],
            "source": [
                "#################################################################\n",
                "# 3. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
                "#################################################################\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from src.prompts import ANALYSIS_SYSTEM_PROMPT\n",
                "from langchain_core.runnables import RunnableLambda\n",
                "\n",
                "# LLM ì„¤ì •\n",
                "llm = ChatOpenAI(model=ConfigLLM.OPENAI_MODEL, temperature=0)\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸\n",
                "prompt = ChatPromptTemplate.from_template(ANALYSIS_SYSTEM_PROMPT)\n",
                "\n",
                "# ë¬¸ì„œ í¬ë§·íŒ…\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "# Chain êµ¬ì„±\n",
                "rag_chain = (\n",
                "    {\n",
                "        \"context\": RunnableLambda(advanced_retriever) | RunnableLambda(format_docs), \n",
                "        \"query\": RunnablePassthrough()\n",
                "    }\n",
                "    | prompt\n",
                "    | llm\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "# ë‹µë³€ ìƒì„± ë° Context ìˆ˜ì§‘\n",
                "answers = []\n",
                "contexts = []\n",
                "\n",
                "print(\"í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
                "for q in eval_data['question']:\n",
                "    # 1. ë¬¸ì„œ ê²€ìƒ‰\n",
                "    retrieved_docs = advanced_retriever(q)\n",
                "    context_text = [doc.page_content for doc in retrieved_docs]\n",
                "    contexts.append(context_text)\n",
                "    \n",
                "    # 2. ë‹µë³€ ìƒì„±\n",
                "    # Chainì„ ì§ì ‘ í˜¸ì¶œí•˜ë©´ contextë¥¼ ì¬ê²€ìƒ‰í•˜ë¯€ë¡œ(retrieverê°€ chain ì•ˆì— ìˆìŒ),\n",
                "    # íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¯¸ë¦¬ ê²€ìƒ‰í•œ docsë¥¼ ì£¼ì…í•˜ê±°ë‚˜, ê·¸ëƒ¥ chainì„ invokeí•©ë‹ˆë‹¤.\n",
                "    # ì—¬ê¸°ì„œëŠ” ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ chain invoke ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "    response = rag_chain.invoke(q)\n",
                "    answers.append(response)\n",
                "    # print(f\"Q: {q}\\nA: {response[:50]}...\\n\")\n",
                "\n",
                "# ë°ì´í„°ì…‹ì— ê²°ê³¼ ì¶”ê°€\n",
                "eval_data['answer'] = answers\n",
                "eval_data['contexts'] = contexts\n",
                "\n",
                "final_dataset = Dataset.from_dict(eval_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating:   8%|â–Š         | 5/60 [00:17<03:09,  3.44s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 23/60 [00:39<00:39,  1.08s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 34/60 [00:57<00:31,  1.20s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 35/60 [01:03<00:45,  1.82s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/60 [01:16<00:17,  1.15s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:42<00:00,  1.70s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========== í‰ê°€ ê²°ê³¼ ==========\n",
                        "{'context_precision': 0.9758, 'context_recall': 0.7944, 'faithfulness': 0.8450, 'answer_relevancy': 0.5700}\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_input</th>\n",
                            "      <th>retrieved_contexts</th>\n",
                            "      <th>reference_contexts</th>\n",
                            "      <th>response</th>\n",
                            "      <th>reference</th>\n",
                            "      <th>persona_name</th>\n",
                            "      <th>query_style</th>\n",
                            "      <th>query_length</th>\n",
                            "      <th>context_precision</th>\n",
                            "      <th>context_recall</th>\n",
                            "      <th>faithfulness</th>\n",
                            "      <th>answer_relevancy</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\n3. Data Preparation\\n- ë°ì´í„°...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.750000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.604906</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>íŠ¹ì„± ì¶”ì¶œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ë°ì´í„°ì—ì„œ ìœ ì˜...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ì›ë³¸ ë°ì´í„°ì—ì„œ...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.800</td>\n",
                            "      <td>0.434434</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹&nbsp;&nbsp;\\në¨¸ì‹ ëŸ¬ë‹(Machine ...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•&nbsp;&nbsp;\\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë‘˜ ë‹¤ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.666667</td>\n",
                            "      <td>0.500</td>\n",
                            "      <td>0.663401</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì–´ë–¤ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.800000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.587451</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ë­ì— ì“°ëŠ”ê±°ì•¼?</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nTODO: breast_cancer data ëª¨ë¸ë§...</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ìœ ë°©ì•” ì§„ë‹¨ì„ ìœ„í•œ ë°ì´í„°ë¡œ, ì£¼ë¡œ ì•”ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì˜ˆ...</td>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°ì´í„°...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.200000</td>\n",
                            "      <td>0.375</td>\n",
                            "      <td>0.570012</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nTODO: breast_cancer data ëª¨ë¸ë§...</td>\n",
                            "      <td>[[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ìœ ë°©ì•” ì§„ë‹¨ ê²°ê³¼ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì£¼ìš” ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.500</td>\n",
                            "      <td>0.515874</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>ê°•ì˜ì—ì„œ ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤„ ìˆ˜ ìˆë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 01_ë”¥ëŸ¬...</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]</td>\n",
                            "      <td>ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤: **ìˆœì „íŒŒ(...</td>\n",
                            "      <td>ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ì£¼ë¡œ ë°ì´í„°ì˜ ì…ë ¥, ê°€ì¤‘ì¹˜ ì¡°ì •, ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°,...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>0.804167</td>\n",
                            "      <td>0.600000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.692942</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 01_ë”¥ëŸ¬...</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ìˆœì „íŒŒ(forward propag...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.800000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.472914</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ë­ì—ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 04_ì²«ë²ˆ...</td>\n",
                            "      <td>[[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.600000</td>\n",
                            "      <td>0.500</td>\n",
                            "      <td>0.600810</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„&nbsp;&nbsp;\\n**ë¶„ë¥˜ì˜ ê²½ìš° Pre...</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...</td>\n",
                            "      <td>ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í–ˆ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negat...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.500000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.999999</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...</td>\n",
                            "      <td>ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì‹¤ì œ ì–‘ì„±ì¸ í™˜ìë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°(FN,...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì‹¤ì œ Positiv...</td>\n",
                            "      <td>Data Scientist</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.470403</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...</td>\n",
                            "      <td>[[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Negative ë°ì´í„°ë¥¼ ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Negative ë°ì´í„°ë¥¼ ...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>12</th>\n",
                            "      <td>LLMì´ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_LLM_LangChain_ê°œìš”]\\n\\nAgents&nbsp;&nbsp;\\n- ì‘ì—…ì„ ...</td>\n",
                            "      <td>[[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...</td>\n",
                            "      <td>LLMì€ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. LLMì€ Agentì˜ ë‘ë‡Œ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì€ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ìˆì–´ ìì—°ì–´...</td>\n",
                            "      <td>Computer Vision Engineer</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.782049</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>í–‰ë™ ì‹¤í–‰ì´ë€ ë¬´ì—‡ì¸ê°€?</td>\n",
                            "      <td>[[ê°•ì˜: 12_Agent_ToolCalling]\\n\\nReAct íŒ¨í„´\\n- Age...</td>\n",
                            "      <td>[[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...</td>\n",
                            "      <td>í–‰ë™ ì‹¤í–‰ì´ë€ AI Agentê°€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì™¸ë¶€ ë„êµ¬ë‚˜ APIë¥¼ í˜¸...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: í–‰ë™ ì‹¤í–‰ì´ë€ Agentê°€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê²°...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>0.833333</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.470256</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>ë¬¸ì œ ë¶„ì„ì´ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹¤ ìˆ˜ ìˆ...</td>\n",
                            "      <td>[[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...</td>\n",
                            "      <td>[[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...</td>\n",
                            "      <td>ë¬¸ì œ ë¶„ì„ì€ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. Agent...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: ë¬¸ì œ ë¶„ì„ì€ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ëª©í‘œ...</td>\n",
                            "      <td>Deep Learning Researcher</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000</td>\n",
                            "      <td>0.684943</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                           user_input  \\\n",
                            "0                           ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.   \n",
                            "1                                      íŠ¹ì„± ì¶”ì¶œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "2                              ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "3      ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì–´ë–¤ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆë‚˜ìš”?   \n",
                            "4                             ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ë­ì— ì“°ëŠ”ê±°ì•¼?   \n",
                            "5                        ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "6               ê°•ì˜ì—ì„œ ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤„ ìˆ˜ ìˆë‚˜ìš”?   \n",
                            "7                             ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "8                                Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ë­ì—ìš”?   \n",
                            "9                              ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?   \n",
                            "10                     ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?   \n",
                            "11                        Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?   \n",
                            "12                      LLMì´ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ë‚˜ìš”?   \n",
                            "13                                      í–‰ë™ ì‹¤í–‰ì´ë€ ë¬´ì—‡ì¸ê°€?   \n",
                            "14  ë¬¸ì œ ë¶„ì„ì´ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹¤ ìˆ˜ ìˆ...   \n",
                            "\n",
                            "                                   retrieved_contexts  \\\n",
                            "0   [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\n3. Data Preparation\\n- ë°ì´í„°...   \n",
                            "1   [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "2   [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹  \\në¨¸ì‹ ëŸ¬ë‹(Machine ...   \n",
                            "3   [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "4   [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nTODO: breast_cancer data ëª¨ë¸ë§...   \n",
                            "5   [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nTODO: breast_cancer data ëª¨ë¸ë§...   \n",
                            "6   [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 01_ë”¥ëŸ¬...   \n",
                            "7   [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 01_ë”¥ëŸ¬...   \n",
                            "8   [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤, [ê°•ì˜: 04_ì²«ë²ˆ...   \n",
                            "9   [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„  \\n**ë¶„ë¥˜ì˜ ê²½ìš° Pre...   \n",
                            "10  [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...   \n",
                            "11  [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...   \n",
                            "12  [[ê°•ì˜: 01_LLM_LangChain_ê°œìš”]\\n\\nAgents  \\n- ì‘ì—…ì„ ...   \n",
                            "13  [[ê°•ì˜: 12_Agent_ToolCalling]\\n\\nReAct íŒ¨í„´\\n- Age...   \n",
                            "14  [[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...   \n",
                            "\n",
                            "                                   reference_contexts  \\\n",
                            "0   [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "1   [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "2   [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\në”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \\n- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€...   \n",
                            "3   [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "4   [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "5   [[ê°•ì˜: 07_ëª¨ë¸ì €ì¥_ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë¸ ìƒì„±]\\n\\nìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ -...   \n",
                            "6                 [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]   \n",
                            "7                 [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]   \n",
                            "8                 [[ê°•ì˜: 05_ì‹ ê²½ë§ êµ¬ì¡°]\\n\\nTrain(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤]   \n",
                            "9   [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...   \n",
                            "10  [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...   \n",
                            "11  [[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\\n\\nì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°\\n- ì‹¤ì œ Positive...   \n",
                            "12  [[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...   \n",
                            "13  [[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...   \n",
                            "14  [[ê°•ì˜: 12_Agent_ToolCalling]\\n\\n- AgentëŠ” ë‹¨ìˆœíˆ ì§ˆë¬¸...   \n",
                            "\n",
                            "                                             response  \\\n",
                            "0   ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ...   \n",
                            "1   **í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ë°ì´í„°ì—ì„œ ìœ ì˜...   \n",
                            "2   **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ë‘˜ ë‹¤ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„...   \n",
                            "3   ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜...   \n",
                            "4   ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ìœ ë°©ì•” ì§„ë‹¨ì„ ìœ„í•œ ë°ì´í„°ë¡œ, ì£¼ë¡œ ì•”ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì˜ˆ...   \n",
                            "5   í•µì‹¬ ë‹µë³€: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ìœ ë°©ì•” ì§„ë‹¨ ê²°ê³¼ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì£¼ìš” ...   \n",
                            "6   ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤: **ìˆœì „íŒŒ(...   \n",
                            "7   **í•µì‹¬ ë‹µë³€**: ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ìˆœì „íŒŒ(forward propag...   \n",
                            "8   **í•µì‹¬ ë‹µë³€**: Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ...   \n",
                            "9   ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í–ˆ...   \n",
                            "10  ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì‹¤ì œ ì–‘ì„±ì¸ í™˜ìë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°(FN,...   \n",
                            "11  **í•µì‹¬ ë‹µë³€**: Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Negative ë°ì´í„°ë¥¼ ...   \n",
                            "12  LLMì€ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. LLMì€ Agentì˜ ë‘ë‡Œ...   \n",
                            "13  í–‰ë™ ì‹¤í–‰ì´ë€ AI Agentê°€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì™¸ë¶€ ë„êµ¬ë‚˜ APIë¥¼ í˜¸...   \n",
                            "14  ë¬¸ì œ ë¶„ì„ì€ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. Agent...   \n",
                            "\n",
                            "                                            reference  \\\n",
                            "0   **í•µì‹¬ ë‹µë³€**: ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±...   \n",
                            "1   **í•µì‹¬ ë‹µë³€**: íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì€ ì›ë³¸ ë°ì´í„°ì—ì„œ...   \n",
                            "2   **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê³µí†µì ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•...   \n",
                            "3   ìœ„ìŠ¤ì½˜ì‹  ëŒ€í•™êµì—ì„œ ì œê³µí•œ ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ...   \n",
                            "4   ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì€ ì¢…ì–‘ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°ì´í„°...   \n",
                            "5   **í•µì‹¬ ë‹µë³€**: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì˜ ì£¼ìš” íŠ¹ì§•ì€ ì¢…ì–‘ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì¸¡ì •...   \n",
                            "6   ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ì£¼ë¡œ ë°ì´í„°ì˜ ì…ë ¥, ê°€ì¤‘ì¹˜ ì¡°ì •, ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°,...   \n",
                            "7   **í•µì‹¬ ë‹µë³€**: ì‹ ê²½ë§ êµ¬ì¡°ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜...   \n",
                            "8   **í•µì‹¬ ë‹µë³€**: Train(í•™ìŠµ) í”„ë¡œì„¸ìŠ¤ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ...   \n",
                            "9   **í•µì‹¬ ë‹µë³€**: ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negat...   \n",
                            "10  **í•µì‹¬ ë‹µë³€**: ì•”í™˜ì íŒì • ëª¨ë¸ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì‹¤ì œ Positiv...   \n",
                            "11  **í•µì‹¬ ë‹µë³€**: Negativeê°€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ì‹¤ì œ Negative ë°ì´í„°ë¥¼ ...   \n",
                            "12  **í•µì‹¬ ë‹µë³€**: LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì€ Agentì˜ ì‘ì—… ìˆ˜í–‰ì— ìˆì–´ ìì—°ì–´...   \n",
                            "13  **í•µì‹¬ ë‹µë³€**: í–‰ë™ ì‹¤í–‰ì´ë€ Agentê°€ íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê²°...   \n",
                            "14  í•µì‹¬ ë‹µë³€: ë¬¸ì œ ë¶„ì„ì€ Agentì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ëª©í‘œ...   \n",
                            "\n",
                            "                persona_name      query_style query_length  context_precision  \\\n",
                            "0   Deep Learning Researcher       MISSPELLED         LONG           1.000000   \n",
                            "1   Deep Learning Researcher  WEB_SEARCH_LIKE        SHORT           1.000000   \n",
                            "2   Deep Learning Researcher  PERFECT_GRAMMAR       MEDIUM           1.000000   \n",
                            "3             Data Scientist     POOR_GRAMMAR         LONG           1.000000   \n",
                            "4             Data Scientist     POOR_GRAMMAR        SHORT           1.000000   \n",
                            "5             Data Scientist  PERFECT_GRAMMAR        SHORT           1.000000   \n",
                            "6             Data Scientist       MISSPELLED         LONG           0.804167   \n",
                            "7   Deep Learning Researcher  PERFECT_GRAMMAR       MEDIUM           1.000000   \n",
                            "8   Deep Learning Researcher     POOR_GRAMMAR       MEDIUM           1.000000   \n",
                            "9             Data Scientist  WEB_SEARCH_LIKE       MEDIUM           1.000000   \n",
                            "10            Data Scientist  PERFECT_GRAMMAR       MEDIUM           1.000000   \n",
                            "11  Deep Learning Researcher  WEB_SEARCH_LIKE       MEDIUM           1.000000   \n",
                            "12  Computer Vision Engineer  WEB_SEARCH_LIKE        SHORT           1.000000   \n",
                            "13  Deep Learning Researcher       MISSPELLED        SHORT           0.833333   \n",
                            "14  Deep Learning Researcher  PERFECT_GRAMMAR         LONG           1.000000   \n",
                            "\n",
                            "    context_recall  faithfulness  answer_relevancy  \n",
                            "0         0.750000         1.000          0.604906  \n",
                            "1         1.000000         0.800          0.434434  \n",
                            "2         0.666667         0.500          0.663401  \n",
                            "3         0.800000         1.000          0.587451  \n",
                            "4         0.200000         0.375          0.570012  \n",
                            "5         1.000000         0.500          0.515874  \n",
                            "6         0.600000         1.000          0.692942  \n",
                            "7         0.800000         1.000          0.472914  \n",
                            "8         0.600000         0.500          0.600810  \n",
                            "9         0.500000         1.000          0.999999  \n",
                            "10        1.000000         1.000          0.470403  \n",
                            "11        1.000000         1.000          0.000000  \n",
                            "12        1.000000         1.000          0.782049  \n",
                            "13        1.000000         1.000          0.470256  \n",
                            "14        1.000000         1.000          0.684943  "
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#################################################################\n",
                "# 4. Ragas í‰ê°€ ì‹¤í–‰\n",
                "#################################################################\n",
                "\n",
                "result = evaluate(\n",
                "    final_dataset,\n",
                "    metrics=[\n",
                "        context_precision,\n",
                "        context_recall,\n",
                "        faithfulness,\n",
                "        answer_relevancy,\n",
                "    ],\n",
                "    llm=llm,\n",
                "    embeddings=OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL)\n",
                ")\n",
                "\n",
                "print(\"\\n========== í‰ê°€ ê²°ê³¼ ==========\")\n",
                "print(result)\n",
                "df_result = result.to_pandas()\n",
                "df_result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# {'context_precision': 0.8333, 'context_recall': 0.7044, 'faithfulness': 0.8240, 'answer_relevancy': 0.5833}\n",
                "# {'context_precision': 0.9758, 'context_recall': 0.7944, 'faithfulness': 0.8450, 'answer_relevancy': 0.5700}"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}