{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced RAG Evaluation with Hybrid Search & Reranker\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì€ í”„ë¡œì íŠ¸ì˜ **ì‹¤ì œ ê²€ìƒ‰ ë¡œì§(Hybrid Search + Reranker + Dual Query)**ì„ ì‚¬ìš©í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
                "ê¸°ì¡´ì˜ ë‹¨ìˆœ Vector Search í‰ê°€ë³´ë‹¤ í›¨ì”¬ ì •í™•í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install ragas langchain langchain-openai qdrant-client rank_bm25 sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:17: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:17: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:17: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:17: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
                        "  from ragas.metrics import (\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:36: DeprecationWarning: Importing LLMContextRecall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextRecall\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:36: DeprecationWarning: Importing LLMContextPrecisionWithReference from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextPrecisionWithReference\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:36: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/856362602.py:36: DeprecationWarning: Importing AnswerRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerRelevancy\n",
                        "  from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (src ëª¨ë“ˆ import ìœ„í•´)\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env ë° .env.local)\n",
                "load_dotenv(override=True)\n",
                "load_dotenv('.env.local', override=True)\n",
                "\n",
                "from src.utils.config import ConfigDB, ConfigLLM, ConfigAPI\n",
                "from src.retrievals.search_agent import execute_dual_query_search\n",
                "from langchain_core.documents import Document\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "from ragas import evaluate\n",
                "from ragas.metrics import (\n",
                "    context_precision,\n",
                "    context_recall,\n",
                "    faithfulness,\n",
                "    answer_relevancy\n",
                ")\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "from langchain_qdrant import QdrantVectorStore\n",
                "from qdrant_client import QdrantClient\n",
                "from langchain_text_splitters import TokenTextSplitter\n",
                "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnablePassthrough \n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.documents import Document\n",
                "from ragas import evaluate\n",
                "from ragas.testset import TestsetGenerator\n",
                "from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithReference, Faithfulness, AnswerRelevancy\n",
                "from ragas.llms import LangchainLLMWrapper\n",
                "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
                "\n",
                "from src.utils.config import ConfigDB, ConfigLLM\n",
                "from src.prompts import ANALYSIS_SYSTEM_PROMPT\n",
                "from src.retrievals.search_agent import execute_dual_query_search\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM ëª¨ë¸ íŒŒì¸íŠœë‹(Fine tuning)  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(Foundation ëª¨ë¸)\n",
                        "**íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**(**foundation model**)ì€ **ëŒ€ê·œëª¨ ë°ì´í„°**(**í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“±**)ë¡œ ì‚¬ì „ í•™ìŠµ(pre-training)ëœ **ë²”ìš© ì¸ê³µì§€ëŠ¥ ëª¨ë¸**ë¡œ, ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…(Downstream task)ë¥¼ ìœ„í•œ **íŒŒì¸íŠœë‹**ì´ë‚˜ **í”„ë¡¬í”„íŠ¸**(**prompt**) ê¸°ë°˜ì˜ ì‘ë‹µ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§í•œë‹¤.  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ íŠ¹ì§•\n",
                        "- **ê´‘ë²”ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ**\n",
                        "- ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ(pre-training)ë˜ë©°, ê·¸ ê²°ê³¼ **ë‹¤ì–‘í•œ ì£¼ì œì™€ ì—¬ëŸ¬ ì–¸ì–´ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ê³¼ ì–¸ì–´ íŒ¨í„´ë“± ì–¸ì–´ì— ëŒ€í•œ ë²”ìš©ì  ì§€ì‹**ì„ í¬í•¨í•˜ê³  ìˆë‹¤.\n",
                        "- ì´ë¯¸ì§€ ê¸°ë°˜ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(image-based foundation model) ë˜í•œ ëŒ€ê·œëª¨ ì´ë¯¸ì§€Â·ì˜ìƒ ë°ì´í„°ì—ì„œ ì‚¬ë¬¼, ì¥ë©´, ë¬¼ì²´ êµ¬ì¡°ì˜ ì‹œê°ì  íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬, **ê°œë³„ ì‚¬ë¬¼ì˜ íŠ¹ì„±(ì˜ˆ: í˜•íƒœ, ì§ˆê°, ë§¥ë½)ê³¼ ë¬¼ì²´ë“¤ì´ ê³µìœ í•˜ëŠ” ì¼ë°˜ì  ë¬¼ë¦¬ íŠ¹ì„±(ì˜ˆ: ê²½ê³„, ì…ì²´ê°, ì¬ì§ˆ, ê³µê°„ ê´€ê³„)ì„ í‘œí˜„ ê³µê°„(representation space)ì— ë°˜ì˜**í•œë‹¤.\n",
                        "- **ëŒ€ê·œëª¨ ëª¨ë¸**\n",
                        "- íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ **Transformer** êµ¬ì¡°ë¥¼ ê¸°ë°˜(íŠ¹íˆ ìì—°ì–´ ëª¨ë¸)ìœ¼ë¡œ í•˜ë©°, **ìˆ˜ì‹­ì–µì—ì„œ ìˆ˜ì¡°ê°œì˜ íŒŒë¼ë¯¸í„°** ê·œëª¨ë¥¼ ê°€ì§€ê³  ìˆë‹¤.\n",
                        "- **ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ ìˆ˜ë¡ ì„±ëŠ¥ê³¼ í‘œí˜„ë ¥ì´ í–¥ìƒëœë‹¤.**\n",
                        "- **ë²”ìš©ì„±**\n",
                        "- **íŠ¹ì • ì‘ì—…ì— êµ­í•œ ë˜ì§€ ì•Šê³ **, ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ì—­ëŸ‰ì„ ì œê³µí•œë‹¤.\n",
                        "- ì¼ë°˜ì ì¸ NLP taskì¸ ë¬¸ì¥ ìƒì„±, ë¬¸ì„œìš”ì•½, ë²ˆì—­, ì§ˆì˜ì‘ë‹µì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\n",
                        "- **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì‹¤í–‰**\n",
                        "- ì¶”ê°€ì ì¸ í•™ìŠµ(íŒŒì¸íŠœë‹) ì—†ì´ë„ **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**ì„ í†µí•´ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n",
                        "- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€ **AI ëª¨ë¸ë¡œë¶€í„° ìµœìƒì˜ ê²°ê³¼ë¬¼ì„ ì–»ì–´ë‚´ê¸° ìœ„í•´ ì§ˆë¬¸ì„ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê³  ìµœì í™”í•˜ëŠ” ê¸°ìˆ ì„ ë§í•œë‹¤.** LLMì€ ì‚¬ì „í•™ìŠµì„ í†µí•´ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— íŒŒì¸íŠœë‹ ì—†ì´ë„ í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸)ì„ ì˜ ì‘ì„±í•˜ë©´ ë‹µì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
                        "- **ì „ì´ í•™ìŠµ/íŒŒì¸íŠœë‹ì˜ ê¸°ë°˜ ëª¨ë¸**\n",
                        "- ëª¨ë¸ì˜ ê¸°ë³¸ ëŠ¥ë ¥(ì–¸ì–´/ì‚¬ë¬¼ì— ëŒ€í•œ ì¼ë°˜ì  íŠ¹ì„±)ì„ ê°€ì§€ê³  ìˆìœ¼ë©° ì´ë¥¼ **íŠ¹ì • ë„ë©”ì¸ ì‘ì—…ì— ë§ê²Œ ì¡°ì •**í•  ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '1_LLM_Finetuningê°œìš”.ipynb', 'lecture_title': '1_LLM_Finetuningê°œìš”', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6061130604709875}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6061130604709875\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM Fine tuningìœ¼ë¡œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
                        "- **ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ í•„ìš”**\n",
                        "- LLMì€ ìµœì†Œ ìˆ˜ì‹­ì–µê°œì—ì„œ ì¡°ë‹¨ìœ„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ëª¨ë¸ì´ë‹¤. ê·¸ë˜ì„œ íŒŒì¸íŠœë‹ì„í•˜ë ¤ë©´ ì´ì „ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë¹„í•´ ë§ì€ GPUë¥¼ ê°€ì§€ëŠ” ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•˜ë‹¤.\n",
                        "- **ì˜¤ëœ í•™ìŠµì‹œê°„**\n",
                        "- íŒŒì¸íŠœë‹ì€ ë°ì´í„° ì „ì²˜ë¦¬, í•™ìŠµ, í‰ê°€ì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.\n",
                        "- **í° ë¹„ìš©**\n",
                        "- ìœ„ì™€ ê°™ì€ ì´ìœ ë¡œ íŒŒì¸íŠœë‹ì€ ë§ì€ ë¹„ìš©ì´ ì†Œìš”ëœë‹¤.\n",
                        "- **ë„ë©”ì¸ í•œì •ì„±**\n",
                        "- íŒŒì¸íŠœë‹ í›„ **ë²”ìš©ì ì¸ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤**.\n",
                        "- íŒŒì¸íŠœë‹ ê³¼ì •ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ë©´ì„œ ê¸°ì¡´ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ ë²”ìš©ì ì¸ ì§€ì‹ì„ ì¼ë¶€ ë®ì–´ì“°ê±°ë‚˜ ì™œê³¡í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.' metadata={'source': '', 'source_file': '1_LLM_Finetuningê°œìš”.ipynb', 'lecture_title': '1_LLM_Finetuningê°œìš”', 'cell_type': 'markdown', 'cell_index': 2, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.631175932}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.631175932\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "íŒŒì¸íŠœë‹  \n",
                        "Data Collator  \n",
                        "- í•™ìŠµ ë„ì¤‘ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
                        "- Dataset -> Data Collatorí•¨ìˆ˜ -> ëª¨ë¸\n",
                        "- ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì— ì „ë‹¬ë˜ëŠ” batchë¥¼ ë°›ì•„ì„œ ëª¨ë¸ì— ì…ë ¥ì „ì— í•´ì•¼í•˜ëŠ” ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜(Callable).  \n",
                        "- êµ¬í˜„í•  ë‚´ìš©\n",
                        "- ëª¨ë¸ chat í˜•ì‹ì˜ ë¬¸ìì—´ì„ transformers ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ìœ„í•œ inputsë¥¼ ë§Œë“ ë‹¤.  \n",
                        "```json\n",
                        "{\n",
                        "\"input_ids\":ì…ë ¥ sequenceì˜ í† í° IDë“¤,\n",
                        "\"attention_mask\":ì…ë ¥í† í°ê³¼ paddingêµ¬ë¶„,\n",
                        "\"labels\": input_idsì—ì„œ ë‹µë³€ë¶€ë¶„ masking. ë‹µë³€ì€ í† í°ID ë‚˜ë¨¸ì§€ëŠ” -100ìœ¼ë¡œ ì±„ìš´ë‹¤.\n",
                        "}\n",
                        "```' metadata={'source': '', 'source_file': '3_Fine_tuning.ipynb', 'lecture_title': '3_Fine_tuning', 'cell_type': 'markdown', 'cell_index': 36, 'code_snippet': '```python\\nimport torch\\n\\nmax_seq_length = 8192\\n\\ndef collate_fn(batch: list[dict]) -> dict[str, torch.Tensor]:\\n    \"\"\"ëª¨ë¸ í•™ìŠµì‹œ batchë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì²˜ë¦¬í•´ì„œ ë°˜í™˜\\n    Args:\\n        batch (list[dict]): ë°°ì¹˜ ë°ì´í„°, dict: {\"train_prompt\":input text}\\n\\n    Returns:\\n        dict[str, torch.Tensor]: ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì²˜ë¦¬ëœ ë°°ì¹˜ ë°ì´í„° (inputs, attention_mask, labels)\\n    \"\"\"\\n    \\n    new_batch = {\\n        \"input_ids\": [],\\n        \"attention_mask\": [],\\n        \"labels\": []\\n    }\\n    \\n    for prompt in batch:\\n        \\n        text = prompt[\\'train_prompt\\'].strip() #<|begin_of_text|>......\\n        # text -> system prompt / user prompt / ai prompt(label)\\n        tokenized = tokenizer(\\n            text,\\n            truncation=True,\\n            max_length=max_seq_length, # max_lengthì´í•˜ëŠ” ìë¥¸ë‹¤. truncation=True\\n            padding=False,             # paddingì€ ë’¤ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•  ê²ƒì´ê¸° ë•Œë¬¸ì— paddingì²˜ë¦¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë™ì  íŒ¨ë”©. batchì˜ max_lengthì— ë§ì¶° íŒ¨ë”©.\\n            return_tensors=None,       # listë¡œ ë°˜í™˜.\\n        )\\n\\n        input_ids = tokenized[\"input_ids\"]\\n        attention_mask = tokenized[\"attention_mask\"]\\n        labels = [-100] * len(input_ids) \\n        # ë‹µë³€ tokenê°’ì„ ë„£ì„ ë¦¬ìŠ¤íŠ¸. system/user prompt ë¶€ë¶„ì€ -100ìœ¼ë¡œ, ë‹µë³€ ë¶€ë¶„ì€ í† í°ê°’ë“¤ë¡œ ë³€ê²½.\\n        # [-100, -100, ...:system/user, 1289, 32312, ... : Label] \\n        \\n        # -100: íŒŒì´í† ì¹˜ì˜ CrossEntropyLoss()ëŠ” -100ì€ Lossê³„ì‚°ì‹œ ë¬´ì‹œ.\\n\\n        # # chat promptì—ì„œ ë‹µë³€ ë¶€ë¶„ì„ ì°¾ì•„ì„œ labelsë¥¼ êµ¬ì„±í•œë‹¤. \\n\\n        # <|begin_of_text|>\\n        # <|start_header_id|>system<|end_header_id|>\\n        # {ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸}<|eot_id|>\\n        # <|start_header_id|>user<|end_header_id|>\\n        # {ì…ë ¥ - title+document}<|eot_id|>\\n        # <|start_header_id|>assistant<|end_header_id|> -------> ì—¬ê¸°ë¥¼ ì°¾ê¸°. ê·¸ëŒ€ë¡œ ë‘ê³ .(-100)\\n        # {ë‹µë³€ - Label}<|eot_id|>                       -------> ì‹¤ì œ í† í°ìœ¼ë¡œ ë³€ê²½\\n\\n        # assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\\\n\"\\n        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\\n\\n        eot_token = \"<|eot_id|>\"\\n        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\\n\\n        i = 0\\n        while i <= len(input_ids) - len(assistant_tokens):\\n            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens: # assistant_tokens ì°¾ê¸°\\n                start = i + len(assistant_tokens) # \"<|start_header_id|>assistant<|end_header_id|>\\\\n\" ë‹¤ìŒ í† í°ìœ¼ë¡œ ì´ë™\\n                end = start\\n                while end <= len(input_ids) - len(eot_tokens):\\n                    if input_ids[end:end + len(eot_tokens)] == eot_tokens: # ë§ˆì§€ë§‰ í† í°ì¸ \"<|eot_id|>\" ì˜ indexë¥¼ ì°¾ëŠ”ë‹¤.\\n                        break\\n                    end += 1\\n                for j in range(start, end):\\n                    labels[j] = input_ids[j]  # Label ë‹µë³€ í† í°ë§Œ labelsì— ë„£ê¸°.\\n                for j in range(end, end + len(eot_tokens)):\\n                    labels[j] = input_ids[j]\\n                break\\n            i += 1\\n        \\n        # # ìƒì„±ëœ input_ids, attention_mask, labelsë¥¼ new_batchì— ì¶”ê°€í•œë‹¤.\\n        # new_batch[\"input_ids\"].append(input_ids)\\n        new_batch[\"attention_mask\"].append(attention_mask)\\n        new_batch[\"labels\"].append(labels)\\n    # ------------------ë¼ë²¨ ì²˜ë¦¬--------------- \\n\\n    # #  íŒ¨ë”© ì²˜ë¦¬ - ë™ì  íŒ¨ë”©.\\n    #  -  ë°°ì¹˜ë‚´ ì…ë ¥ì¤‘ ê°€ì¥ ê¸´ sampleì— ê¸¸ì´ë¥¼ ë§ì¶˜ë‹¤.\\n    # max_length = max(len(ids) for ids in new_batch[\"input_ids\"])  # batchì˜ max_length ê³„ì‚°.        \\n    for i in range(len(new_batch[\"input_ids\"])):\\n        pad_len = max_length - len(new_batch[\"input_ids\"][i]) \\n        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len) # íŒ¨ë”© í† í° ì¶”ê°€\\n        new_batch[\"attention_mask\"][i].extend([0] * pad_len)                 # íŒ¨ë”© í† í°ì˜ ìœ„ì¹˜ 0 ì„¤ì •.\\n        new_batch[\"labels\"][i].extend([-100] * pad_len)\\n\\n    # list -> torch.Tensorë¡œ ë³€í™˜.\\n    for k in new_batch:\\n        new_batch[k] = torch.tensor(new_batch[k])\\n\\n    return new_batch\\n```\\n\\n```python\\n# # í™•ì¸\\n# example = [trainset[11], trainset[2], trainset[3]]\\nbatch = collate_fn(example)\\n\\nprint(\"batch:\")\\nprint(\"input_ids í¬ê¸°:\", batch[\"input_ids\"].shape)\\nprint(\"attention_mask í¬ê¸°:\", batch[\"attention_mask\"].shape)\\nprint(\"labels í¬ê¸°:\", batch[\"labels\"].shape)\\n```', 'chunk_index': 9, 'original_score': 0.5990258213908755}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5990258213908755\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: 3\n",
                        "[1] [ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM Fine tuningìœ¼ë¡œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ\n",
                        "- **ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ í•„ìš”**\n",
                        "- LLMì€ ìµœì†Œ ìˆ˜ì‹­ì–µê°œì—ì„œ ì¡°ë‹¨ìœ„ì˜ ë§¤ê°œë³€ìˆ˜... (Score: 0.6312)\n",
                        "[2] [ê°•ì˜: 1_LLM_Finetuningê°œìš”]\n",
                        "\n",
                        "LLM ëª¨ë¸ íŒŒì¸íŠœë‹(Fine tuning)  \n",
                        "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(Foundation ëª¨ë¸)\n",
                        "**íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**(**foundati... (Score: 0.6061)\n",
                        "[3] [ê°•ì˜: 3_Fine_tuning]\n",
                        "\n",
                        "íŒŒì¸íŠœë‹  \n",
                        "Data Collator  \n",
                        "- í•™ìŠµ ë„ì¤‘ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
                        "- Dataset -> Data Collatorí•¨ìˆ˜... (Score: 0.5990)\n"
                    ]
                }
            ],
            "source": [
                "#################################################################\n",
                "# 1. Advanced Retriever ì •ì˜ (Hybrid + Reranker)\n",
                "#################################################################\n",
                "\n",
                "def advanced_retriever(query: str):\n",
                "    \"\"\"\n",
                "    src.retrievals.search_agentì˜ execute_dual_query_searchë¥¼ ì‚¬ìš©í•˜ëŠ” Retriever í•¨ìˆ˜\n",
                "    - Dual Query (í•œê¸€/ì˜ì–´)\n",
                "    - Hybrid Search (Vector + Keyword + BM25)\n",
                "    - Reranking (Cross-Encoder)\n",
                "    \"\"\"\n",
                "    # ê²€ìƒ‰ ì‹¤í–‰\n",
                "    results, info = execute_dual_query_search(query)\n",
                "    \n",
                "    # Dict ê²°ê³¼ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜\n",
                "    documents = []\n",
                "    for r in results:\n",
                "        doc = Document(\n",
                "            page_content=r['content'],\n",
                "            metadata=r.get('metadata', {})\n",
                "        )\n",
                "        # ì ìˆ˜ ì •ë³´ë„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€\n",
                "        doc.metadata['score'] = r.get('score', 0)\n",
                "        doc.metadata['source'] = r.get('source', '')\n",
                "        documents.append(doc)\n",
                "        \n",
                "    return documents\n",
                "\n",
                "# í…ŒìŠ¤íŠ¸\n",
                "test_docs = advanced_retriever(\"íŒŒì¸íŠœë‹ì´ ë­ì•¼?\")\n",
                "print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(test_docs)}\")\n",
                "for i, doc in enumerate(test_docs):\n",
                "    print(f\"[{i+1}] {doc.page_content[:100]}... (Score: {doc.metadata.get('score'):.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<>:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "<>:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/3359774379.py:21: SyntaxWarning: invalid escape sequence '\\)'\n",
                        "  - **ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬**: ë³¸ë¬¸ ë‚´ì˜ í°ë”°ì˜´í‘œ(\"), ë°±ìŠ¬ë˜ì‹œ(\\) ë“± íŠ¹ìˆ˜ ë¬¸ìëŠ” ë°˜ë“œì‹œ ì—­ìŠ¬ë˜ì‹œ(\\\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•˜ì‹­ì‹œì˜¤.\n"
                    ]
                }
            ],
            "source": [
                "llm_context = \"\"\"ë‹¹ì‹ ì€ Python í”„ë¡œê·¸ë˜ë° êµìœ¡ ê³¼ì •ì˜ **AI ì¡°êµ(Teaching Assistant)**ì´ì ë°ì´í„°ì…‹ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
                "ì œê³µëœ [ê°•ì˜ ìë£Œ(Lecture)]ì™€ [Python ê³µì‹ ë¬¸ì„œ(RST)]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, í•™ìŠµì í‰ê°€ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆì˜ JSON í¬ë§· ì§ˆì˜ì‘ë‹µ(QA) ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.\n",
                "\n",
                "ìƒì„± ì‹œ ë‹¤ìŒì˜ **ì§€ì¹¨(Instruction)**ê³¼ ë‹µë³€ ìŠ¤íƒ€ì¼(Tone & Style)ì„ ì² ì €íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "---\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "llm_context += ANALYSIS_SYSTEM_PROMPT\n",
                "\n",
                "llm_context += \"\"\"\n",
                "\n",
                "---\n",
                "\n",
                "## QA ìƒì„± ì œì•½ (Constraints)\n",
                "- **ì–¸ì–´**: ê°€ëŠ¥í•˜ë©´ í•œêµ­ì–´ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µë³€í•˜ë˜, ì½”ë“œ ì˜ˆì œëŠ” ì˜ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
                "- **ë¬¸ë²•**: ë¬¸ì¥ì˜ ëì€ ë§ˆì¹¨í‘œ(.) ë“± **êµ¬ë‘ì ì„ ë°˜ë“œì‹œ í‘œê¸°**í•˜ì—¬ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
                "- **JSON í¬ë§· ì—„ìˆ˜**:\n",
                "  - ì¶œë ¥ì€ ë°˜ë“œì‹œ íŒŒì‹± ê°€ëŠ¥í•œ **JSON ë°°ì—´** í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤ (`[{\"user_input\": \"...\", \"reference\": \"...\"}, ...]`).\n",
                "  - **ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬**: ë³¸ë¬¸ ë‚´ì˜ í°ë”°ì˜´í‘œ(\"), ë°±ìŠ¬ë˜ì‹œ(\\) ë“± íŠ¹ìˆ˜ ë¬¸ìëŠ” ë°˜ë“œì‹œ ì—­ìŠ¬ë˜ì‹œ(\\\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•˜ì‹­ì‹œì˜¤.\n",
                "  - ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì— JSON ë¬¸ë²•ì„ í•´ì¹˜ëŠ” ìš”ì†Œê°€ ìˆë”ë¼ë„, ìµœì¢… ì¶œë ¥ì€ ìœ íš¨í•œ JSONì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "---\n",
                "\n",
                "## QA ìƒì„± ì¶œë ¥ ì˜ˆì‹œ\n",
                "[\n",
                "  {\n",
                "    \"user_input\": \"íŒŒì¸íŠœë‹ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€?\",\n",
                "    \"reference\": \"íŒŒì¸íŠœë‹ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(Foundation ëª¨ë¸)ì„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë„ë©”ì¸ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ ìµœì í™” í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, íŠ¹ì • ì‘ì—…/ë„ë©”ì¸ì— ìµœì í™”, ì‚¬ìš©ì ë§ì¶¤í˜• í†¤ì•¤ë§¤ë„ˆ ì ìš©, ì•ˆì „ì„±ê³¼ ìœ¤ë¦¬ ê°•í™”ë¥¼ ìœ„í•´ í•„ìš”í•˜ë‹¤.\"\n",
                "  }\n",
                "]\n",
                "\n",
                "ìœ„ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ í•™ìŠµ ê°€ì¹˜ê°€ ë†’ì€ QA ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['[ê°•ì˜: 09_CNN_ê°œìš”]\\n\\n1D Convolution ì—°ì‚°',\n",
                            " '[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\\n- **í‰ê°€ ìš”ì†Œ**\\n- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\\n- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\\n- **ìƒì„± ë‹¨ê³„ í‰ê°€**\\n- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\\n- **í‰ê°€ ë°©ë²•**\\n- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\\n1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\\n- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\\n2. **ì˜¨ë¼ì¸ í‰ê°€**\\n- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\\n- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\\n1. ì •ëŸ‰ì  í‰ê°€\\n- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\\n2. ì •ì„±ì  í‰ê°€\\n- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.',\n",
                            " '[ê°•ì˜: 02_ì²«ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ - Iris_ë¶„ì„]\\n\\nê·¸ëŸ°ë° ì´ ê²°ê³¼ê°€ ë§ì„ê¹Œ?',\n",
                            " '[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.cluster.KMeans\\n- í•˜ì´í¼íŒŒë¼ë¯¸í„°\\n- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\\n- ì†ì„±\\n- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label',\n",
                            " '[ê°•ì˜: 09_CNN_ê°œìš”]\\n\\nStrides\\n- Filter(Kernel)ê°€ í•œë²ˆ Convolution ì—°ì‚°ì„ ìˆ˜í–‰í•œ í›„ ì˜† í˜¹ì€ ì•„ë˜ë¡œ ì–¼ë§ˆë‚˜ ì´ë™í•  ê²ƒì¸ê°€ë¥¼ ì„¤ì •.\\n- ê°’ìœ¼ë¡œëŠ” ë‹¤ìŒ ë‘ê°€ì§€ íƒ€ì…ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.\\n- tuple: (heightë°©í–¥ ì´ë™í¬ê¸°, width ë°©í–¥ ì´ë™í¬ê¸°)\\n- int: height, width ë°©í–¥ ì´ë™í¬ê¸°ê°€ ê°™ì€ ê²½ìš° ì •ìˆ˜ê°’ í•˜ë‚˜ë¡œ ì„¤ì •í•œë‹¤.\\n- ì˜ˆ) stride=2: í•œ ë²ˆì— ë‘ ì¹¸ì”© ì´ë™\\n- convolution layerì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ 1ì„ ì§€ì •í•œë‹¤.  \\nstride=(2,2)ì˜ ì˜ˆ',\n",
                            " '[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torch.empty(\\\\*size)**',\n",
                            " '[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Supervised Learning)\\n- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\\n- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\\n- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.',\n",
                            " '[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹ë³„ì ê·œì¹™\\n- ì‹ë³„ìë€ íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì£¼ëŠ” ì´ë¦„ì„ ë§í•œë‹¤.\\n- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¸ìëŠ” **ì¼ë°˜ ë¬¸ì**(ì˜ì–´ ì•ŒíŒŒë²³ ë¿ ë§Œ ì•„ë‹ˆë¼ í•œê¸€ í•œì ë“± ëª¨ë“  ì¼ë°˜ ë¬¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.), **ìˆ«ì, íŠ¹ìˆ˜ ë¬¸ìëŠ”_(underscore) ë§Œ** ê°€ëŠ¥.\\n- ìˆ«ìëŠ” **ë‘ë²ˆì§¸ ê¸€ì** ë¶€í„° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\\n- **ì˜ˆì•½ì–´(keyword, reserved word)** ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\\n- ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë³„í•œë‹¤.  \\n> íŒŒì´ì¬ í‚¤ì›Œë“œ\\n> ```python\\n> False await else import pass\\n> None break except in raise\\n> True class finally is return\\n> and continue for lambda try\\n> as def from nonlocal while\\n> assert del global not with\\n> async elif if or yield\\n> ```',\n",
                            " '[ê°•ì˜: 02_Numpy_ë°°ì—´ ì›ì†Œ ì¡°íšŒ_ë°°ì—´ í˜•íƒœ ë³€ê²½_ì—°ì‚°]\\n\\në°°ì—´ì˜ í˜•íƒœ(shape) ë³€ê²½  \\n- ë°°ì—´ì˜ **ì›ì†Œì˜ ê°œìˆ˜ë¥¼ ìœ ì§€í•˜ëŠ” ìƒíƒœ**ì—ì„œ shapeì„ ë³€ê²½í•  ìˆ˜ìˆë‹¤.\\n- ì˜ˆ) (16, ) -> (4,4) -> (2,2,4) -> (2,2,2,2), -> (4,4,1) -> (1, 16)']"
                        ]
                    },
                    "execution_count": 47,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "client = QdrantClient(host='localhost', port=ConfigDB.PORT)\n",
                "info = client.get_collection(ConfigDB.COLLECTION_NAME)\n",
                "\n",
                "results, next_id = client.scroll(\n",
                "    collection_name=ConfigDB.COLLECTION_NAME,\n",
                "    limit=info.points_count,\n",
                ")\n",
                "\n",
                "# sampling\n",
                "import random\n",
                "sample_dataset = random.sample(results, 50) # ë¦¬ìŠ¤íŠ¸ì—ì„œ ëœë¤í•˜ê²Œ Kê°œë¥¼ ì¶”ì¶œ\n",
                "\n",
                "\n",
                "# ë¬¸ì„œ ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
                "docs = [point.payload['page_content'] for point in sample_dataset if point.payload.get('metadata', {}).get('source', '') == 'lecture']\n",
                "docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/3796546008.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
                        "  generator_llm = LangchainLLMWrapper(ChatOpenAI(model=ConfigLLM.OPENAI_MODEL))\n",
                        "/var/folders/by/sbz15vxn1xz8499rjvlm_ctm0000gn/T/ipykernel_10052/3796546008.py:13: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
                        "  generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL))\n",
                        "Applying SummaryExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.31it/s]\n",
                        "Applying CustomNodeFilter:   0%|          | 0/9 [00:00<?, ?it/s]Node efdc7d68-c8ca-4ee4-8375-3af7115748ee does not have a summary. Skipping filtering.\n",
                        "Node 17b91ff1-55b5-4caa-b053-95fed96aa284 does not have a summary. Skipping filtering.\n",
                        "Node e1854982-b67a-4d48-9b31-ba5623411d5c does not have a summary. Skipping filtering.\n",
                        "Node 67faabce-00e5-4685-bb37-f5af6236235e does not have a summary. Skipping filtering.\n",
                        "Node 42364f99-dc6c-4b7f-b598-6ac6befd80cd does not have a summary. Skipping filtering.\n",
                        "Node 3e5c2dd7-b284-4cdf-a668-f1fe70c94f72 does not have a summary. Skipping filtering.\n",
                        "Node fb5702bd-7e02-4167-8c58-1675c2847939 does not have a summary. Skipping filtering.\n",
                        "Node b8859577-7ebb-4bec-8dd2-1e9e6fdf9037 does not have a summary. Skipping filtering.\n",
                        "Node 1dee4f3d-4bcb-44d4-ab53-54d66f392ebe does not have a summary. Skipping filtering.\n",
                        "Applying CustomNodeFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 3876.04it/s]\n",
                        "Applying EmbeddingExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.76it/s]\n",
                        "Applying ThemesExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.31it/s]\n",
                        "Applying NERExtractor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.37it/s]\n",
                        "Applying CosineSimilarityBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 599.10it/s]\n",
                        "Applying OverlapScoreBuilder: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2045.00it/s]\n",
                        "Skipping multi_hop_abstract_query_synthesizer due to unexpected error: No relationships match the provided condition. Cannot form clusters.\n",
                        "Generating personas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.00s/it]\n",
                        "Generating Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.25s/it]\n",
                        "Generating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.28it/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Testset(samples=[TestsetSample(eval_sample=SingleTurnSample(user_input='ì •ì„±ì  í‰ê°€ë€ ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\\n- **í‰ê°€ ìš”ì†Œ**\\n- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\\n- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\\n- **ìƒì„± ë‹¨ê³„ í‰ê°€**\\n- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\\n- **í‰ê°€ ë°©ë²•**\\n- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\\n1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\\n- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\\n2. **ì˜¨ë¼ì¸ í‰ê°€**\\n- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\\n- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\\n1. ì •ëŸ‰ì  í‰ê°€\\n- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\\n2. ì •ì„±ì  í‰ê°€\\n- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Educator', query_style='WEB_SEARCH_LIKE', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\\n- **í‰ê°€ ìš”ì†Œ**\\n- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\\n- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\\n- **ìƒì„± ë‹¨ê³„ í‰ê°€**\\n- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\\n- **í‰ê°€ ë°©ë²•**\\n- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\\n1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\\n- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\\n2. **ì˜¨ë¼ì¸ í‰ê°€**\\n- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\\n- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\\n1. ì •ëŸ‰ì  í‰ê°€\\n- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\\n2. ì •ì„±ì  í‰ê°€\\n- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì—ëŠ” ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ ì˜¨ë¼ì¸ í‰ê°€ê°€ ìˆìŠµë‹ˆë‹¤. ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ , ì˜¨ë¼ì¸ í‰ê°€ëŠ” ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Student', query_style='POOR_GRAMMAR', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ë­ì—ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\\n- **í‰ê°€ ìš”ì†Œ**\\n- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\\n- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\\n- **ìƒì„± ë‹¨ê³„ í‰ê°€**\\n- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\\n- **í‰ê°€ ë°©ë²•**\\n- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\\n1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\\n- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\\n2. **ì˜¨ë¼ì¸ í‰ê°€**\\n- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\\n- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\\n1. ì •ëŸ‰ì  í‰ê°€\\n- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\\n2. ì •ì„±ì  í‰ê°€\\n- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Student', query_style='POOR_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='What are the key attributes of sklearn.cluster.KMeans as mentioned in the lecture?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.cluster.KMeans\\n- í•˜ì´í¼íŒŒë¼ë¯¸í„°\\n- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\\n- ì†ì„±\\n- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference=\"The key attributes of sklearn.cluster.KMeans mentioned in the lecture include 'n_clusters', which specifies the number of categories to classify into, and 'labels_', which provides the label for each data point.\", rubrics=None, persona_name='Data Science Student', query_style='PERFECT_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ë¬´ì—‡ì„ ì§€ì •í•˜ë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.cluster.KMeans\\n- í•˜ì´í¼íŒŒë¼ë¯¸í„°\\n- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\\n- ì†ì„±\\n- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ëª‡ ê°œì˜ categoryë¡œ ë¶„ë¥˜í• ì§€ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Evaluation Analyst', query_style='MISSPELLED', query_length='LONG'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='sklearn.cluster.KMeansëŠ” ë¬´ì—‡ì¸ê°€ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.cluster.KMeans\\n- í•˜ì´í¼íŒŒë¼ë¯¸í„°\\n- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\\n- ì†ì„±\\n- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='sklearn.cluster.KMeansëŠ” êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„° í¬ì¸íŠ¸ë¥¼ n_clustersë¡œ ì§€ì •ëœ ê°œìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Educator', query_style='MISSPELLED', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='tensor ì–´ë–»ê²Œ ìƒì„±í•˜ë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torch.empty(\\\\*size)**'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë¹ˆ tensorëŠ” `torch.empty(*size)`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Educator', query_style='MISSPELLED', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='How can I create an empty tensor using torch?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torch.empty(\\\\*size)**'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='You can create an empty tensor using the function torch.empty(*size).', rubrics=None, persona_name='Data Science Educator', query_style='PERFECT_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='tensor ì–´ë–»ê²Œ ë§Œë“¤ì–´ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torch.empty(\\\\*size)**'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë¹ˆ tensor ìƒì„±ì€ **torch.empty(*size)**ë¥¼ ì‚¬ìš©í•˜ì—¬ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', rubrics=None, persona_name='Data Evaluation Analyst', query_style='POOR_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Supervised Learning)\\n- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\\n- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\\n- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='íšŒê·€ëŠ” ì§€ë„í•™ìŠµì˜ í•œ ì¢…ë¥˜ë¡œ, ëª¨ë¸ì´ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ê³¼ ì •ë‹µ(Label)ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰˜ë©°, íšŒê·€ëŠ” ì£¼ë¡œ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Student', query_style='WEB_SEARCH_LIKE', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ë¨¸ì‹ ëŸ¬ë‹ì€ ë­ì—ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Supervised Learning)\\n- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\\n- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\\n- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë¨¸ì‹ ëŸ¬ë‹ì€ ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•ê³¼ ì •ë‹µì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¤ëŠ” ì§€ë„í•™ìŠµ ë°©ì‹ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì— í•´ë‹¹í•©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Science Educator', query_style='POOR_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•ì€ ë­ì—ìš”?', retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Supervised Learning)\\n- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\\n- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\\n- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference='ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•(Feature)ì€ ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•ê³¼ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.', rubrics=None, persona_name='Data Evaluation Analyst', query_style='POOR_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input=\"íŒŒì´ì¬ì—ì„œ 'pass'ëŠ” ë­í•˜ëŠ” ê±°ì•¼?\", retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹ë³„ì ê·œì¹™\\n- ì‹ë³„ìë€ íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì£¼ëŠ” ì´ë¦„ì„ ë§í•œë‹¤.\\n- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¸ìëŠ” **ì¼ë°˜ ë¬¸ì**(ì˜ì–´ ì•ŒíŒŒë²³ ë¿ ë§Œ ì•„ë‹ˆë¼ í•œê¸€ í•œì ë“± ëª¨ë“  ì¼ë°˜ ë¬¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.), **ìˆ«ì, íŠ¹ìˆ˜ ë¬¸ìëŠ”_(underscore) ë§Œ** ê°€ëŠ¥.\\n- ìˆ«ìëŠ” **ë‘ë²ˆì§¸ ê¸€ì** ë¶€í„° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\\n- **ì˜ˆì•½ì–´(keyword, reserved word)** ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\\n- ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë³„í•œë‹¤.  \\n> íŒŒì´ì¬ í‚¤ì›Œë“œ\\n> ```python\\n> False await else import pass\\n> None break except in raise\\n> True class finally is return\\n> and continue for lambda try\\n> as def from nonlocal while\\n> assert del global not with\\n> async elif if or yield\\n> ```'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference=\"'pass'ëŠ” íŒŒì´ì¬ì—ì„œ ì˜ˆì•½ì–´ë¡œ, ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë¹ˆ ë¬¸ì¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì£¼ë¡œ ì½”ë“œ ë¸”ë¡ì´ í•„ìš”í•˜ì§€ë§Œ ì‹¤í–‰í•  ì½”ë“œê°€ ì—†ì„ ë•Œ ì‚¬ìš©ëœë‹¤.\", rubrics=None, persona_name='Data Science Educator', query_style='POOR_GRAMMAR', query_length='MEDIUM'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input=\"What is the significance of the term 'await' in Python?\", retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹ë³„ì ê·œì¹™\\n- ì‹ë³„ìë€ íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì£¼ëŠ” ì´ë¦„ì„ ë§í•œë‹¤.\\n- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¸ìëŠ” **ì¼ë°˜ ë¬¸ì**(ì˜ì–´ ì•ŒíŒŒë²³ ë¿ ë§Œ ì•„ë‹ˆë¼ í•œê¸€ í•œì ë“± ëª¨ë“  ì¼ë°˜ ë¬¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.), **ìˆ«ì, íŠ¹ìˆ˜ ë¬¸ìëŠ”_(underscore) ë§Œ** ê°€ëŠ¥.\\n- ìˆ«ìëŠ” **ë‘ë²ˆì§¸ ê¸€ì** ë¶€í„° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\\n- **ì˜ˆì•½ì–´(keyword, reserved word)** ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\\n- ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë³„í•œë‹¤.  \\n> íŒŒì´ì¬ í‚¤ì›Œë“œ\\n> ```python\\n> False await else import pass\\n> None break except in raise\\n> True class finally is return\\n> and continue for lambda try\\n> as def from nonlocal while\\n> assert del global not with\\n> async elif if or yield\\n> ```'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference=\"'await' is a reserved keyword in Python, which means it cannot be used as an identifier. It is part of the syntax used in asynchronous programming to pause the execution of a coroutine until the awaited task is completed.\", rubrics=None, persona_name='Data Science Student', query_style='PERFECT_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input=\"What is the significance of the term 'await' in Python?\", retrieved_contexts=None, reference_contexts=['[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹ë³„ì ê·œì¹™\\n- ì‹ë³„ìë€ íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì£¼ëŠ” ì´ë¦„ì„ ë§í•œë‹¤.\\n- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¸ìëŠ” **ì¼ë°˜ ë¬¸ì**(ì˜ì–´ ì•ŒíŒŒë²³ ë¿ ë§Œ ì•„ë‹ˆë¼ í•œê¸€ í•œì ë“± ëª¨ë“  ì¼ë°˜ ë¬¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.), **ìˆ«ì, íŠ¹ìˆ˜ ë¬¸ìëŠ”_(underscore) ë§Œ** ê°€ëŠ¥.\\n- ìˆ«ìëŠ” **ë‘ë²ˆì§¸ ê¸€ì** ë¶€í„° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\\n- **ì˜ˆì•½ì–´(keyword, reserved word)** ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\\n- ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë³„í•œë‹¤.  \\n> íŒŒì´ì¬ í‚¤ì›Œë“œ\\n> ```python\\n> False await else import pass\\n> None break except in raise\\n> True class finally is return\\n> and continue for lambda try\\n> as def from nonlocal while\\n> assert del global not with\\n> async elif if or yield\\n> ```'], retrieved_context_ids=None, reference_context_ids=None, response=None, multi_responses=None, reference=\"The term 'await' is a Python keyword that is used in asynchronous programming to pause the execution of a coroutine until the awaited task is completed.\", rubrics=None, persona_name='Data Science Educator', query_style='PERFECT_GRAMMAR', query_length='SHORT'), synthesizer_name='single_hop_specific_query_synthesizer')])"
                        ]
                    },
                    "execution_count": 48,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#################################################################\n",
                "# 2. Testset ì¤€ë¹„\n",
                "# (ê¸°ì¡´ì— ìƒì„±í•´ë‘” í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±)\n",
                "#################################################################\n",
                "import pandas as pd\n",
                "from datasets import Dataset\n",
                "\n",
                "# ì˜ˆì‹œ ë°ì´í„°ì…‹ (ì‹¤ì œë¡œëŠ” TestsetGeneratorë¡œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”)\n",
                "# ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ìë‹˜ì´ ì´ì „ì— ìƒì„±í–ˆë˜ ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
                "# ë§Œì•½ ì €ì¥ëœ ë°ì´í„°ê°€ ìˆë‹¤ë©´ pd.read_csv() ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
                "\n",
                "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=ConfigLLM.OPENAI_MODEL))\n",
                "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL))\n",
                "\n",
                "generator = TestsetGenerator(\n",
                "    llm=generator_llm,\n",
                "    embedding_model=generator_embeddings,\n",
                "    llm_context=llm_context \n",
                ")\n",
                "\n",
                "# testset\n",
                "testset = generator.generate_with_chunks(\n",
                "    docs, testset_size=20  # context ë‚´ìš©, í…ŒìŠ¤íŠ¸ë°ì´í„°ì…‹ ëª‡ê°œë¥¼ ë§Œë“¤ì§€.\n",
                ")\n",
                "testset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reference ë°ì´í„° ì¬ìƒì„± ì¤‘...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_input</th>\n",
                            "      <th>reference_contexts</th>\n",
                            "      <th>reference</th>\n",
                            "      <th>persona_name</th>\n",
                            "      <th>query_style</th>\n",
                            "      <th>query_length</th>\n",
                            "      <th>synthesizer_name</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>ì •ì„±ì  í‰ê°€ë€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ ì˜¨ë¼ì¸ í‰ê°€ë¡œ ë‚˜ë‰˜...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ë­ì—ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>What are the key attributes of sklearn.cluster...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `l...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ë¬´ì—‡...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...</td>\n",
                            "      <td>Data Evaluation Analyst</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>single_hop_specific_query_synthesizer</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                          user_input  \\\n",
                            "0                                     ì •ì„±ì  í‰ê°€ë€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "1                        RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?   \n",
                            "2                                      ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ë­ì—ìš”?   \n",
                            "3  What are the key attributes of sklearn.cluster...   \n",
                            "4  sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ë¬´ì—‡...   \n",
                            "\n",
                            "                                  reference_contexts  \\\n",
                            "0  [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "1  [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "2  [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "3  [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "4  [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "\n",
                            "                                           reference             persona_name  \\\n",
                            "0  **í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...    Data Science Educator   \n",
                            "1  **í•µì‹¬ ë‹µë³€**: RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ ì˜¨ë¼ì¸ í‰ê°€ë¡œ ë‚˜ë‰˜...     Data Science Student   \n",
                            "2  **í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜...     Data Science Student   \n",
                            "3  **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `l...     Data Science Student   \n",
                            "4  **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...  Data Evaluation Analyst   \n",
                            "\n",
                            "       query_style query_length                       synthesizer_name  \n",
                            "0  WEB_SEARCH_LIKE        SHORT  single_hop_specific_query_synthesizer  \n",
                            "1     POOR_GRAMMAR         LONG  single_hop_specific_query_synthesizer  \n",
                            "2     POOR_GRAMMAR       MEDIUM  single_hop_specific_query_synthesizer  \n",
                            "3  PERFECT_GRAMMAR       MEDIUM  single_hop_specific_query_synthesizer  \n",
                            "4       MISSPELLED         LONG  single_hop_specific_query_synthesizer  "
                        ]
                    },
                    "execution_count": 49,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "eval_df = testset.to_pandas()\n",
                "\n",
                "# Reference ë°ì´í„° ì¬ìƒì„± (ANALYSIS_SYSTEM_PROMPT ì ìš©)\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from src.prompts import ANALYSIS_SYSTEM_PROMPT\n",
                "\n",
                "# LLM ì„¤ì •\n",
                "gt_llm = ChatOpenAI(model=ConfigLLM.OPENAI_MODEL, temperature=0)\n",
                "gt_prompt = ChatPromptTemplate.from_template(ANALYSIS_SYSTEM_PROMPT)\n",
                "gt_chain = gt_prompt | gt_llm | StrOutputParser()\n",
                "\n",
                "print(\"Reference ë°ì´í„° ì¬ìƒì„± ì¤‘...\")\n",
                "new_references = []\n",
                "for idx, row in eval_df.iterrows():\n",
                "    if isinstance(row['reference_contexts'], list):\n",
                "        context_str = \"\\n\\n\".join(row['reference_contexts'])\n",
                "    else:\n",
                "        context_str = str(row['reference_contexts'])\n",
                "    \n",
                "    query = row['user_input']\n",
                "    \n",
                "    # Generate\n",
                "    try:\n",
                "        new_ref = gt_chain.invoke({\"context\": context_str, \"query\": query})\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating reference for query {query}: {e}\")\n",
                "        new_ref = row.get('reference', '') # Fallback to original\n",
                "        \n",
                "    new_references.append(new_ref)\n",
                "\n",
                "eval_df['reference'] = new_references\n",
                "\n",
                "# ë‹¤ìŒ ì…€ í˜¸í™˜ì„±ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
                "eval_data = eval_df.rename(columns={'user_input': 'question'}).to_dict('list')\n",
                "\n",
                "eval_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ì¤‘...\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAG í‰ê°€ ê°œìš”\n",
                        "- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
                        "- **í‰ê°€ ìš”ì†Œ**\n",
                        "- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\n",
                        "- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\n",
                        "- **ìƒì„± ë‹¨ê³„ í‰ê°€**\n",
                        "- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\n",
                        "- **í‰ê°€ ë°©ë²•**\n",
                        "- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\n",
                        "1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\n",
                        "- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\n",
                        "2. **ì˜¨ë¼ì¸ í‰ê°€**\n",
                        "- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\n",
                        "1. ì •ëŸ‰ì  í‰ê°€\n",
                        "- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\n",
                        "2. ì •ì„±ì  í‰ê°€\n",
                        "- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n!uv pip install ragas rapidfuzz\\n```', 'chunk_index': 0, 'original_score': 0.6524498320000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6524498320000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì‹¤ìŠµ' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '```python\\n# !uv pip install ragas rapidfuzz\\n# ì„¤ì¹˜ í›„ ì»¤ë„ ì¬ì‹œì‘\\n```\\n\\n```python\\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\\nfrom qdrant_client import QdrantClient, models\\nfrom qdrant_client.models import Distance, SparseVectorParams, VectorParams\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\n# ##############################################################\\n# ë°ì´í„° ì¤€ë¹„\\n# def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\\n    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\\n        olympic_text = fr.read()\\n\\n    # Split\\n    splitter = MarkdownHeaderTextSplitter(\\n        headers_to_split_on=[\\n            (\"#\", \"Header 1\"),\\n            (\"##\", \"Header 2\"),\\n            (\"###\", \"Header 3\"),\\n        ],\\n        # strip_headers=False, # ë¬¸ì„œì— header í¬í•¨ ì—¬ë¶€(default: True - ì œê±°)\\n    )\\n\\n    return splitter.split_text(olympic_text)\\n```\\n\\n```python\\n# # Vector DB ì—°ê²°\\n# retriever ìƒì„±\\n# def get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\\n\\n    dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\n    client = QdrantClient(host=\"localhost\", port=6333)\\n\\n    # ì»¬ë ‰ì…˜ ì‚­ì œ\\n    if client.collection_exists(collection_name):\\n        result = client.delete_collection(collection_name=collection_name)\\n\\n    # ì»¬ë ‰ì…˜ ìƒì„±\\n    client.create_collection(\\n        collection_name=collection_name,\\n        vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\\n    )\\n\\n    vectorstore = QdrantVectorStore(\\n        client=client,\\n        collection_name=collection_name,    \\n        embedding=dense_embeddings\\n    )\\n    \\n    # # Documentë“¤ ì¶”ê°€\\n    # documents = load_and_split_olympic_data()\\n    vectorstore.add_documents(documents=documents)\\n\\n    return vectorstore\\n\\ndef get_retriever(vectorstore, k: int = 5):\\n    retriever = vectorstore.as_retriever(\\n        search_kwargs={\"k\": k}\\n    )\\n    return retriever\\n```\\n\\n```python\\nvectorstore = get_vectorstore()\\n\\nretriever = get_retriever(vectorstore)\\nretriever\\n```\\n\\n```python\\n# # í‰ê°€í•  RAG Chain\\n# from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough \\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.documents import Document\\n\\nvectorstore = get_vectorstore()\\nretriever = get_retriever(vectorstore)\\n\\nprompt_txt = \"\"\"# Instruction:\\në‹¹ì‹ ì€ ì •ë³´ì œê³µì„ ëª©ì ìœ¼ë¡œí•˜ëŠ” ìœ ëŠ¥í•œ AI Assistant ì…ë‹ˆë‹¤.\\nì£¼ì–´ì§„ contextì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° \"ì •ë³´ê°€ ë¶€ì¡±í•´ ë‹µì„ í•  ìˆ˜ì—†ìŠµë‹ˆë‹¤.\" ë¼ê³  ë‹µí•©ë‹ˆë‹¤.\\nì ˆëŒ€ ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µì„ í•˜ê±°ë‚˜ Context ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ì„œ ë‹µë³€í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\\n\\n# Context:\\n{context}\\n\\n# ì§ˆë¬¸:\\n{query}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(\\n    template=prompt_txt\\n)\\n\\ndef format_docs(documents:list)->str:\\n    \"\"\"\\n    VectorStoreì— ì¡°íšŒí•œ ë¬¸ì„œë“¤ì—ì„œ ë‚´ìš©(page_content)ë§Œ ì¶”ì¶œí•´ì„œ strìœ¼ë¡œ í•©ì³ì„œ ë°˜í™˜.\\n    VectorStoreì˜ ê²€ìƒ‰ê²°ê³¼ì¸ List[Document]ë¥¼ ë°›ì•„ì„œ Documentë“¤ì—ì„œ page_contentì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•œë‹¤.\\n    \\n    Args:\\n        documents(list[Document]): [Document(..), Document(...), ..]}\\n    Returns:\\n        str: ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ \"\\\\n\\\\n\"ìœ¼ë¡œ ì—°ê²°í•œ string\\n    \"\"\"\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in documents)\\n\\nmodel = ChatOpenAI(model=\"gpt-5-mini\")\\nparser = StrOutputParser()\\n\\n# chain = {\\n#     \"context\":retriever | format_docs,\\n#     \"query\":RunnablePassthrough()\\n# } | prompt | model | parser\\n\\n# RAG í‰ê°€ë¥¼ ìœ„í•´ì„œ \"ë‹µë³€\", \"ê²€ìƒ‰í•œ ë¬¸ì„œ\" ë‘˜ì´ ì¶œë ¥ë˜ë„ë¡ ë³€ê²½.\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\\nfrom operator import itemgetter\\n\\ndef format_doc_list(docs_dict: dict) -> list:\\n    # dictionary[list[Document], query:str] -> list[str]  \\n    # ë¬¸ì„œë‚´ìš©ë§Œ ì¶”ì¶œí•´ì„œ(Document.page_content)ë§Œ ì¶”ì¶œí•œ ë¦¬ìŠ¤íŠ¸\\n    return [doc.page_content for doc in docs_dict[\\'context\\']]\\n\\n# dict | dict -> dict\\n# RunnablePassthrough() | dict | dict ==> RunnableSequence\\nchain = RunnablePassthrough() | {\\n    \"context\":retriever,\\n    \"query\":RunnablePassthrough()\\n} | {\\n    \"response\": prompt | model | parser,\\n    \"retrieved_context\": format_doc_list, # RAGAS í‰ê°€ì‹œ context -> List[str]\\n}\\n```\\n\\n```python\\nres = chain.invoke(\"1íšŒ ì˜¬ë¦¼í”½ì€ ì–¸ì œ ì–´ë””ì„œ ì—´ë ¸ì§€\")\\n```', 'chunk_index': 8, 'original_score': 0.4427659205164829}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4427659205164829\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ëª¨ë¸ í‰ê°€\n",
                        "- ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ëŠ” ëª¨ë¸ë§ ì¤‘ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê²€ì¦ ë‹¨ê³„ì™€ ìµœì¢… ì„±ëŠ¥ í‰ê°€ì—ì„œ ì§„í–‰í•œë‹¤.\n",
                        "- ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê°€ì™€ ëª¨ë¸ì˜ ì–´ë–¤ ì¸¡ë©´ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê°€ì— ë”°ë¼ ë‹¤ì–‘í•œ í‰ê°€ ë°©ë²•ì´ ìˆë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4487071119087696}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4487071119087696\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAG í‰ê°€ ê°œìš”\n",
                        "- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
                        "- **í‰ê°€ ìš”ì†Œ**\n",
                        "- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\n",
                        "- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\n",
                        "- **ìƒì„± ë‹¨ê³„ í‰ê°€**\n",
                        "- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\n",
                        "- **í‰ê°€ ë°©ë²•**\n",
                        "- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\n",
                        "1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\n",
                        "- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\n",
                        "2. **ì˜¨ë¼ì¸ í‰ê°€**\n",
                        "- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\n",
                        "1. ì •ëŸ‰ì  í‰ê°€\n",
                        "- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\n",
                        "2. ì •ì„±ì  í‰ê°€\n",
                        "- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n!uv pip install ragas rapidfuzz\\n```', 'chunk_index': 0, 'original_score': 0.6524498320000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6524498320000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì‹¤ìŠµ' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '```python\\n# !uv pip install ragas rapidfuzz\\n# ì„¤ì¹˜ í›„ ì»¤ë„ ì¬ì‹œì‘\\n```\\n\\n```python\\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\\nfrom qdrant_client import QdrantClient, models\\nfrom qdrant_client.models import Distance, SparseVectorParams, VectorParams\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\n# ##############################################################\\n# ë°ì´í„° ì¤€ë¹„\\n# def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\\n    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\\n        olympic_text = fr.read()\\n\\n    # Split\\n    splitter = MarkdownHeaderTextSplitter(\\n        headers_to_split_on=[\\n            (\"#\", \"Header 1\"),\\n            (\"##\", \"Header 2\"),\\n            (\"###\", \"Header 3\"),\\n        ],\\n        # strip_headers=False, # ë¬¸ì„œì— header í¬í•¨ ì—¬ë¶€(default: True - ì œê±°)\\n    )\\n\\n    return splitter.split_text(olympic_text)\\n```\\n\\n```python\\n# # Vector DB ì—°ê²°\\n# retriever ìƒì„±\\n# def get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\\n\\n    dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\n    client = QdrantClient(host=\"localhost\", port=6333)\\n\\n    # ì»¬ë ‰ì…˜ ì‚­ì œ\\n    if client.collection_exists(collection_name):\\n        result = client.delete_collection(collection_name=collection_name)\\n\\n    # ì»¬ë ‰ì…˜ ìƒì„±\\n    client.create_collection(\\n        collection_name=collection_name,\\n        vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\\n    )\\n\\n    vectorstore = QdrantVectorStore(\\n        client=client,\\n        collection_name=collection_name,    \\n        embedding=dense_embeddings\\n    )\\n    \\n    # # Documentë“¤ ì¶”ê°€\\n    # documents = load_and_split_olympic_data()\\n    vectorstore.add_documents(documents=documents)\\n\\n    return vectorstore\\n\\ndef get_retriever(vectorstore, k: int = 5):\\n    retriever = vectorstore.as_retriever(\\n        search_kwargs={\"k\": k}\\n    )\\n    return retriever\\n```\\n\\n```python\\nvectorstore = get_vectorstore()\\n\\nretriever = get_retriever(vectorstore)\\nretriever\\n```\\n\\n```python\\n# # í‰ê°€í•  RAG Chain\\n# from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough \\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.documents import Document\\n\\nvectorstore = get_vectorstore()\\nretriever = get_retriever(vectorstore)\\n\\nprompt_txt = \"\"\"# Instruction:\\në‹¹ì‹ ì€ ì •ë³´ì œê³µì„ ëª©ì ìœ¼ë¡œí•˜ëŠ” ìœ ëŠ¥í•œ AI Assistant ì…ë‹ˆë‹¤.\\nì£¼ì–´ì§„ contextì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° \"ì •ë³´ê°€ ë¶€ì¡±í•´ ë‹µì„ í•  ìˆ˜ì—†ìŠµë‹ˆë‹¤.\" ë¼ê³  ë‹µí•©ë‹ˆë‹¤.\\nì ˆëŒ€ ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µì„ í•˜ê±°ë‚˜ Context ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ì„œ ë‹µë³€í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\\n\\n# Context:\\n{context}\\n\\n# ì§ˆë¬¸:\\n{query}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(\\n    template=prompt_txt\\n)\\n\\ndef format_docs(documents:list)->str:\\n    \"\"\"\\n    VectorStoreì— ì¡°íšŒí•œ ë¬¸ì„œë“¤ì—ì„œ ë‚´ìš©(page_content)ë§Œ ì¶”ì¶œí•´ì„œ strìœ¼ë¡œ í•©ì³ì„œ ë°˜í™˜.\\n    VectorStoreì˜ ê²€ìƒ‰ê²°ê³¼ì¸ List[Document]ë¥¼ ë°›ì•„ì„œ Documentë“¤ì—ì„œ page_contentì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•œë‹¤.\\n    \\n    Args:\\n        documents(list[Document]): [Document(..), Document(...), ..]}\\n    Returns:\\n        str: ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ \"\\\\n\\\\n\"ìœ¼ë¡œ ì—°ê²°í•œ string\\n    \"\"\"\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in documents)\\n\\nmodel = ChatOpenAI(model=\"gpt-5-mini\")\\nparser = StrOutputParser()\\n\\n# chain = {\\n#     \"context\":retriever | format_docs,\\n#     \"query\":RunnablePassthrough()\\n# } | prompt | model | parser\\n\\n# RAG í‰ê°€ë¥¼ ìœ„í•´ì„œ \"ë‹µë³€\", \"ê²€ìƒ‰í•œ ë¬¸ì„œ\" ë‘˜ì´ ì¶œë ¥ë˜ë„ë¡ ë³€ê²½.\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\\nfrom operator import itemgetter\\n\\ndef format_doc_list(docs_dict: dict) -> list:\\n    # dictionary[list[Document], query:str] -> list[str]  \\n    # ë¬¸ì„œë‚´ìš©ë§Œ ì¶”ì¶œí•´ì„œ(Document.page_content)ë§Œ ì¶”ì¶œí•œ ë¦¬ìŠ¤íŠ¸\\n    return [doc.page_content for doc in docs_dict[\\'context\\']]\\n\\n# dict | dict -> dict\\n# RunnablePassthrough() | dict | dict ==> RunnableSequence\\nchain = RunnablePassthrough() | {\\n    \"context\":retriever,\\n    \"query\":RunnablePassthrough()\\n} | {\\n    \"response\": prompt | model | parser,\\n    \"retrieved_context\": format_doc_list, # RAGAS í‰ê°€ì‹œ context -> List[str]\\n}\\n```\\n\\n```python\\nres = chain.invoke(\"1íšŒ ì˜¬ë¦¼í”½ì€ ì–¸ì œ ì–´ë””ì„œ ì—´ë ¸ì§€\")\\n```', 'chunk_index': 8, 'original_score': 0.4427659205164829}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4427659205164829\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ëª¨ë¸ í‰ê°€\n",
                        "- ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ëŠ” ëª¨ë¸ë§ ì¤‘ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê²€ì¦ ë‹¨ê³„ì™€ ìµœì¢… ì„±ëŠ¥ í‰ê°€ì—ì„œ ì§„í–‰í•œë‹¤.\n",
                        "- ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê°€ì™€ ëª¨ë¸ì˜ ì–´ë–¤ ì¸¡ë©´ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê°€ì— ë”°ë¼ ë‹¤ì–‘í•œ í‰ê°€ ë°©ë²•ì´ ìˆë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4487071119087696}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4487071119087696\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAG í‰ê°€ ê°œìš”\n",
                        "- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
                        "- **í‰ê°€ ìš”ì†Œ**\n",
                        "- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\n",
                        "- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\n",
                        "- **ìƒì„± ë‹¨ê³„ í‰ê°€**\n",
                        "- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\n",
                        "- **í‰ê°€ ë°©ë²•**\n",
                        "- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\n",
                        "1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\n",
                        "- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\n",
                        "2. **ì˜¨ë¼ì¸ í‰ê°€**\n",
                        "- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\n",
                        "1. ì •ëŸ‰ì  í‰ê°€\n",
                        "- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\n",
                        "2. ì •ì„±ì  í‰ê°€\n",
                        "- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n!uv pip install ragas rapidfuzz\\n```', 'chunk_index': 0, 'original_score': 0.8239932400000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.8239932400000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì§€í‘œ ê°œìš”  \n",
                        "- **Generation**\n",
                        "- llm ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë“¤.\n",
                        "- **Faithfulness(ì‹ ë¢°ì„±)**\n",
                        "- ìƒì„±ëœ ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ\n",
                        "- ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ì— ëŒ€í•œ í‰ê°€ë¡œ ë³¼ ìˆ˜ìˆë‹¤.\n",
                        "- **Answer relevancy(ë‹µë³€ ì í•©ì„±)**\n",
                        "- ìƒì„±ëœ ë‹µë³€ê³¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ\n",
                        "- ìƒì„±ëœ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- **Retrieval**\n",
                        "- ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰í•œ ë¬¸ì„œ(context)ë“¤ì— ëŒ€í•œ í‰ê°€\n",
                        "- **Context Precision(ë¬¸ë§¥ ì •ë°€ë„)**\n",
                        "- ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë“¤ ì¤‘ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ê²ƒë“¤ì´ **ì–¼ë§ˆë‚˜ ìƒìœ„ ìˆœìœ„ì— ìœ„ì¹˜í•˜ëŠ”ì§€** í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- **Context Recall(ë¬¸ë§¥ ì¬í˜„ë¥ )**\n",
                        "- ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°€ ì •ë‹µ(ground-truth)ì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- ì´ëŸ¬í•œ ì§€í‘œë“¤ì€ RAG íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ëŠ” ë° í™œìš©ëœë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5749876690940638}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5749876690940638\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "ì£¼ìš” í‰ê°€ì§€í‘œ\n",
                        "Generation í‰ê°€\n",
                        "- LLMì´ ìƒì„±í•œ ë‹µë³€ì— ëŒ€í•œ í‰ê°€  \n",
                        "Faithfulness (ì‹ ë¢°ì„±)\n",
                        "- ìƒì„±ëœ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì£¼ì–´ì§„ ê²€ìƒ‰ ë¬¸ì„œë“¤(context)ë¥¼ ì˜ ë°˜ì˜í•´ì„œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í‰ê°€í•œë‹¤. í• ë£¨ì‹œë„¤ì´ì…˜ì— ëŒ€í•œ í‰ê°€ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n",
                        "- ì ìˆ˜ë²”ìœ„: **0 ~ 1** (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)\n",
                        "- ë‹µë³€ì— í¬í•¨ëœ ëª¨ë“  ì£¼ì¥ì´ contextì—ì„œ ì–¼ë§ˆë‚˜ ì¶”ì¶œ ê°€ëŠ¥í•œì§€ë¥¼ í™•ì¸í•œë‹¤.  \n",
                        "í‰ê°€ ë°©ë²•\n",
                        "1. Answerì—ì„œ ì£¼ì¥ êµ¬ë¬¸(claim statement)ë“¤ì„ ìƒì„±(ì¶”ì¶œ)í•œë‹¤. (ì£¼ì¥ì´ë€, ì§ˆë¬¸(user input)ê³¼ ê´€ë ¨ëœ ë‚´ìš©)\n",
                        "- ì˜ˆ)\n",
                        "- **ì§ˆë¬¸**: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì´ê³  ì¸êµ¬ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\n",
                        "- **LLM ë‹µë³€**: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ê³  ì¸êµ¬ìˆ˜ëŠ” 3000ë§Œëª…ì´ë‹¤.\n",
                        "- **ì£¼ì¥(claim)**:\n",
                        "1. í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.\n",
                        "2. ì¸êµ¬ìˆ˜ëŠ” 3000ë§Œëª…ì´ë‹¤.\n",
                        "2. ê° ì£¼ì¥ë“¤ì„ contextë¡œ ë¶€í„° ì¶”ë¡  ê°€ëŠ¥í•œì§€ íŒë‹¨í•œë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ faithfulness ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.\n",
                        "- ì˜ˆ)\n",
                        "- context: í•œêµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•˜ê³  ìˆëŠ” ë‚˜ë¼ë‹¤. í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤. .... í•œêµ­ì˜ ì¸êµ¬ëŠ” 5000ë§Œëª…ì´ê³  ì„œìš¸ì— 1000ë§Œì´ ì‚´ê³  ìˆë‹¤.\n",
                        "- ìœ„ contextì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œ ì£¼ì¥:\n",
                        "- í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤. -> contextì—ì„œ ì¶”ë¡ ê°€ëŠ¥í•œ ì£¼ì¥.\n",
                        "- í•œêµ­ì˜ ì¸êµ¬ëŠ” 3000ë§Œëª…ì´ë‹¤. -> contextì—ì„œ ì¶”ë¡  ë¶ˆê°€ëŠ¥í•œ ì£¼ì¥.\n",
                        "3. **Faithfulness score** ë¥¼ ê³„ì‚°í•œë‹¤. ì´ ì£¼ì¥ ìˆ˜ ì¤‘ì—ì„œ contextë¡œ ë¶€í„° ì¶”ë¡ ê°€ëŠ¥í•œ ì£¼ì¥ì˜ ê°œìˆ˜.\n",
                        "- ì˜ˆ)\n",
                        "- Faithfulness Score = \\cfrac{1}{2} = 0.5 (ë‘ ê°œì˜ ì£¼ì¥ ì¤‘ í•œ ê°œì˜ ì£¼ì¥ë§Œ contextì—ì„œ ìœ ì¶”í•  ìˆ˜ìˆë‹¤.)\n",
                        "- LLM ë‹µë³€ì—ì„œ ì£¼ì¥ì„ ì¶”ì¶œ í•˜ëŠ” ê²ƒê³¼ ê° ì£¼ì¥ì´ contextì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œ ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì€ LLM ì„ í™œìš©í•œë‹¤.\n",
                        "- ê³µì‹  \n",
                        "\\text{Faithfulness Score}\\;=\\;\\cfrac{\\text{ì£¼ì–´ì§„\\;context\\;ì—ì„œ\\;ì¶”ë¡ í• \\;ìˆ˜\\;ìˆëŠ”\\;ì£¼ì¥ì˜\\;ê°œìˆ˜}}{\\text{ì´\\;ì£¼ì¥\\;ê°œìˆ˜}}' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.511166835748944}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.511166835748944\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 10_AdvancedRAG]\n",
                        "\n",
                        "MapReduce RAG ë°©ì‹  \n",
                        "ê°œìš”  \n",
                        "- RAG(Retrieval-Augmented Generation)ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ì¤‘ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë§Œì„ ì„ ë³„í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n",
                        "- ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì§ˆë¬¸ ë‹µë³€ì— ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë¬¸ì„œë§Œì„ LLMì„ í†µí•´ ì„ ë³„í•œ í›„ ì „ë‹¬í•˜ëŠ” ë°©ì‹ì´ë‹¤.  \n",
                        "MapReduce ë°©ì‹ í”„ë¡œì„¸ìŠ¤\n",
                        "1. Map (ë¬¸ì„œ ê²€ìƒ‰)\n",
                        "- ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ëŠ”ë‹¤.\n",
                        "- ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¨ìˆœ ë²¡í„° ìœ ì‚¬ë„ë§Œ ê³ ë ¤í•˜ë¯€ë¡œ ì§ˆë¬¸ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ëŠ” ë¬¸ì„œë„ í¬í•¨ë  ìˆ˜ ìˆë‹¤.\n",
                        "2. Reduce (ë¬¸ì„œ ì„ ë³„ ë° ìš”ì•½)\n",
                        "- ê²€ìƒ‰ëœ ê° ë¬¸ì„œê°€ ì§ˆë¬¸ ë‹µë³€ì— ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ì§€ LLMì—ê²Œ í‰ê°€ ìš”ì²­í•œë‹¤.\n",
                        "- ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ë§Œ ì„ ë³„í•˜ì—¬ ìš”ì•½í•˜ê±°ë‚˜ ê²°í•©í•œë‹¤.\n",
                        "- í•„ìš”ì‹œ ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë” ì‘ë‹µì— ì í•©í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.\n",
                        "3. Generate (ìµœì¢… ë‹µë³€ ìƒì„±)\n",
                        "- ì§ˆë¬¸ê³¼ ì„ ë³„ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ LLMì— ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•œë‹¤.  \n",
                        "ì¥ë‹¨ì   \n",
                        "- **ì¥ì **\n",
                        "- **ë†’ì€ ì •í™•ë„**: ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.\n",
                        "- **ë…¸ì´ì¦ˆ ì œê±°**: ìœ ì‚¬í•˜ì§€ë§Œ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¡œ ì¸í•œ í˜¼ë™ì„ ë°©ì§€í•œë‹¤.\n",
                        "- **ì»¨í…ìŠ¤íŠ¸ ìµœì í™”**: ì œí•œëœ í† í° ë²”ìœ„ ë‚´ì—ì„œ ê°€ì¥ ìœ ìš©í•œ ì •ë³´ë§Œ ì „ë‹¬í•œë‹¤.\n",
                        "- **í™•ì¥ì„±**: ë§ì€ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì–´ë„ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì„ ë³„í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.\n",
                        "- ë‹¨ì \n",
                        "- **ì¶”ê°€ ë¹„ìš©**: ë¬¸ì„œ ì„ ë³„ì„ ìœ„í•œ LLM í˜¸ì¶œë¡œ ì¸í•œ ë¹„ìš©ì´ ì¦ê°€í•œë‹¤.\n",
                        "- **ì²˜ë¦¬ ì‹œê°„**: ë¬¸ì„œ í‰ê°€ ë‹¨ê³„ê°€ ì¶”ê°€ë˜ì–´ ì‘ë‹µ ì†ë„ê°€ ì €í•˜ëœë‹¤.\n",
                        "- **ë³µì¡ì„±**: êµ¬í˜„ê³¼ ê´€ë¦¬ê°€ ë” ë³µì¡í•˜ë‹¤.\n",
                        "- **ì˜ì¡´ì„±**: ë¬¸ì„œ ì„ ë³„ ì„±ëŠ¥ì´ LLMì˜ íŒë‹¨ ëŠ¥ë ¥ì— í¬ê²Œ ì˜ì¡´í•œë‹¤.' metadata={'source': '', 'source_file': '10_AdvancedRAG.ipynb', 'lecture_title': '10_AdvancedRAG', 'cell_type': 'markdown', 'cell_index': 27, 'code_snippet': '```python\\n# reduce: ì§ˆë¬¸ê³¼ ë¬¸ì„œê°„ì˜ ì§ì ‘ì  ì—°ê´€ì´ ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•´ì„œ ì—°ê´€ìˆëŠ” ë¬¸ì„œë§Œ ë°˜í™˜.\\nreduce_prompt = ChatPromptTemplate.from_template(\\n    template=\"\"\"# Instruction\\nContextì˜ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ì…ë ¥ëœ contextë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê³  ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.\\n- ê´€ë ¨ì„± íŒë‹¨ê¸°ì¤€\\n    - Contextì— ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì´ë‚˜ ë‹µì„ ìœ„í•œ ë‹¨ì„œê°€ ìˆì–´ì•¼ í•œë‹¤.\\n    - Contextì˜ ì •ë³´ê°€ ì§ˆë¬¸ì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì¤˜ì•¼ í•œë‹¤.\\n    \\n# Context\\n{context}\\n\\n# ì§ˆë¬¸\\n{query}\\n\\n# Output Indicator\\n- ì§ˆë¬¸ê³¼ Contextê°€ ê´€ë ¨ìˆìœ¼ë©´ contextì˜ ë‚´ìš©ë§Œ ì •í™•íˆ ê·¸ëŒ€ë¡œ ë°˜í™˜í•œë‹¤. ì–´ë– í•œ ë‚´ìš©ë„ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤. \\n- ì§ˆë¬¸ê³¼ Contextê°€ ê´€ë ¨ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤. \\n\"\"\"\\n)\\n\\nreduce_chain = reduce_prompt | ChatOpenAI(model=\"gpt-5-mini\") | StrOutputParser()\\n```\\n\\n```python\\n# í…ŒìŠ¤íŠ¸\\n# reduce_chain.invoke({\"context\":\"ì‚¬ê³¼ëŠ” ê³¼ì¼ì…ë‹ˆë‹¤.\", \"query\":\"ì˜¬ë¦¼í”½ ì¢…ëª©ì— ëŒ€í•´ ì•Œë ¤ì¤˜.\"})\\nreduce_chain.invoke({\"context\":\"ì˜¬ë¦¼í”½ì—ëŠ” 300ì—¬ê°œì˜ ì¢…ëª©ì´ ìˆìŠµë‹ˆë‹¤.\", \"query\":\"ì˜¬ë¦¼í”½ ì¢…ëª©ì— ëŒ€í•´ ì•Œë ¤ì¤˜.\"})\\n```\\n\\n```python\\nfrom langchain_core.runnables import chain\\n@chain\\ndef map_reduce(inputs:dict) -> str:\\n    \"\"\"\\n    Retrieverê°€ ì¡°íšŒí•œ Documentë“¤ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ì„œ ì§ˆë¬¸ê³¼ ê°œë³„ ë¬¸ì„œê°„ì˜ ê´€ë ¨ì„±ì„ reduce_chainì„ ì´ìš©í•´ ê²€ì‚¬í•œë‹¤.\\n    document ë“¤ ì¤‘ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë“¤ë§Œ ì¶”ë ¤ì„œ ë°˜í™˜í•œë‹¤.\\n\\n    Args:\\n        inputs(dict) - dict[list[Document], query:str]  {Retrieverê°€ ì¡°íšŒí•œ ë¬¸ì„œë“¤, ì‚¬ìš©ì ì§ˆë¬¸}\\n    \"\"\" \\n    docs = inputs[\\'documents\\']\\n    query = inputs[\\'query\\']\\n    contexts = \"\"  # ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ” ë¬¸ì„œë“¤ì„ ëª¨ì„ str\\n    for doc in docs:\\n        res = reduce_chain.invoke({\"context\":doc.page_content, \"query\":query})\\n        if res.strip() != None:\\n            contexts += res+\"\\\\n\\\\n\"\\n\\n    return contexts.strip()\\n\\nretriever = get_retriever(vectorstore, k=10) # KëŠ” ì¢€ í¬ê²Œí•´ì„œ ë§ì´ ì¡°íšŒí•œë‹¤.\\n\\nmap_reduce_chain = {\\n    \"documents\": retriever, \"query\": RunnablePassthrough()\\n} | map_reduce\\n```\\n\\n```python\\nmap_reduce_chain.invoke(\"êµ­ì œ ì˜¬ë¦¼í”½ ê¸°êµ¬ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.\")\\n```\\n\\n```python\\n# # ìµœì¢… chain\\n# prompt = ChatPromptTemplate.from_template(\\n    template=\"\"\"# Instruction\\në‹¹ì‹ ì€ ì˜¬ë¦¼í”½ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤.\\nì œê³µëœ Contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\në§Œì•½ Contextì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ ì—†ìœ¼ë©´ \"ì •í™•í•œ ì •ë³´ê°€ ì—†ì–´ì„œ ë‹µì„ ì•Œ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\" ë¼ê³  ëŒ€ë‹µí•©ë‹ˆë‹¤.\\nContextì— ì—†ëŠ” ë‚´ìš©ì„ ë‹µë³€ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n\\n# ì§ˆë¬¸\\n{query}\\n\\n# Context\\n{context}\\n\\n# Output Indicator\\n- ë‹µë³€ì„ ë§Œë“¤ ë•Œ ì°¸ì¡°í•œ contextì˜ ë‚´ìš©ì„ ê°™ì´ ì¶œë ¥í•©ë‹ˆë‹¤.\\n- ì°¸ì¡° ë‚´ìš©ì€ ê°ì£¼ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\\n\\n# ì¶œë ¥ ì˜ˆ:\\në‹µë³€ ë‚´ìš©[1] ë‹µë³€ë‚´ìš©[2]\\n\\n[1] ì°¸ì¡°ë‚´ìš©1\\n[2] ì°¸ì¡°ë‚´ìš©2\\n\"\"\"\\n)\\nmodel = ChatOpenAI(model=\"gpt-5-mini\")\\nchain = {\\n    \"context\":map_reduce_chain, \"query\": RunnablePassthrough()\\n} | prompt | model | StrOutputParser()\\n```\\n\\n```python\\nresponse = chain.invoke(\"ì˜¬ë¦¼í”½ ê²½ê¸° ì¢…ëª©ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.\")\\n```', 'chunk_index': 4, 'original_score': 0.5625964627574787}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5625964627574787\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì‹¤ìŠµ' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '```python\\n# !uv pip install ragas rapidfuzz\\n# ì„¤ì¹˜ í›„ ì»¤ë„ ì¬ì‹œì‘\\n```\\n\\n```python\\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\\nfrom qdrant_client import QdrantClient, models\\nfrom qdrant_client.models import Distance, SparseVectorParams, VectorParams\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\n# ##############################################################\\n# ë°ì´í„° ì¤€ë¹„\\n# def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\\n    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\\n        olympic_text = fr.read()\\n\\n    # Split\\n    splitter = MarkdownHeaderTextSplitter(\\n        headers_to_split_on=[\\n            (\"#\", \"Header 1\"),\\n            (\"##\", \"Header 2\"),\\n            (\"###\", \"Header 3\"),\\n        ],\\n        # strip_headers=False, # ë¬¸ì„œì— header í¬í•¨ ì—¬ë¶€(default: True - ì œê±°)\\n    )\\n\\n    return splitter.split_text(olympic_text)\\n```\\n\\n```python\\n# # Vector DB ì—°ê²°\\n# retriever ìƒì„±\\n# def get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\\n\\n    dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\n    client = QdrantClient(host=\"localhost\", port=6333)\\n\\n    # ì»¬ë ‰ì…˜ ì‚­ì œ\\n    if client.collection_exists(collection_name):\\n        result = client.delete_collection(collection_name=collection_name)\\n\\n    # ì»¬ë ‰ì…˜ ìƒì„±\\n    client.create_collection(\\n        collection_name=collection_name,\\n        vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\\n    )\\n\\n    vectorstore = QdrantVectorStore(\\n        client=client,\\n        collection_name=collection_name,    \\n        embedding=dense_embeddings\\n    )\\n    \\n    # # Documentë“¤ ì¶”ê°€\\n    # documents = load_and_split_olympic_data()\\n    vectorstore.add_documents(documents=documents)\\n\\n    return vectorstore\\n\\ndef get_retriever(vectorstore, k: int = 5):\\n    retriever = vectorstore.as_retriever(\\n        search_kwargs={\"k\": k}\\n    )\\n    return retriever\\n```\\n\\n```python\\nvectorstore = get_vectorstore()\\n\\nretriever = get_retriever(vectorstore)\\nretriever\\n```\\n\\n```python\\n# # í‰ê°€í•  RAG Chain\\n# from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough \\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.documents import Document\\n\\nvectorstore = get_vectorstore()\\nretriever = get_retriever(vectorstore)\\n\\nprompt_txt = \"\"\"# Instruction:\\në‹¹ì‹ ì€ ì •ë³´ì œê³µì„ ëª©ì ìœ¼ë¡œí•˜ëŠ” ìœ ëŠ¥í•œ AI Assistant ì…ë‹ˆë‹¤.\\nì£¼ì–´ì§„ contextì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° \"ì •ë³´ê°€ ë¶€ì¡±í•´ ë‹µì„ í•  ìˆ˜ì—†ìŠµë‹ˆë‹¤.\" ë¼ê³  ë‹µí•©ë‹ˆë‹¤.\\nì ˆëŒ€ ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µì„ í•˜ê±°ë‚˜ Context ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ì„œ ë‹µë³€í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\\n\\n# Context:\\n{context}\\n\\n# ì§ˆë¬¸:\\n{query}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(\\n    template=prompt_txt\\n)\\n\\ndef format_docs(documents:list)->str:\\n    \"\"\"\\n    VectorStoreì— ì¡°íšŒí•œ ë¬¸ì„œë“¤ì—ì„œ ë‚´ìš©(page_content)ë§Œ ì¶”ì¶œí•´ì„œ strìœ¼ë¡œ í•©ì³ì„œ ë°˜í™˜.\\n    VectorStoreì˜ ê²€ìƒ‰ê²°ê³¼ì¸ List[Document]ë¥¼ ë°›ì•„ì„œ Documentë“¤ì—ì„œ page_contentì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•œë‹¤.\\n    \\n    Args:\\n        documents(list[Document]): [Document(..), Document(...), ..]}\\n    Returns:\\n        str: ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ \"\\\\n\\\\n\"ìœ¼ë¡œ ì—°ê²°í•œ string\\n    \"\"\"\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in documents)\\n\\nmodel = ChatOpenAI(model=\"gpt-5-mini\")\\nparser = StrOutputParser()\\n\\n# chain = {\\n#     \"context\":retriever | format_docs,\\n#     \"query\":RunnablePassthrough()\\n# } | prompt | model | parser\\n\\n# RAG í‰ê°€ë¥¼ ìœ„í•´ì„œ \"ë‹µë³€\", \"ê²€ìƒ‰í•œ ë¬¸ì„œ\" ë‘˜ì´ ì¶œë ¥ë˜ë„ë¡ ë³€ê²½.\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\\nfrom operator import itemgetter\\n\\ndef format_doc_list(docs_dict: dict) -> list:\\n    # dictionary[list[Document], query:str] -> list[str]  \\n    # ë¬¸ì„œë‚´ìš©ë§Œ ì¶”ì¶œí•´ì„œ(Document.page_content)ë§Œ ì¶”ì¶œí•œ ë¦¬ìŠ¤íŠ¸\\n    return [doc.page_content for doc in docs_dict[\\'context\\']]\\n\\n# dict | dict -> dict\\n# RunnablePassthrough() | dict | dict ==> RunnableSequence\\nchain = RunnablePassthrough() | {\\n    \"context\":retriever,\\n    \"query\":RunnablePassthrough()\\n} | {\\n    \"response\": prompt | model | parser,\\n    \"retrieved_context\": format_doc_list, # RAGAS í‰ê°€ì‹œ context -> List[str]\\n}\\n```\\n\\n```python\\nres = chain.invoke(\"1íšŒ ì˜¬ë¦¼í”½ì€ ì–¸ì œ ì–´ë””ì„œ ì—´ë ¸ì§€\")\\n```', 'chunk_index': 8, 'original_score': 0.6509500425471539}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6509500425471539\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 5ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAG í‰ê°€ ê°œìš”\n",
                        "- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
                        "- **í‰ê°€ ìš”ì†Œ**\n",
                        "- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\n",
                        "- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\n",
                        "- **ìƒì„± ë‹¨ê³„ í‰ê°€**\n",
                        "- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\n",
                        "- **í‰ê°€ ë°©ë²•**\n",
                        "- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\n",
                        "1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\n",
                        "- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\n",
                        "2. **ì˜¨ë¼ì¸ í‰ê°€**\n",
                        "- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\n",
                        "1. ì •ëŸ‰ì  í‰ê°€\n",
                        "- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\n",
                        "2. ì •ì„±ì  í‰ê°€\n",
                        "- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n!uv pip install ragas rapidfuzz\\n```', 'chunk_index': 0, 'original_score': 0.6932134290727161}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6932134290727161\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì§€í‘œ ê°œìš”  \n",
                        "- **Generation**\n",
                        "- llm ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë“¤.\n",
                        "- **Faithfulness(ì‹ ë¢°ì„±)**\n",
                        "- ìƒì„±ëœ ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ\n",
                        "- ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ì— ëŒ€í•œ í‰ê°€ë¡œ ë³¼ ìˆ˜ìˆë‹¤.\n",
                        "- **Answer relevancy(ë‹µë³€ ì í•©ì„±)**\n",
                        "- ìƒì„±ëœ ë‹µë³€ê³¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ\n",
                        "- ìƒì„±ëœ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- **Retrieval**\n",
                        "- ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰í•œ ë¬¸ì„œ(context)ë“¤ì— ëŒ€í•œ í‰ê°€\n",
                        "- **Context Precision(ë¬¸ë§¥ ì •ë°€ë„)**\n",
                        "- ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë“¤ ì¤‘ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ê²ƒë“¤ì´ **ì–¼ë§ˆë‚˜ ìƒìœ„ ìˆœìœ„ì— ìœ„ì¹˜í•˜ëŠ”ì§€** í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- **Context Recall(ë¬¸ë§¥ ì¬í˜„ë¥ )**\n",
                        "- ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°€ ì •ë‹µ(ground-truth)ì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì§€í‘œ.\n",
                        "- ì´ëŸ¬í•œ ì§€í‘œë“¤ì€ RAG íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ëŠ” ë° í™œìš©ëœë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5514420155020314}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5514420155020314\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS ë¥¼ ì´ìš©í•´ í‰ê°€ë¥¼ ìœ„í•œ í•©ì„± ë°ì´í„° ì…‹ ë§Œë“¤ê¸°  \n",
                        "- í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì„±\n",
                        "- `user_input`: ì‚¬ìš©ì ì§ˆë¬¸\n",
                        "- `retrieved_contexts`: Vectorstoreì—ì„œ ê²€ìƒ‰í•œ context\n",
                        "- `response`: LLMì˜ ì‘ë‹µ\n",
                        "- `reference`: ì •ë‹µ  \n",
                        "TestsetGenerator\n",
                        "- **ë¬¸ì„œ(retrieved_contexts)ë¥¼ ê¸°ì¤€**ìœ¼ë¡œ **ì§ˆë¬¸**, **ì •ë‹µ** ì„ ìƒì„±í•œë‹¤.\n",
                        "- í‰ê°€í•  LLMìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸ì„ ë„£ì–´ ë‹µë³€ì„ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 22, 'code_snippet': '```python\\n# ì£¼í”¼í„°ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ë¹„ë™ê¸°ì  ì²˜ë¦¬ ìœ„í•´\\n# script(.py) ë¡œ ì‘ì„±í•  ê²½ìš°ëŠ” í•„ìš” ì—†ë‹¤.\\n\\nimport nest_asyncio\\nnest_asyncio.apply()\\n```\\n\\n```python\\n# # ë°ì´í„°ì…‹ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•  contextë¥¼ ì¶”ì¶œ - sampling\\n# client = QdrantClient(host=\"localhost\", port=6333)\\nCOLLECTION_NAME = \"olympic_info_wiki\"\\n\\n# ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ì¡°íšŒí•´ì„œ ê·¸ ì¤‘ ëœë¤í•˜ê²Œ Kê°œë§Œ sampling\\ninfo = client.get_collection(COLLECTION_NAME)\\n# info.points_count # ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜\\nresults, next_id = client.scroll(\\n    collection_name=COLLECTION_NAME,\\n    limit=info.points_count\\n)\\n\\n# sampling\\nimport random\\nsample_dataset = random.sample(results, 5) # ë¦¬ìŠ¤íŠ¸ì—ì„œ ëœë¤í•˜ê²Œ Kê°œë¥¼ ì¶”ì¶œ\\n\\n# ë¬¸ì„œ ë‚´ìš©ë§Œ ì¶”ì¶œ\\ndocs = [point.payload[\\'page_content\\'] for point in sample_dataset]\\ndocs\\n```\\n\\n```python\\nfrom ragas.testset import TestsetGenerator\\nfrom ragas.llms import LangchainLLMWrapper\\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\\n\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\n# gpt-5 ë²„ì „ì€ ì‚¬ìš©í•  ìˆ˜ì—†ë‹¤. (temperature ì„¤ì • ë¬¸ì œê°€ ìˆë‹¤.)\\ngenerator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\")) #Langchainëª¨ë¸ì„ RAGASì—ì„œ ì‚¬ìš©í•  ìˆ˜ìˆë„ë¡ ë³€í™˜(Wrapping)\\ngenerator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\\n\\ngenerator = TestsetGenerator(\\n    llm=generator_llm,\\n    embedding_model=generator_embeddings,\\n    llm_context=\"ë°ì´í„°ì…‹ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. ë°ì´í„°ì…‹ì€ JSON ë¬¸ë²•ì„ ê¼­ ì§€ì¼œì„œ ì‘ì„±í•œë‹¤.\\\\\\níŠ¹íˆ êµ¬ë‘ì ì€ ê¼­ ì§€ì¼œì•¼ í•œë‹¤. Documentì— JSON ë¬¸ë²•ì— ë§ì§€ ì•ŠëŠ” í‘œí˜„ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ìˆ˜ì •í•´ì„œ ì²˜ë¦¬í•œë‹¤.\"  \\n    # ì§ˆë¬¸/ë‹µë³€ ìƒì„±í•  ë•Œ llmì— ì „ë‹¬í•  ì¶”ê°€ system promptë¥¼ ì„¤ì •.\\n)\\n```\\n\\n```python\\ntestset = generator.generate_with_chunks(\\n    docs, testset_size=10  # context ë‚´ìš©, í…ŒìŠ¤íŠ¸ë°ì´í„°ì…‹ ëª‡ê°œë¥¼ ë§Œë“¤ì§€.\\n)\\n```\\n\\n```python\\nsample1  = testset.samples[0].eval_sample\\nprint(sample1.user_input)         # ì‚¬ìš©ì ì§ˆë¬¸\\nprint(sample1.reference_contexts) # Vector DBì˜ context (ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ user_inputê³¼ refereneceë¥¼ ìƒì„±)\\nprint(sample1.retrieved_contexts) # RAG pipelineì´ ë²¡í„°ë””ë¹„ì—ì„œ ê²€ìƒ‰í•œ ë¬¸ì„œ\\nprint(sample1.reference)          # ì •ë‹µ(ground truth)\\nprint(sample1.response)           # LLM ì‘ë‹µ (ì •ë‹µì¶”ì •ê°’)\\n\\n# í‰ê°€ì‹œ í•„ìš”í•œ ì†ì„±: user_input, retrieved_contexts, reference, response\\n#   retrieved_context, response -> í‰ê°€í•  RAG Chainìœ¼ë¡œ ë¶€í„° ê°€ì ¸ì™€ì•¼ í•œë‹¤.\\n```\\n\\n```python\\n# Testsetì„ pandas DataFrameì„ ë³€í™˜ -> to_pandas(), to_xxxx() ë¥¼ í†µí•´ì„œ ë‹¤ë¥¸ êµ¬ì¡°ë¡œ ë³€í™˜ì´ ê°€ëŠ¥.\\neval_df = testset.to_pandas()\\neval_df\\n```\\n\\n```python\\nres = chain.invoke(eval_df[\\'user_input\\'][0])\\n```\\n\\n```python\\nres[\\'response\\']\\nres[\\'retrieved_context\\']\\n```\\n\\n```python\\n# Chain ì‘ë‹µë“¤ì„ ì €ì¥í•  list\\nresponse_list = []\\n# Chain ì´ ë°˜í™˜í•œ contextë“¤ì„ ì§€ì •í•  list\\nretrieved_context_list = []\\n\\nfor user_input in eval_df[\\'user_input\\']:\\n    res = chain.invoke(user_input)\\n    response_list.append(res[\\'response\\'])\\n    retrieved_context_list.append(res[\\'retrieved_context\\'])\\n```\\n\\n```python\\nprint(len(response_list), len(retrieved_context_list))\\n```\\n\\n```python\\n# eval_df ì— ì¶”ê°€\\neval_df[\\'response\\'] = response_list\\neval_df[\\'retrieved_contexts\\'] = retrieved_context_list\\neval_df.head()\\n```\\n\\n```python\\n# # EvaluationDatasetì„ ìƒì„± -> RAGAS í‰ê°€ ë°ì´í„°ì…‹ íƒ€ì…\\n# from ragas import EvaluationDataset\\n# from_xxxx() xxxx íƒ€ì…ì˜ ê°ì²´ë¥¼ EvaluationDatasetê°ì²´ë¡œ ë³€í™˜.\\neval_dataset = EvaluationDataset.from_pandas(\\n    eval_df[[\"user_input\", \"retrieved_contexts\", \"response\", \"reference\"]]\\n)\\neval_dataset\\n```\\n\\n```python\\n# # í‰ê°€\\n# from ragas.metrics import (\\n    LLMContextRecall, # Context Recall í‰ê°€í•¨ìˆ˜\\n    LLMContextPrecisionWithReference, # Context Precision\\n    Faithfulness, \\n    AnswerRelevancy\\n)\\n\\nfrom ragas import evaluate\\n\\neval_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\\neval_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\\n# í‰ê°€í•  í•¨ìˆ˜ë“¤ì„ Listë¡œ ë¬¶ì–´ì¤€ë‹¤.\\nmetrics = [\\n    LLMContextRecall(llm=eval_llm),\\n    LLMContextPrecisionWithReference(llm=eval_llm),\\n    Faithfulness(llm=eval_llm),\\n    AnswerRelevancy(llm=eval_llm, embeddings=eval_embeddings)\\n]\\n\\n# í‰ê°€ë¥¼ ì§„í–‰ - evaluate() í•¨ìˆ˜ ì´ìš©\\neval_result = evaluate(dataset=eval_dataset, metrics=metrics)\\n```\\n\\n```python\\nprint(type(eval_result))\\neval_result\\n```\\n\\n```python\\n# ê°œë³„ Pointì— ëŒ€í•œ í‰ê°€ ê²°ê³¼.\\nresult_df = eval_result.to_pandas()\\nresult_df\\n```', 'chunk_index': 9, 'original_score': 0.5223433825863857}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5223433825863857\n",
                        "--------------------------------------------------\n",
                        "4. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAGAS í‰ê°€ ì‹¤ìŠµ' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '```python\\n# !uv pip install ragas rapidfuzz\\n# ì„¤ì¹˜ í›„ ì»¤ë„ ì¬ì‹œì‘\\n```\\n\\n```python\\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\\nfrom qdrant_client import QdrantClient, models\\nfrom qdrant_client.models import Distance, SparseVectorParams, VectorParams\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\n# ##############################################################\\n# ë°ì´í„° ì¤€ë¹„\\n# def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\\n    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\\n        olympic_text = fr.read()\\n\\n    # Split\\n    splitter = MarkdownHeaderTextSplitter(\\n        headers_to_split_on=[\\n            (\"#\", \"Header 1\"),\\n            (\"##\", \"Header 2\"),\\n            (\"###\", \"Header 3\"),\\n        ],\\n        # strip_headers=False, # ë¬¸ì„œì— header í¬í•¨ ì—¬ë¶€(default: True - ì œê±°)\\n    )\\n\\n    return splitter.split_text(olympic_text)\\n```\\n\\n```python\\n# # Vector DB ì—°ê²°\\n# retriever ìƒì„±\\n# def get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\\n\\n    dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\n    client = QdrantClient(host=\"localhost\", port=6333)\\n\\n    # ì»¬ë ‰ì…˜ ì‚­ì œ\\n    if client.collection_exists(collection_name):\\n        result = client.delete_collection(collection_name=collection_name)\\n\\n    # ì»¬ë ‰ì…˜ ìƒì„±\\n    client.create_collection(\\n        collection_name=collection_name,\\n        vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\\n    )\\n\\n    vectorstore = QdrantVectorStore(\\n        client=client,\\n        collection_name=collection_name,    \\n        embedding=dense_embeddings\\n    )\\n    \\n    # # Documentë“¤ ì¶”ê°€\\n    # documents = load_and_split_olympic_data()\\n    vectorstore.add_documents(documents=documents)\\n\\n    return vectorstore\\n\\ndef get_retriever(vectorstore, k: int = 5):\\n    retriever = vectorstore.as_retriever(\\n        search_kwargs={\"k\": k}\\n    )\\n    return retriever\\n```\\n\\n```python\\nvectorstore = get_vectorstore()\\n\\nretriever = get_retriever(vectorstore)\\nretriever\\n```\\n\\n```python\\n# # í‰ê°€í•  RAG Chain\\n# from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough \\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.documents import Document\\n\\nvectorstore = get_vectorstore()\\nretriever = get_retriever(vectorstore)\\n\\nprompt_txt = \"\"\"# Instruction:\\në‹¹ì‹ ì€ ì •ë³´ì œê³µì„ ëª©ì ìœ¼ë¡œí•˜ëŠ” ìœ ëŠ¥í•œ AI Assistant ì…ë‹ˆë‹¤.\\nì£¼ì–´ì§„ contextì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ í•©ë‹ˆë‹¤.\\nContextì— ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° \"ì •ë³´ê°€ ë¶€ì¡±í•´ ë‹µì„ í•  ìˆ˜ì—†ìŠµë‹ˆë‹¤.\" ë¼ê³  ë‹µí•©ë‹ˆë‹¤.\\nì ˆëŒ€ ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µì„ í•˜ê±°ë‚˜ Context ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ì„œ ë‹µë³€í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.\\n\\n# Context:\\n{context}\\n\\n# ì§ˆë¬¸:\\n{query}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(\\n    template=prompt_txt\\n)\\n\\ndef format_docs(documents:list)->str:\\n    \"\"\"\\n    VectorStoreì— ì¡°íšŒí•œ ë¬¸ì„œë“¤ì—ì„œ ë‚´ìš©(page_content)ë§Œ ì¶”ì¶œí•´ì„œ strìœ¼ë¡œ í•©ì³ì„œ ë°˜í™˜.\\n    VectorStoreì˜ ê²€ìƒ‰ê²°ê³¼ì¸ List[Document]ë¥¼ ë°›ì•„ì„œ Documentë“¤ì—ì„œ page_contentì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•œë‹¤.\\n    \\n    Args:\\n        documents(list[Document]): [Document(..), Document(...), ..]}\\n    Returns:\\n        str: ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ \"\\\\n\\\\n\"ìœ¼ë¡œ ì—°ê²°í•œ string\\n    \"\"\"\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in documents)\\n\\nmodel = ChatOpenAI(model=\"gpt-5-mini\")\\nparser = StrOutputParser()\\n\\n# chain = {\\n#     \"context\":retriever | format_docs,\\n#     \"query\":RunnablePassthrough()\\n# } | prompt | model | parser\\n\\n# RAG í‰ê°€ë¥¼ ìœ„í•´ì„œ \"ë‹µë³€\", \"ê²€ìƒ‰í•œ ë¬¸ì„œ\" ë‘˜ì´ ì¶œë ¥ë˜ë„ë¡ ë³€ê²½.\\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\\nfrom operator import itemgetter\\n\\ndef format_doc_list(docs_dict: dict) -> list:\\n    # dictionary[list[Document], query:str] -> list[str]  \\n    # ë¬¸ì„œë‚´ìš©ë§Œ ì¶”ì¶œí•´ì„œ(Document.page_content)ë§Œ ì¶”ì¶œí•œ ë¦¬ìŠ¤íŠ¸\\n    return [doc.page_content for doc in docs_dict[\\'context\\']]\\n\\n# dict | dict -> dict\\n# RunnablePassthrough() | dict | dict ==> RunnableSequence\\nchain = RunnablePassthrough() | {\\n    \"context\":retriever,\\n    \"query\":RunnablePassthrough()\\n} | {\\n    \"response\": prompt | model | parser,\\n    \"retrieved_context\": format_doc_list, # RAGAS í‰ê°€ì‹œ context -> List[str]\\n}\\n```\\n\\n```python\\nres = chain.invoke(\"1íšŒ ì˜¬ë¦¼í”½ì€ ì–¸ì œ ì–´ë””ì„œ ì—´ë ¸ì§€\")\\n```', 'chunk_index': 8, 'original_score': 0.6915045533333333}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6915045533333333\n",
                        "--------------------------------------------------\n",
                        "5. page_content='[ê°•ì˜: 10_AdvancedRAG]\n",
                        "\n",
                        "Advanced RAGë€ ë¬´ì—‡ì¸ê°€  \n",
                        "**RAG**ëŠ” LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì´ ë‹µì„ ë§Œë“¤ ë•Œ, **ì™¸ë¶€ ì§€ì‹(ë¬¸ì„œ/DB/ì›¹ ë“±)ì„ ê²€ìƒ‰(Retrieval)í•´ì„œ ê·¼ê±°(Context)ë¡œ ë¶™ì¸ ë’¤ ìƒì„±(Generation)**í•˜ëŠ” êµ¬ì¡°ì´ë‹¤. ê·¸ëŸ°ë° ê¸°ë³¸ RAG(Naive RAG)ëŠ” ì‹¤ë¬´ì—ì„œ ë‹¤ìŒ ë¬¸ì œê°€ ìˆë‹¤.  \n",
                        "- ê²€ìƒ‰ì´ \"ë¹„ìŠ·í•œ ê²ƒ\"ì€ ì°¾ëŠ”ë° \"ì •ë‹µ ê·¼ê±°\"ë¥¼ ì œëŒ€ë¡œ ëª» ì°¾ëŠ” ê²½ìš°\n",
                        "- ê·¼ê±°ê°€ ìˆì–´ë„ LLMì´ **hallucination(í™˜ê°)**ë¥¼ ì„ê±°ë‚˜, ê·¼ê±°ì™€ ë‹¤ë¥¸ ë‹µì„ í•˜ëŠ” ê²½ìš°\n",
                        "- ê²€ìƒ‰í•œ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ê³  ë³µì¡í•´ì„œ **ì •ë‹µì— í•„ìš”í•œ ë¶€ë¶„**(**chunk**)ì„ LLMì´ ì œëŒ€ë¡œ ì°¾ì§€ ëª»í•´ ë‹µë³€ í’ˆì§ˆì´ ì•ˆì¢‹ì€ ê²½ìš°  \n",
                        "**Advanced RAG**ëŠ” ì´ëŸ° \"í˜„ì—…í˜• ë¬¸ì œ\"ë¥¼ ì¤„ì´ê¸° ìœ„í•´ **ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë‹¨ê³„, ê²€ìƒ‰ ì „ë‹¨ê³„, ê²€ìƒ‰ í›„ ë‹¨ê³„, ìƒì„± ë‹¨ê³„ì—ì„œ ê³ ë„í™”**í•˜ëŠ” ì„¤ê³„ íŒ¨í„´ì´ë‹¤.\n",
                        "ê¸°ë³¸ RAG(Naive RAG)ê°€ \"ì‚¬ëŒì´ ëŒ€ì¶© ì°¾ì•„ì¤€ ì°¸ê³ ìë£Œë¡œ ê¸€ ì“°ëŠ” ê²ƒ\"ì´ë¼ë©´, Advanced RAGëŠ” \"ì‚¬ì„œ(ê²€ìƒ‰) + í¸ì§‘ì(ì •ì œ) + íŒ©íŠ¸ì²´ì»¤(ê²€ì¦) + ì‘ê°€(ìƒì„±)ê°€ í˜‘ì—…í•˜ëŠ” íŒŒì´í”„ë¼ì¸\"ì— ê°€ê¹ë‹¤.  \n",
                        "ì¢…ë¥˜  \n",
                        "ê²€ìƒ‰ í’ˆì§ˆì„ ì˜¬ë¦¬ëŠ” ê³ ê¸‰ Retrieval  \n",
                        "- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í‚¤ì›Œë“œ(BM25) + ë²¡í„°(ì„ë² ë”©) ì¡°í•©\n",
                        "- **ë©€í‹°-ì¿¼ë¦¬/ì¿¼ë¦¬ ì¬ì‘ì„±(Query rewriting)**: ì§ˆë¬¸ì„ ë” ì˜ ê²€ìƒ‰ë˜ê²Œ ë°”ê¿” ê²€ìƒ‰ ì„±ê³µë¥ ì„ ì˜¬ë¦¼\n",
                        "- **ë©”íƒ€ë°ì´í„° í•„í„°ë§**: ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í†µí•´ ë²”ìœ„ë¥¼ ì¢í˜€ **ì •í™•ë„**ë¥¼ ì˜¬ë¦¼. ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì¡°í•©í•œ íš¨ê³¼.\n",
                        "- **ë¦¬ë­í‚¹(Re-ranking)**: í›„ë³´ ë¬¸ì„œë¥¼ ë§ì´ ê°€ì ¸ì˜¨ ë’¤, \"ì •ë‹µì— ê°€ê¹Œìš´ ìˆœì„œ\"ë¡œ ë‹¤ì‹œ ì •ë ¬  \n",
                        "Chunking/ì¸ë±ì‹± ì „ëµ ê³ ë„í™”\n",
                        "- ë²¡í„° ë°ì´í„° ë² ì´ìŠ¤ êµ¬ì¶•ì‹œ ì–´ë–¤ êµ¬ì¡°ë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³  Indexing í• ì§€ì˜ ì „ëµ\n",
                        "- **êµ¬ì¡° ê¸°ë°˜ ë¶„í• **: í—¤ë”/ì„¹ì…˜/í‘œ/ì½”ë“œë¸”ë¡\n",
                        "- **ê³„ì¸µí˜• ì¸ë±ìŠ¤**: ë¬¸ì„œ ìš”ì•½ â†’ ì„¹ì…˜ â†’ ì„¸ë¶€ chunk, Parent-Child êµ¬ì¡°\n",
                        "- **ìœˆë„ìš° í™•ì¥**: í•„ìš” ì‹œ ì• ë’¤ ë¬¸ë§¥ì„ í•¨ê»˜ ë¶™ì„  \n",
                        "ê²€ìƒ‰ ê²°ê³¼ì˜ \"ì •ì œ/ì¡°ë¦½\" ë‹¨ê³„ ì¶”ê°€  \n",
                        "- ì¤‘ë³µ ì œê±°, ë…¸ì´ì¦ˆ ì œê±°, ê´€ë ¨ ë¶€ë¶„ë§Œ ë°œì·Œ\n",
                        "- ì—¬ëŸ¬ chunkë¥¼ **ì§ˆë¬¸ ê´€ì ìœ¼ë¡œ ì¬êµ¬ì„±**\n",
                        "â†’ LLMì— ê·¸ëŒ€ë¡œ ë˜ì§€ëŠ” ê²Œ ì•„ë‹ˆë¼ \"ë‹µë³€ì— ì“°ê¸° ì¢‹ì€ ê·¼ê±° ë¬¶ìŒ\"ìœ¼ë¡œ í¸ì§‘í•˜ì—¬ ì „ë‹¬í•œë‹¤.  \n",
                        "ìƒì„± ë‹¨ê³„ì˜ ì‹ ë¢°ì„± ê°•í™”(ê°€ë“œë ˆì¼)' metadata={'source': '', 'source_file': '10_AdvancedRAG.ipynb', 'lecture_title': '10_AdvancedRAG', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n# # VectorStore, Retriever ì¤€ë¹„\\n# from dotenv import load_dotenv\\n\\nload_dotenv()\\n```\\n\\n```python\\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\\nfrom qdrant_client import QdrantClient, models\\nfrom qdrant_client.models import Distance, SparseVectorParams, VectorParams\\nfrom langchain_openai import OpenAIEmbeddings\\n```\\n\\n```python\\n# ##############################################################\\n# ë°ì´í„° ì¤€ë¹„\\n# def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\\n    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\\n        olympic_text = fr.read()\\n\\n    # Split\\n    splitter = MarkdownHeaderTextSplitter(\\n        headers_to_split_on=[\\n            (\"#\", \"Header 1\"),\\n            (\"##\", \"Header 2\"),\\n            (\"###\", \"Header 3\"),\\n        ],\\n        # strip_headers=False, # ë¬¸ì„œì— header í¬í•¨ ì—¬ë¶€(default: True - ì œê±°)\\n    )\\n\\n    return splitter.split_text(olympic_text)\\n```\\n\\n```python\\n# # Vector DB ì—°ê²°\\n# retriever ìƒì„±\\n# # collection ì‚­ì œí›„ ìƒì„± (ë°ì´í„° ë„£ì§€ëŠ” ì•ŠìŒ)\\ndef get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\\n\\n    # # Qdrant Collection ìƒì„± (sparse + dense)\\n    # dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\\n    \\n    client = QdrantClient(host=\"localhost\", port=6333)\\n\\n    #ì‚­ì œí›„ ìƒì„±\\n    if client.collection_exists(collection_name):\\n        result = client.delete_collection(collection_name=collection_name)\\n\\n    client.create_collection(\\n        collection_name=collection_name,\\n        vectors_config={\"dense\": VectorParams(size=3072, distance=Distance.COSINE)},\\n        sparse_vectors_config={ \\n            \"sparse\": SparseVectorParams()\\n        },\\n    )\\n\\n    # # VectorStore ìƒì„± (Hybrid ëª¨ë“œ)\\n    # vector_store = QdrantVectorStore(\\n        client=client,\\n        collection_name=collection_name,\\n    \\n        embedding=dense_embeddings,\\n        \\n        sparse_embedding=sparse_embeddings,\\n        retrieval_mode=RetrievalMode.HYBRID,\\n    \\n        vector_name=\"dense\",\\n        sparse_vector_name=\"sparse\",\\n    )\\n    \\n    # # Documentë“¤ ì¶”ê°€\\n    # documents = load_and_split_olympic_data()\\n    vector_store.add_documents(documents=documents)\\n\\n    return vector_store\\n\\ndef get_retriever(vector_store, k: int = 5):\\n    retriever = vector_store.as_retriever(\\n        search_kwargs={\"k\": k}\\n    )\\n    return retriever\\n```\\n\\n```python\\nvectorstore = get_vectorstore()\\nbasic_retriever = get_retriever(vectorstore)  # naive rag\\n```\\n\\n```python\\nbasic_retriever.invoke(\"ê·¼ëŒ€ ì˜¬ë¦¼í”½ì€ ì–¸ì œ ì‹œì‘ë˜ì—ˆë‚˜ìš”?\")\\n```\\n\\n```python\\n# ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì••ì¶•(ì¤„ì—¬ì„œ)í•´ì„œ Contextë¥¼ ì¬ìƒì„±í•˜ëŠ” Retriever.\\nfrom langchain_classic.retrievers import ContextualCompressionRetriever\\nfrom langchain_classic.retrievers.document_compressors import CrossEncoderReranker\\n# CrossEncoder ë¥¼ HuggingfaceHubì—ì„œ ê°€ì ¸ì˜¨ë‹¤. - ê²€ìƒ‰: NLP > text ranking > ëª¨ë¸ì´ë¦„ì— reranker ë¶™ì€ ëª¨ë¸ë“¤.\\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder \\n\\nretriever = get_retriever(vectorstore, k=10)   # Rerank í•  ë•ŒëŠ” ê²€ìƒ‰ ê°œìˆ˜ë¥¼ í¬ê²Œ ê°€ì ¸ì˜¨ë‹¤.\\n\\nreranker_model = \"BAAI/bge-reranker-v2-m3\"\\nreranker = HuggingFaceCrossEncoder(model_name=reranker_model) # HuggingFace Rerankerëª¨ë¸ ìƒì„±\\ncompressor = CrossEncoderReranker(model=reranker, top_n=5) # Compressor ìƒì„±(Contextë¥¼ ì¤„ì´ëŠ” ë°©ë²•)\\ncompression_retriever = ContextualCompressionRetriever(\\n    base_retriever=retriever,  # ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • -> ì´ê²ƒìœ¼ë¡œ k(10) ê°œ ê°€ì ¸ì˜¨ë‹¤.\\n    base_compressor=compressor # ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ê°€ ê°€ì ¸ì˜¨ ë¬¸ì„œë¥¼ ì••ì¶•í•˜ëŠ” ëª¨ë¸(Reranker) ì„¤ì •. ì—¬ê¸°ì„œ top_nê°œ ì¶”ì¶œ\\n)\\n```\\n\\n```python\\nquery = \"ì˜¬ë¦¼í”½ì— ë°œìƒí•œ ë‹¤ì–‘í•œ ë…¼ë€ë“¤ì— ëŒ€í•´ ì •ë¦¬í•´ì¤˜.\"\\nbase_result = basic_retriever.invoke(query)\\nrerank_result = compression_retriever.invoke(query)\\n```\\n\\n```python\\nbase_result_str = [doc.page_content for doc in base_result]\\nrerank_result_str = [doc.page_content for doc in rerank_result]\\n```', 'chunk_index': 0, 'original_score': 0.46361831973755346}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.46361831973755346\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 5ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 11_RAG_evaluation]\n",
                        "\n",
                        "RAG í‰ê°€ ê°œìš”\n",
                        "- RAG í‰ê°€ë€ RAG ì‹œìŠ¤í…œì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ì˜ë¯¸í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
                        "- **í‰ê°€ ìš”ì†Œ**\n",
                        "- **ê²€ìƒ‰ ë‹¨ê³„ í‰ê°€**\n",
                        "- ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë‚˜ ì •ë³´ì˜ ê´€ë ¨ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€.\n",
                        "- **ìƒì„± ë‹¨ê³„ í‰ê°€**\n",
                        "- ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ, ì •í™•ì„±ë“±ì„ í‰ê°€.\n",
                        "- **í‰ê°€ ë°©ë²•**\n",
                        "- ì˜¨/ì˜¤í”„ë¼ì¸ í‰ê°€\n",
                        "1. **ì˜¤í”„ë¼ì¸ í‰ê°€**\n",
                        "- ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.\n",
                        "2. **ì˜¨ë¼ì¸ í‰ê°€**\n",
                        "- ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ê³¼ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
                        "- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€\n",
                        "1. ì •ëŸ‰ì  í‰ê°€\n",
                        "- ìë™í™”ëœ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.\n",
                        "2. ì •ì„±ì  í‰ê°€\n",
                        "- ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì£¼ê´€ì ì¸ ì§€í‘œë¥¼ í‰ê°€í•œë‹¤.' metadata={'source': '', 'source_file': '11_RAG_evaluation.ipynb', 'lecture_title': '11_RAG_evaluation', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '```python\\n!uv pip install ragas rapidfuzz\\n```', 'chunk_index': 0, 'original_score': 0.6857279199999999}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6857279199999999\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 05_í‰ê°€ì§€í‘œ]\n",
                        "\n",
                        "ëª¨ë¸ í‰ê°€\n",
                        "- ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ëŠ” ëª¨ë¸ë§ ì¤‘ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê²€ì¦ ë‹¨ê³„ì™€ ìµœì¢… ì„±ëŠ¥ í‰ê°€ì—ì„œ ì§„í–‰í•œë‹¤.\n",
                        "- ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê°€ì™€ ëª¨ë¸ì˜ ì–´ë–¤ ì¸¡ë©´ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê°€ì— ë”°ë¼ ë‹¤ì–‘í•œ í‰ê°€ ë°©ë²•ì´ ìˆë‹¤.' metadata={'source': '', 'source_file': '05_í‰ê°€ì§€í‘œ.ipynb', 'lecture_title': '05_í‰ê°€ì§€í‘œ', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.4517450439087695}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4517450439087695\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 12_ì„ í˜•ëª¨ë¸_ì„ í˜•íšŒê·€]\n",
                        "\n",
                        "LinearRegressionìœ¼ë¡œ í‰ê°€' metadata={'source': '', 'source_file': '12_ì„ í˜•ëª¨ë¸_ì„ í˜•íšŒê·€.ipynb', 'lecture_title': '12_ì„ í˜•ëª¨ë¸_ì„ í˜•íšŒê·€', 'cell_type': 'markdown', 'cell_index': 90, 'code_snippet': \"```python\\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\\nfrom metrics import print_regression_metrcis\\n\\nlr = LinearRegression()\\nlr.fit(X_train_poly, y_train)\\n\\nprint_regression_metrcis(y_train, lr.predict(X_train_poly))\\nprint('-------------------------------------')\\nprint_regression_metrcis(y_test, lr.predict(X_test_poly))\\n```\", 'chunk_index': 15, 'original_score': 0.4320330881332508}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4320330881332508\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "íŠ¹ì§•\n",
                        "- K-meansì€ êµ°ì§‘ì„ ì› ëª¨ì–‘ìœ¼ë¡œ ê°„ì£¼ í•œë‹¤.\n",
                        "- ëª¨ë“  íŠ¹ì„±ì€ ë™ì¼í•œ Scaleì„ ê°€ì ¸ì•¼ í•œë‹¤.\n",
                        "- **Feature Scaling í•„ìš”**\n",
                        "- ì´ìƒì¹˜ì— ì·¨ì•½í•˜ë‹¤.' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.46396154762156844}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.46396154762156844\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.7152693160000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7152693160000001\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.5359450798265496}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5359450798265496\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "íŠ¹ì§•\n",
                        "- K-meansì€ êµ°ì§‘ì„ ì› ëª¨ì–‘ìœ¼ë¡œ ê°„ì£¼ í•œë‹¤.\n",
                        "- ëª¨ë“  íŠ¹ì„±ì€ ë™ì¼í•œ Scaleì„ ê°€ì ¸ì•¼ í•œë‹¤.\n",
                        "- **Feature Scaling í•„ìš”**\n",
                        "- ì´ìƒì¹˜ì— ì·¨ì•½í•˜ë‹¤.' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 3, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.4639554636215684}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4639554636215684\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.715273498}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.715273498\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.5359531498265495}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5359531498265495\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.8104576000000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.8104576000000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ]\n",
                        "\n",
                        "ì£¼ìš” í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
                        "- **ë¶„ë¥˜: sklearn.neighbors.KNeighborsClassifier**, **íšŒê·€: sklearn.neighbors.KNeighborsRegressor**\n",
                        "- **ì´ì›ƒ ìˆ˜**\n",
                        "- n_neighbors = K\n",
                        "- **Kê°€ ì‘ì„ ìˆ˜ë¡ ì´ìƒì¹˜ì— ë°˜ì‘í•  ê°€ëŠ¥ì´ ë†’ì•„ì ¸ overfitting ìˆ˜ ìˆë‹¤. Kê°€ ë„ˆë¬´ í¬ë©´ ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ í•˜ê²Œ ë˜ë¯€ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚˜ë¹ ì ¸ underfittingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.**\n",
                        "- **Overfitting**: Kê°’ì„ ë” í¬ê²Œ ì¡ëŠ”ë‹¤.\n",
                        "- **Underfitting**: Kê°’ì„ ë” ì‘ê²Œ ì¡ëŠ”ë‹¤.\n",
                        "- n_neighborsëŠ” Featureìˆ˜ì˜ ì œê³±ê·¼ ì •ë„ë¥¼ ì§€ì •í•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.\n",
                        "- **ê±°ë¦¬ ì¬ëŠ” ë°©ë²•**\n",
                        "- p=2: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(Euclidean distance - ê¸°ë³¸ê°’ - L2 Norm)\n",
                        "- p=1: ë§¨í•˜íƒ„ ê±°ë¦¬(Manhattan distance - L1 Norm)' metadata={'source': '', 'source_file': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ.ipynb', 'lecture_title': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.4458596910610739}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4458596910610739\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.4404409192396112}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4404409192396112\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.8104576000000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.8104576000000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ]\n",
                        "\n",
                        "ì£¼ìš” í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
                        "- **ë¶„ë¥˜: sklearn.neighbors.KNeighborsClassifier**, **íšŒê·€: sklearn.neighbors.KNeighborsRegressor**\n",
                        "- **ì´ì›ƒ ìˆ˜**\n",
                        "- n_neighbors = K\n",
                        "- **Kê°€ ì‘ì„ ìˆ˜ë¡ ì´ìƒì¹˜ì— ë°˜ì‘í•  ê°€ëŠ¥ì´ ë†’ì•„ì ¸ overfitting ìˆ˜ ìˆë‹¤. Kê°€ ë„ˆë¬´ í¬ë©´ ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ í•˜ê²Œ ë˜ë¯€ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚˜ë¹ ì ¸ underfittingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.**\n",
                        "- **Overfitting**: Kê°’ì„ ë” í¬ê²Œ ì¡ëŠ”ë‹¤.\n",
                        "- **Underfitting**: Kê°’ì„ ë” ì‘ê²Œ ì¡ëŠ”ë‹¤.\n",
                        "- n_neighborsëŠ” Featureìˆ˜ì˜ ì œê³±ê·¼ ì •ë„ë¥¼ ì§€ì •í•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.\n",
                        "- **ê±°ë¦¬ ì¬ëŠ” ë°©ë²•**\n",
                        "- p=2: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(Euclidean distance - ê¸°ë³¸ê°’ - L2 Norm)\n",
                        "- p=1: ë§¨í•˜íƒ„ ê±°ë¦¬(Manhattan distance - L1 Norm)' metadata={'source': '', 'source_file': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ.ipynb', 'lecture_title': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.4458596910610739}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4458596910610739\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.4404409192396112}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4404409192396112\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.748529942}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.748529942\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "k-means (K-í‰ê· )\n",
                        "- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” êµ°ì§‘ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜.\n",
                        "- ë°ì´í„°ì…‹ì„ Kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. KëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©ìê°€ ì§€ì •í•œë‹¤.\n",
                        "- êµ°ì§‘ì˜ ì¤‘ì‹¬ì´ ë  ê²ƒ ê°™ì€ ì„ì˜ì˜ ì§€ì (Centroid)ì„ ì„ íƒí•´ í•´ë‹¹ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸ë“œë“¤ì„ ì„ íƒí•˜ëŠ” ê¸°ë²•.' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5419962583875675}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5419962583875675\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.5776337698265496}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5776337698265496\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans\n",
                        "- sklearn.cluster.KMeans\n",
                        "- í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
                        "- n_clusters: ëª‡ê°œì˜ categoryë¡œ ë¶„ë¥˜í•  ì§€ ì§€ì •.\n",
                        "- ì†ì„±\n",
                        "- labels_ : ë°ì´í„°í¬ì¸íŠ¸ë³„ label' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 4, 'code_snippet': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```\", 'chunk_index': 3, 'original_score': 0.748529942}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.748529942\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "k-means (K-í‰ê· )\n",
                        "- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” êµ°ì§‘ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜.\n",
                        "- ë°ì´í„°ì…‹ì„ Kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. KëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©ìê°€ ì§€ì •í•œë‹¤.\n",
                        "- êµ°ì§‘ì˜ ì¤‘ì‹¬ì´ ë  ê²ƒ ê°™ì€ ì„ì˜ì˜ ì§€ì (Centroid)ì„ ì„ íƒí•´ í•´ë‹¹ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸ë“œë“¤ì„ ì„ íƒí•˜ëŠ” ê¸°ë²•.' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 1, 'original_score': 0.5419962583875675}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5419962583875675\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 14 êµ°ì§‘_Clustering]\n",
                        "\n",
                        "KMeans ìƒì„± ë° í•™ìŠµ' metadata={'source': '', 'source_file': '14 êµ°ì§‘_Clustering.ipynb', 'lecture_title': '14 êµ°ì§‘_Clustering', 'cell_type': 'markdown', 'cell_index': 8, 'code_snippet': \"```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # ëª‡ê°œ êµ°ì§‘(cluster)ì„ ë‚˜ëˆŒì§€ \\nkmeans.fit(X_scaled) #  n_clusters ê°œìˆ˜ì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ”.\\n```\\n\\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```\\n\\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  ì •ë‹µ\\ndf['cluster y'] = kmeans.labels_\\n```\\n\\n```python\\npd.options.display.max_rows = 150\\ndf\\n```\\n\\n```python\\ndf['cluster y'].value_counts()\\n```\", 'chunk_index': 4, 'original_score': 0.5776337698265496}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5776337698265496\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] array\n",
                        "[TITLE] array\n",
                        "[H1] array --- Efficient arrays of numeric values\n",
                        "\n",
                        "Notes:\n",
                        "\n",
                        "(1)\n",
                        "   It can be 16 bits or 32 bits depending on the platform.\n",
                        "\n",
                        "      ``array('u')`` now uses :cwchar_t as C type instead of deprecated\n",
                        "      ``Py_UNICODE``. This change doesn't affect its behavior because\n",
                        "      ``Py_UNICODE`` is alias of :cwchar_t since Python 3.3.\n",
                        "\n",
                        "(2)\n",
                        "\n",
                        "The actual representation of values is determined by the machine architecture\n",
                        "(strictly speaking, by the C implementation). The actual size can be accessed\n",
                        "through the array.itemsize attribute.\n",
                        "\n",
                        "The module defines the following item:\n",
                        "\n",
                        "data typecodes\n",
                        "\n",
                        "   A string with all available type codes.\n",
                        "\n",
                        "The module defines the following type:\n",
                        "\n",
                        "class array(typecode[, initializer])\n",
                        "\n",
                        "   A new array whose items are restricted by *typecode*, and initialized\n",
                        "   from the optional *initializer* value, which must be a bytes\n",
                        "   or bytearray object, a Unicode string, or iterable over elements\n",
                        "   of the appropriate type.' metadata={'source': '', 'title': 'array', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/array.rst', 'section': 'array --- Efficient arrays of numeric values', 'subsection': 'array --- Efficient arrays of numeric values', 'has_code': True, 'chunk_index': 5, 'snippet': \"[KEYWORDS] array [TITLE] array [H1] array --- Efficient arrays of numeric values  Notes:  (1)    It can be 16 bits or 32 bits depending on the platform.        ``array('u')`` now uses :cwchar_t as C t\", 'original_score': 0.49045534181818184}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.49045534181818184\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] array floor division\n",
                        "[TITLE] array\n",
                        "[H1] array --- Efficient arrays of numeric values\n",
                        "\n",
                        "The string representation of array objects has the form\n",
                        "``array(typecode, initializer)``.\n",
                        "The *initializer* is omitted if the array is empty, otherwise it is\n",
                        "a Unicode string if the *typecode* is ``'u'`` or ``'w'``, otherwise it is\n",
                        "a list of numbers.\n",
                        "The string representation is guaranteed to be able to be converted back to an\n",
                        "array with the same type and value using eval, so long as the\n",
                        "array.array class has been imported using ``from array import array``.\n",
                        "Variables ``inf`` and ``nan`` must also be defined if it contains\n",
                        "corresponding floating-point values.\n",
                        "Examples::\n",
                        "\n",
                        "   array('l')\n",
                        "   array('w', 'hello \\u2641')\n",
                        "   array('l', [1, 2, 3, 4, 5])\n",
                        "   array('d', [1.0, 2.0, 3.14, -inf, nan])\n",
                        "\n",
                        "   Module struct\n",
                        "      Packing and unpacking of heterogeneous binary data.\n",
                        "\n",
                        "   NumPy <https://numpy.org/>\n",
                        "      The NumPy package defines another array type.' metadata={'source': '', 'title': 'array', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/array.rst', 'section': 'array --- Efficient arrays of numeric values', 'subsection': 'array --- Efficient arrays of numeric values', 'has_code': True, 'chunk_index': 13, 'snippet': '[KEYWORDS] array floor division [TITLE] array [H1] array --- Efficient arrays of numeric values  The string representation of array objects has the form ``array(typecode, initializer)``. The *initiali', 'original_score': 0.499058497684843}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.499058497684843\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] stdtypes\n",
                        "[H1] Binary Sequence Types --- bytes, bytearray, memoryview\n",
                        "[H2] Memory Views\n",
                        "\n",
                        "A tuple of integers the length of ndim giving the shape of the\n",
                        "      memory as an N-dimensional array.\n",
                        "\n",
                        "         An empty tuple instead of ``None`` when ndim = 0.\n",
                        "\n",
                        "attribute strides\n",
                        "\n",
                        "      A tuple of integers the length of ndim giving the size in bytes to\n",
                        "      access each element for each dimension of the array.\n",
                        "\n",
                        "         An empty tuple instead of ``None`` when ndim = 0.\n",
                        "\n",
                        "attribute suboffsets\n",
                        "\n",
                        "      Used internally for PIL-style arrays. The value is informational only.\n",
                        "\n",
                        "attribute c_contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is C-contiguous.\n",
                        "\n",
                        "attribute f_contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is Fortran contiguous.\n",
                        "\n",
                        "attribute contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is contiguous.' metadata={'source': '', 'title': 'stdtypes', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/stdtypes.rst', 'section': 'Binary Sequence Types --- bytes, bytearray, memoryview', 'subsection': 'Memory Views', 'has_code': True, 'chunk_index': 248, 'snippet': '[TITLE] stdtypes [H1] Binary Sequence Types --- bytes, bytearray, memoryview [H2] Memory Views  A tuple of integers the length of ndim giving the shape of the       memory as an N-dimensional array.  ', 'original_score': 0.48024370152148443}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.48024370152148443\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] array\n",
                        "[TITLE] array\n",
                        "[H1] array --- Efficient arrays of numeric values\n",
                        "\n",
                        "Notes:\n",
                        "\n",
                        "(1)\n",
                        "   It can be 16 bits or 32 bits depending on the platform.\n",
                        "\n",
                        "      ``array('u')`` now uses :cwchar_t as C type instead of deprecated\n",
                        "      ``Py_UNICODE``. This change doesn't affect its behavior because\n",
                        "      ``Py_UNICODE`` is alias of :cwchar_t since Python 3.3.\n",
                        "\n",
                        "(2)\n",
                        "\n",
                        "The actual representation of values is determined by the machine architecture\n",
                        "(strictly speaking, by the C implementation). The actual size can be accessed\n",
                        "through the array.itemsize attribute.\n",
                        "\n",
                        "The module defines the following item:\n",
                        "\n",
                        "data typecodes\n",
                        "\n",
                        "   A string with all available type codes.\n",
                        "\n",
                        "The module defines the following type:\n",
                        "\n",
                        "class array(typecode[, initializer])\n",
                        "\n",
                        "   A new array whose items are restricted by *typecode*, and initialized\n",
                        "   from the optional *initializer* value, which must be a bytes\n",
                        "   or bytearray object, a Unicode string, or iterable over elements\n",
                        "   of the appropriate type.' metadata={'source': '', 'title': 'array', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/array.rst', 'section': 'array --- Efficient arrays of numeric values', 'subsection': 'array --- Efficient arrays of numeric values', 'has_code': True, 'chunk_index': 5, 'snippet': \"[KEYWORDS] array [TITLE] array [H1] array --- Efficient arrays of numeric values  Notes:  (1)    It can be 16 bits or 32 bits depending on the platform.        ``array('u')`` now uses :cwchar_t as C t\", 'original_score': 0.49045534181818184}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.49045534181818184\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] array floor division\n",
                        "[TITLE] array\n",
                        "[H1] array --- Efficient arrays of numeric values\n",
                        "\n",
                        "The string representation of array objects has the form\n",
                        "``array(typecode, initializer)``.\n",
                        "The *initializer* is omitted if the array is empty, otherwise it is\n",
                        "a Unicode string if the *typecode* is ``'u'`` or ``'w'``, otherwise it is\n",
                        "a list of numbers.\n",
                        "The string representation is guaranteed to be able to be converted back to an\n",
                        "array with the same type and value using eval, so long as the\n",
                        "array.array class has been imported using ``from array import array``.\n",
                        "Variables ``inf`` and ``nan`` must also be defined if it contains\n",
                        "corresponding floating-point values.\n",
                        "Examples::\n",
                        "\n",
                        "   array('l')\n",
                        "   array('w', 'hello \\u2641')\n",
                        "   array('l', [1, 2, 3, 4, 5])\n",
                        "   array('d', [1.0, 2.0, 3.14, -inf, nan])\n",
                        "\n",
                        "   Module struct\n",
                        "      Packing and unpacking of heterogeneous binary data.\n",
                        "\n",
                        "   NumPy <https://numpy.org/>\n",
                        "      The NumPy package defines another array type.' metadata={'source': '', 'title': 'array', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/array.rst', 'section': 'array --- Efficient arrays of numeric values', 'subsection': 'array --- Efficient arrays of numeric values', 'has_code': True, 'chunk_index': 13, 'snippet': '[KEYWORDS] array floor division [TITLE] array [H1] array --- Efficient arrays of numeric values  The string representation of array objects has the form ``array(typecode, initializer)``. The *initiali', 'original_score': 0.499058497684843}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.499058497684843\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] stdtypes\n",
                        "[H1] Binary Sequence Types --- bytes, bytearray, memoryview\n",
                        "[H2] Memory Views\n",
                        "\n",
                        "A tuple of integers the length of ndim giving the shape of the\n",
                        "      memory as an N-dimensional array.\n",
                        "\n",
                        "         An empty tuple instead of ``None`` when ndim = 0.\n",
                        "\n",
                        "attribute strides\n",
                        "\n",
                        "      A tuple of integers the length of ndim giving the size in bytes to\n",
                        "      access each element for each dimension of the array.\n",
                        "\n",
                        "         An empty tuple instead of ``None`` when ndim = 0.\n",
                        "\n",
                        "attribute suboffsets\n",
                        "\n",
                        "      Used internally for PIL-style arrays. The value is informational only.\n",
                        "\n",
                        "attribute c_contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is C-contiguous.\n",
                        "\n",
                        "attribute f_contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is Fortran contiguous.\n",
                        "\n",
                        "attribute contiguous\n",
                        "\n",
                        "      A bool indicating whether the memory is contiguous.' metadata={'source': '', 'title': 'stdtypes', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/stdtypes.rst', 'section': 'Binary Sequence Types --- bytes, bytearray, memoryview', 'subsection': 'Memory Views', 'has_code': True, 'chunk_index': 248, 'snippet': '[TITLE] stdtypes [H1] Binary Sequence Types --- bytes, bytearray, memoryview [H2] Memory Views  A tuple of integers the length of ndim giving the shape of the       memory as an N-dimensional array.  ', 'original_score': 0.48024370152148443}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.48024370152148443\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[TITLE] typing\n",
                        "[H1] Annotating tuples\n",
                        "\n",
                        "# Error: the type annotation indicates a tuple of length 1,\n",
                        "   # but ``z`` has been assigned to a tuple of length 3\n",
                        "   z: tuple[int] = (1, 2, 3)\n",
                        "\n",
                        "To denote a tuple which could be of *any* length, and in which all elements are\n",
                        "of the same type ``T``, use the literal ellipsis ``...``: ``tuple[T, ...]``.\n",
                        "To denote an empty tuple, use\n",
                        "``tuple[()]``. Using plain ``tuple`` as an annotation is equivalent to using\n",
                        "``tuple[Any, ...]``::\n",
                        "\n",
                        "   x: tuple[int, ...] = (1, 2)\n",
                        "   # These reassignments are OK: ``tuple[int, ...]`` indicates x can be of any length\n",
                        "   x = (1, 2, 3)\n",
                        "   x = ()\n",
                        "   # This reassignment is an error: all elements in ``x`` must be ints\n",
                        "   x = (\"foo\", \"bar\")\n",
                        "\n",
                        "   # ``y`` can only ever be assigned to an empty tuple\n",
                        "   y: tuple[()] = ()\n",
                        "\n",
                        "   z: tuple = (\"foo\", \"bar\")\n",
                        "   # These reassignments are OK: plain ``tuple`` is equivalent to ``tuple[Any, ...]``\n",
                        "   z = (1, 2, 3)\n",
                        "   z = ()' metadata={'source': '', 'title': 'typing', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/typing.rst', 'section': 'Annotating tuples', 'subsection': 'Annotating tuples', 'has_code': True, 'chunk_index': 18, 'snippet': '[TITLE] typing [H1] Annotating tuples  # Error: the type annotation indicates a tuple of length 1,    # but ``z`` has been assigned to a tuple of length 3    z: tuple[int] = (1, 2, 3)  To denote a tup', 'original_score': 0.4570789605714286}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4570789605714286\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[API] method scheduler.empty()\n",
                        "[KEYWORDS] empty method scheduler empty run method scheduler run wait scheduler\n",
                        "[TITLE] sched\n",
                        "[H1] sched --- Event scheduler\n",
                        "[H2] Scheduler Objects\n",
                        "\n",
                        "method scheduler.empty()\n",
                        "\n",
                        "   Return ``True`` if the event queue is empty.\n",
                        "\n",
                        "method scheduler.run(blocking=True)\n",
                        "\n",
                        "   Run all scheduled events. This method will wait  (using the *delayfunc*\n",
                        "   function passed to the constructor) for the next event, then execute it and so\n",
                        "   on until there are no more scheduled events.\n",
                        "\n",
                        "   If *blocking* is false executes the scheduled events due to expire soonest\n",
                        "   (if any) and then return the deadline of the next scheduled call in the\n",
                        "   scheduler (if any).\n",
                        "\n",
                        "   Either *action* or *delayfunc* can raise an exception.  In either case, the\n",
                        "   scheduler will maintain a consistent state and propagate the exception.  If an\n",
                        "   exception is raised by *action*, the event will not be attempted in future calls\n",
                        "   to run.' metadata={'source': '', 'title': 'sched', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/sched.rst', 'section': 'sched --- Event scheduler', 'subsection': 'Scheduler Objects', 'has_code': False, 'chunk_index': 6, 'snippet': '[API] method scheduler.empty() [KEYWORDS] empty method scheduler empty run method scheduler run wait scheduler [TITLE] sched [H1] sched --- Event scheduler [H2] Scheduler Objects  method scheduler.emp', 'original_score': 0.41463336750533475}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.41463336750533475\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[KEYWORDS] apply\n",
                        "[TITLE] expressions\n",
                        "[H1] Atoms\n",
                        "[H2] Parenthesized forms\n",
                        "\n",
                        "A parenthesized form is an optional expression list enclosed in parentheses:\n",
                        "\n",
                        "A parenthesized expression list yields whatever that expression list yields: if\n",
                        "the list contains at least one comma, it yields a tuple; otherwise, it yields\n",
                        "the single expression that makes up the expression list.\n",
                        "\n",
                        "An empty pair of parentheses yields an empty tuple object. Since tuples are\n",
                        "immutable, the same rules as for literals apply (i.e., two occurrences of the empty\n",
                        "tuple may or may not yield the same object).\n",
                        "\n",
                        "Note that tuples are not formed by the parentheses, but rather by use of the\n",
                        "comma. The exception is the empty tuple, for which parentheses *are*\n",
                        "required --- allowing unparenthesized \"nothing\" in expressions would cause\n",
                        "ambiguities and allow common typos to pass uncaught.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Atoms', 'subsection': 'Parenthesized forms', 'has_code': False, 'chunk_index': 9, 'snippet': '[KEYWORDS] apply [TITLE] expressions [H1] Atoms [H2] Parenthesized forms  A parenthesized form is an optional expression list enclosed in parentheses:  A parenthesized expression list yields whatever ', 'original_score': 0.3662879144437441}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3662879144437441\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[TITLE] typing\n",
                        "[H1] Annotating tuples\n",
                        "\n",
                        "# Error: the type annotation indicates a tuple of length 1,\n",
                        "   # but ``z`` has been assigned to a tuple of length 3\n",
                        "   z: tuple[int] = (1, 2, 3)\n",
                        "\n",
                        "To denote a tuple which could be of *any* length, and in which all elements are\n",
                        "of the same type ``T``, use the literal ellipsis ``...``: ``tuple[T, ...]``.\n",
                        "To denote an empty tuple, use\n",
                        "``tuple[()]``. Using plain ``tuple`` as an annotation is equivalent to using\n",
                        "``tuple[Any, ...]``::\n",
                        "\n",
                        "   x: tuple[int, ...] = (1, 2)\n",
                        "   # These reassignments are OK: ``tuple[int, ...]`` indicates x can be of any length\n",
                        "   x = (1, 2, 3)\n",
                        "   x = ()\n",
                        "   # This reassignment is an error: all elements in ``x`` must be ints\n",
                        "   x = (\"foo\", \"bar\")\n",
                        "\n",
                        "   # ``y`` can only ever be assigned to an empty tuple\n",
                        "   y: tuple[()] = ()\n",
                        "\n",
                        "   z: tuple = (\"foo\", \"bar\")\n",
                        "   # These reassignments are OK: plain ``tuple`` is equivalent to ``tuple[Any, ...]``\n",
                        "   z = (1, 2, 3)\n",
                        "   z = ()' metadata={'source': '', 'title': 'typing', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/typing.rst', 'section': 'Annotating tuples', 'subsection': 'Annotating tuples', 'has_code': True, 'chunk_index': 18, 'snippet': '[TITLE] typing [H1] Annotating tuples  # Error: the type annotation indicates a tuple of length 1,    # but ``z`` has been assigned to a tuple of length 3    z: tuple[int] = (1, 2, 3)  To denote a tup', 'original_score': 0.4571236545714286}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4571236545714286\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[API] method scheduler.empty()\n",
                        "[KEYWORDS] empty method scheduler empty run method scheduler run wait scheduler\n",
                        "[TITLE] sched\n",
                        "[H1] sched --- Event scheduler\n",
                        "[H2] Scheduler Objects\n",
                        "\n",
                        "method scheduler.empty()\n",
                        "\n",
                        "   Return ``True`` if the event queue is empty.\n",
                        "\n",
                        "method scheduler.run(blocking=True)\n",
                        "\n",
                        "   Run all scheduled events. This method will wait  (using the *delayfunc*\n",
                        "   function passed to the constructor) for the next event, then execute it and so\n",
                        "   on until there are no more scheduled events.\n",
                        "\n",
                        "   If *blocking* is false executes the scheduled events due to expire soonest\n",
                        "   (if any) and then return the deadline of the next scheduled call in the\n",
                        "   scheduler (if any).\n",
                        "\n",
                        "   Either *action* or *delayfunc* can raise an exception.  In either case, the\n",
                        "   scheduler will maintain a consistent state and propagate the exception.  If an\n",
                        "   exception is raised by *action*, the event will not be attempted in future calls\n",
                        "   to run.' metadata={'source': '', 'title': 'sched', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/sched.rst', 'section': 'sched --- Event scheduler', 'subsection': 'Scheduler Objects', 'has_code': False, 'chunk_index': 6, 'snippet': '[API] method scheduler.empty() [KEYWORDS] empty method scheduler empty run method scheduler run wait scheduler [TITLE] sched [H1] sched --- Event scheduler [H2] Scheduler Objects  method scheduler.emp', 'original_score': 0.4146236415053347}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.4146236415053347\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[KEYWORDS] apply\n",
                        "[TITLE] expressions\n",
                        "[H1] Atoms\n",
                        "[H2] Parenthesized forms\n",
                        "\n",
                        "A parenthesized form is an optional expression list enclosed in parentheses:\n",
                        "\n",
                        "A parenthesized expression list yields whatever that expression list yields: if\n",
                        "the list contains at least one comma, it yields a tuple; otherwise, it yields\n",
                        "the single expression that makes up the expression list.\n",
                        "\n",
                        "An empty pair of parentheses yields an empty tuple object. Since tuples are\n",
                        "immutable, the same rules as for literals apply (i.e., two occurrences of the empty\n",
                        "tuple may or may not yield the same object).\n",
                        "\n",
                        "Note that tuples are not formed by the parentheses, but rather by use of the\n",
                        "comma. The exception is the empty tuple, for which parentheses *are*\n",
                        "required --- allowing unparenthesized \"nothing\" in expressions would cause\n",
                        "ambiguities and allow common typos to pass uncaught.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Atoms', 'subsection': 'Parenthesized forms', 'has_code': False, 'chunk_index': 9, 'snippet': '[KEYWORDS] apply [TITLE] expressions [H1] Atoms [H2] Parenthesized forms  A parenthesized form is an optional expression list enclosed in parentheses:  A parenthesized expression list yields whatever ', 'original_score': 0.3663045344437441}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.3663045344437441\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 6ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\n",
                        "\n",
                        "ì›í•˜ëŠ” í˜•íƒœ(shape) í…ì„œ ìƒì„±\n",
                        "- **torch.tensor(ìë£Œêµ¬ì¡° \\[, dtype\\])**\n",
                        "- ì§€ì •í•œ dtype(Data type)ì— ë§ëŠ” Tensorê°ì²´ë¥¼ ìƒì„±í•´ì„œ ë°˜í™˜í•œë‹¤.  \n",
                        "íŠ¹ì • íƒ€ì…ì˜ Tensorë¥¼ ì§ì ‘ ìƒì„±\n",
                        "- torch.tensor()ë¡œ ìƒì„±í•˜ë©´ì„œ dtypeì„ ì§€ì •í•˜ë©´ ì•„ë˜ íƒ€ì…ì˜ Tensorê°ì²´ê°€ ìƒì„±ëœë‹¤.\n",
                        "- ì›í•˜ëŠ” Typeì˜ Tensorí´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì§ì ‘ ìƒì„±í•´ë„ ëœë‹¤.\n",
                        "- **torch.FloatTensor(ìë£Œêµ¬ì¡°)**\n",
                        "- float32 íƒ€ì… í…ì„œ ìƒì„±\n",
                        "- **torch.LongTensor(ìë£Œêµ¬ì¡°)**\n",
                        "- int64 íƒ€ì… í…ì„œìƒì„±\n",
                        "- ê·¸ì™¸\n",
                        "- BoolTensor(bool), CharTensor(int8), ShortTensor(int16), IntTensor(int32), DoubleTensor(float64)  \n",
                        "tensor ìƒíƒœ ì¡°íšŒ\n",
                        "- **tensor.shape, tensor.size(\\[ì¶•ë²ˆí˜¸\\])**\n",
                        "- tensorì˜ shapeì¡°íšŒ\n",
                        "- **tensor.dtype, tensor.type()**\n",
                        "- tensor ì›ì†Œë“¤ì˜ ë°ì´í„°íƒ€ì… ì¡°íšŒ\n",
                        "- dtypeì€ **data type**ì„ type()ì€ tensor **ê°ì²´ì˜ í´ë˜ìŠ¤ íƒ€ì…**ì„ ë°˜í™˜í•œë‹¤.\n",
                        "- **tensor.ndim, tensor.dim()** : tensor ì°¨ì›\n",
                        "- **tensor.numel()**: ì „ì²´ ì›ì†Œ ê°œìˆ˜' metadata={'source': '', 'source_file': '02. tensor ë‹¤ë£¨ê¸°.ipynb', 'lecture_title': '02. tensor ë‹¤ë£¨ê¸°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '```python\\nimport torch\\nimport numpy as np\\n```\\n\\n```python\\nnp.array([1, 2, 3], dtype=np.int8) #\"int8\" )\\n```\\n\\n```python\\nprint(\\'torch Data types\\')\\nprint(\"float\", torch.float16, torch.float32, torch.float64, torch.float, torch.double)  #torch.float: float32\\nprint(\"int:\", torch.int8, torch.int16, torch.int32, torch.int64, torch.short, torch.int, torch.long)# ê°€ëŠ¥\\nprint(\"uint:\", torch.uint8, torch.uint16, torch.uint32, torch.uint64)\\nprint(\"bool:\", torch.bool)\\n```\\n\\n```python\\na = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\\n# a = torch.tensor([[1,2],[3,4]], dtype=torch.float32, \\n#                   device=\\'cuda\\') # tensorë¥¼ GPU ë©”ëª¨ë¦¬ë¡œ ì˜¬ë¦°ë‹¤.\\n\\nprint(\"shape:\", a.shape, a.size())\\nprint(\"0ì¶• í¬ê¸°:\", a.shape[0], a.size(0))\\nprint(\"type:\", a.type(), a.dtype)  # type(): Tensor ê°ì²´ íƒ€ì…. dtype: data type => ë‘˜ì€ ì¢€ ë‹¤ë¥´ë‹¤.\\nprint(\\'ì°¨ì›í¬ê¸°:\\', a.dim(), a.ndim)\\nprint(\\'ì›ì†Œê°œìˆ˜:\\', a.numel())\\nprint(\"device:\", a.device)  # tensorë¥¼ ë‹¤ë£¨ëŠ” processor(ë©”ëª¨ë¦¬ìœ„ì¹˜) - cpu, cuda(gpu)\\n```\\n\\n```python\\naa = torch.tensor(range(10), dtype=torch.int8)\\nprint(aa)\\naa.type()\\n```\\n\\n```python\\n#Float, Double(32, 64bit ì‹¤ìˆ˜)/Int, Long(32, 64 bit ì •ìˆ˜) type Tensor\\nb = torch.FloatTensor([1,3,7])  #float32  # torch.tensor([1, 3, 7], dtype=torch.float32)\\nprint(b.dtype)\\nc = torch.IntTensor([10,20,30])    # int32\\nprint(c.dtype)\\nd = torch.DoubleTensor([1, 2, 3])  # float64\\nprint(d.dtype)\\ne = torch.LongTensor([10, 20, 30]) # int64\\nprint(e.dtype)\\n```', 'chunk_index': 1, 'original_score': 0.7311708100000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7311708100000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\n",
                        "\n",
                        "Tensor ìƒì„±\n",
                        "- íŒŒì´í† ì¹˜ì—ì„œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡°\n",
                        "- ndarrayì™€ ì„±ê²©, ì‚¬ìš©ë²•ì´ ìœ ì‚¬í•˜ë‹¤.  \n",
                        "> **pytorchì˜ tensorëŠ” ìˆ«ì ë°ì´í„° íƒ€ì…ë§Œ ì§€ì›í•œë‹¤.**' metadata={'source': '', 'source_file': '02. tensor ë‹¤ë£¨ê¸°.ipynb', 'lecture_title': '02. tensor ë‹¤ë£¨ê¸°', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6710769376923077}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6710769376923077\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_Numpyê°œìš”_ë°°ì—´ìƒì„±]\n",
                        "\n",
                        "ë„˜íŒŒì´ì—ì„œ ë°ì´í„° êµ¬ì¡°  \n",
                        "- ìŠ¤ì¹¼ë¼ (Scalar)\n",
                        "- ê°’ í•˜ë‚˜.\n",
                        "- ë²¡í„° (Vector)\n",
                        "- ì—¬ëŸ¬ê°œì˜ ê°’ë“¤ì„ ìˆœì„œëŒ€ë¡œ ëª¨ì•„ë†“ì€ ë°ì´í„° ëª¨ìŒ(ë°ì´í„° ë ˆì½”ë“œ)\n",
                        "- 1D Tensor, 1D Array (1ì°¨ì› ë°°ì—´)\n",
                        "- í–‰ë ¬ (Matrix)\n",
                        "- ë²¡í„°ë“¤ì„ ëª¨ì•„ë†“ì€ ë°ì´í„° ì§‘í•©. 2ê°œì˜ ë°©í–¥ìœ¼ë¡œ ê°’ë“¤ì´ ê´€ë¦¬ëœë‹¤.\n",
                        "- 2D Tensor, 2D Array (2ì°¨ì› ë°°ì—´)\n",
                        "- í…ì„œ (Tensor)\n",
                        "- ê°™ì€ í¬ê¸°ì˜ í–‰ë ¬ë“¤(í…ì„œë“¤)ì„ ëª¨ì•„ë†“ì€ ë°ì´í„° ì§‘í•©. Nê°œì˜ ë°©í–¥ìœ¼ë¡œ ê°’ë“¤ì´ ê´€ë¦¬ëœë‹¤.\n",
                        "- ND Tensor, ND Array (ë‹¤ì°¨ì› ë°°ì—´)' metadata={'source': '', 'source_file': '01_Numpyê°œìš”_ë°°ì—´ìƒì„±.ipynb', 'lecture_title': '01_Numpyê°œìš”_ë°°ì—´ìƒì„±', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6664087693474932}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6664087693474932\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 6ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\n",
                        "\n",
                        "ì›í•˜ëŠ” í˜•íƒœ(shape) í…ì„œ ìƒì„±\n",
                        "- **torch.tensor(ìë£Œêµ¬ì¡° \\[, dtype\\])**\n",
                        "- ì§€ì •í•œ dtype(Data type)ì— ë§ëŠ” Tensorê°ì²´ë¥¼ ìƒì„±í•´ì„œ ë°˜í™˜í•œë‹¤.  \n",
                        "íŠ¹ì • íƒ€ì…ì˜ Tensorë¥¼ ì§ì ‘ ìƒì„±\n",
                        "- torch.tensor()ë¡œ ìƒì„±í•˜ë©´ì„œ dtypeì„ ì§€ì •í•˜ë©´ ì•„ë˜ íƒ€ì…ì˜ Tensorê°ì²´ê°€ ìƒì„±ëœë‹¤.\n",
                        "- ì›í•˜ëŠ” Typeì˜ Tensorí´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì§ì ‘ ìƒì„±í•´ë„ ëœë‹¤.\n",
                        "- **torch.FloatTensor(ìë£Œêµ¬ì¡°)**\n",
                        "- float32 íƒ€ì… í…ì„œ ìƒì„±\n",
                        "- **torch.LongTensor(ìë£Œêµ¬ì¡°)**\n",
                        "- int64 íƒ€ì… í…ì„œìƒì„±\n",
                        "- ê·¸ì™¸\n",
                        "- BoolTensor(bool), CharTensor(int8), ShortTensor(int16), IntTensor(int32), DoubleTensor(float64)  \n",
                        "tensor ìƒíƒœ ì¡°íšŒ\n",
                        "- **tensor.shape, tensor.size(\\[ì¶•ë²ˆí˜¸\\])**\n",
                        "- tensorì˜ shapeì¡°íšŒ\n",
                        "- **tensor.dtype, tensor.type()**\n",
                        "- tensor ì›ì†Œë“¤ì˜ ë°ì´í„°íƒ€ì… ì¡°íšŒ\n",
                        "- dtypeì€ **data type**ì„ type()ì€ tensor **ê°ì²´ì˜ í´ë˜ìŠ¤ íƒ€ì…**ì„ ë°˜í™˜í•œë‹¤.\n",
                        "- **tensor.ndim, tensor.dim()** : tensor ì°¨ì›\n",
                        "- **tensor.numel()**: ì „ì²´ ì›ì†Œ ê°œìˆ˜' metadata={'source': '', 'source_file': '02. tensor ë‹¤ë£¨ê¸°.ipynb', 'lecture_title': '02. tensor ë‹¤ë£¨ê¸°', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '```python\\nimport torch\\nimport numpy as np\\n```\\n\\n```python\\nnp.array([1, 2, 3], dtype=np.int8) #\"int8\" )\\n```\\n\\n```python\\nprint(\\'torch Data types\\')\\nprint(\"float\", torch.float16, torch.float32, torch.float64, torch.float, torch.double)  #torch.float: float32\\nprint(\"int:\", torch.int8, torch.int16, torch.int32, torch.int64, torch.short, torch.int, torch.long)# ê°€ëŠ¥\\nprint(\"uint:\", torch.uint8, torch.uint16, torch.uint32, torch.uint64)\\nprint(\"bool:\", torch.bool)\\n```\\n\\n```python\\na = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\\n# a = torch.tensor([[1,2],[3,4]], dtype=torch.float32, \\n#                   device=\\'cuda\\') # tensorë¥¼ GPU ë©”ëª¨ë¦¬ë¡œ ì˜¬ë¦°ë‹¤.\\n\\nprint(\"shape:\", a.shape, a.size())\\nprint(\"0ì¶• í¬ê¸°:\", a.shape[0], a.size(0))\\nprint(\"type:\", a.type(), a.dtype)  # type(): Tensor ê°ì²´ íƒ€ì…. dtype: data type => ë‘˜ì€ ì¢€ ë‹¤ë¥´ë‹¤.\\nprint(\\'ì°¨ì›í¬ê¸°:\\', a.dim(), a.ndim)\\nprint(\\'ì›ì†Œê°œìˆ˜:\\', a.numel())\\nprint(\"device:\", a.device)  # tensorë¥¼ ë‹¤ë£¨ëŠ” processor(ë©”ëª¨ë¦¬ìœ„ì¹˜) - cpu, cuda(gpu)\\n```\\n\\n```python\\naa = torch.tensor(range(10), dtype=torch.int8)\\nprint(aa)\\naa.type()\\n```\\n\\n```python\\n#Float, Double(32, 64bit ì‹¤ìˆ˜)/Int, Long(32, 64 bit ì •ìˆ˜) type Tensor\\nb = torch.FloatTensor([1,3,7])  #float32  # torch.tensor([1, 3, 7], dtype=torch.float32)\\nprint(b.dtype)\\nc = torch.IntTensor([10,20,30])    # int32\\nprint(c.dtype)\\nd = torch.DoubleTensor([1, 2, 3])  # float64\\nprint(d.dtype)\\ne = torch.LongTensor([10, 20, 30]) # int64\\nprint(e.dtype)\\n```', 'chunk_index': 1, 'original_score': 0.7311708100000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7311708100000001\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\n",
                        "\n",
                        "Tensor ìƒì„±\n",
                        "- íŒŒì´í† ì¹˜ì—ì„œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡°\n",
                        "- ndarrayì™€ ì„±ê²©, ì‚¬ìš©ë²•ì´ ìœ ì‚¬í•˜ë‹¤.  \n",
                        "> **pytorchì˜ tensorëŠ” ìˆ«ì ë°ì´í„° íƒ€ì…ë§Œ ì§€ì›í•œë‹¤.**' metadata={'source': '', 'source_file': '02. tensor ë‹¤ë£¨ê¸°.ipynb', 'lecture_title': '02. tensor ë‹¤ë£¨ê¸°', 'cell_type': 'markdown', 'cell_index': 0, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6710769376923077}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6710769376923077\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_Numpyê°œìš”_ë°°ì—´ìƒì„±]\n",
                        "\n",
                        "ë„˜íŒŒì´ì—ì„œ ë°ì´í„° êµ¬ì¡°  \n",
                        "- ìŠ¤ì¹¼ë¼ (Scalar)\n",
                        "- ê°’ í•˜ë‚˜.\n",
                        "- ë²¡í„° (Vector)\n",
                        "- ì—¬ëŸ¬ê°œì˜ ê°’ë“¤ì„ ìˆœì„œëŒ€ë¡œ ëª¨ì•„ë†“ì€ ë°ì´í„° ëª¨ìŒ(ë°ì´í„° ë ˆì½”ë“œ)\n",
                        "- 1D Tensor, 1D Array (1ì°¨ì› ë°°ì—´)\n",
                        "- í–‰ë ¬ (Matrix)\n",
                        "- ë²¡í„°ë“¤ì„ ëª¨ì•„ë†“ì€ ë°ì´í„° ì§‘í•©. 2ê°œì˜ ë°©í–¥ìœ¼ë¡œ ê°’ë“¤ì´ ê´€ë¦¬ëœë‹¤.\n",
                        "- 2D Tensor, 2D Array (2ì°¨ì› ë°°ì—´)\n",
                        "- í…ì„œ (Tensor)\n",
                        "- ê°™ì€ í¬ê¸°ì˜ í–‰ë ¬ë“¤(í…ì„œë“¤)ì„ ëª¨ì•„ë†“ì€ ë°ì´í„° ì§‘í•©. Nê°œì˜ ë°©í–¥ìœ¼ë¡œ ê°’ë“¤ì´ ê´€ë¦¬ëœë‹¤.\n",
                        "- ND Tensor, ND Array (ë‹¤ì°¨ì› ë°°ì—´)' metadata={'source': '', 'source_file': '01_Numpyê°œìš”_ë°°ì—´ìƒì„±.ipynb', 'lecture_title': '01_Numpyê°œìš”_ë°°ì—´ìƒì„±', 'cell_type': 'markdown', 'cell_index': 1, 'code_snippet': '', 'chunk_index': 0, 'original_score': 0.6664087693474932}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6664087693474932\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \n",
                        "ì§€ë„í•™ìŠµ(Supervised Learning)\n",
                        "- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\n",
                        "- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 16, 'code_snippet': '', 'chunk_index': 12, 'original_score': 0.633398114008159}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.633398114008159\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "- ### ë¶„ë¥˜(Classification):\n",
                        "- **ë‘ê°œ ì´ìƒì˜ í´ë˜ìŠ¤(ë²”ì£¼)ì—ì„œ ì„ íƒì„ ë¬»ëŠ” ì§€ë„ í•™ìŠµë°©ë²•**\n",
                        "- **ì´ì§„ ë¶„ë¥˜** : ë§ëŠ”ì§€ í‹€ë¦°ì§€ë¥¼ ë¶„ë¥˜.\n",
                        "- **ë‹¤ì¤‘ ë¶„ë¥˜** : ì—¬ëŸ¬ê°œì˜ í´ë˜ìŠ¤ì¤‘ í•˜ë‚˜ë¥¼ ë¶„ë¥˜\n",
                        "- ### íšŒê·€(Regression):\n",
                        "- **ìˆ«ì(ì—°ì†ëœê°’)ë¥¼ ì˜ˆì¸¡ í•˜ëŠ” ì§€ë„í•™ìŠµ**' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 17, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.653060752}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.653060752\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ]\n",
                        "\n",
                        "- **ë¶„ë¥˜**\n",
                        "- ì¶”ë¡ í•  featureë“¤ê³¼ ê°€ê¹Œìš´ featureë“¤ë¡œ êµ¬ì„±ëœ data point K ê°œì˜ yì¤‘ ë‹¤ìˆ˜ì˜ classë¡œ ì¶”ë¡ í•œë‹¤.\n",
                        "- **íšŒê·€**\n",
                        "- ì¶”ë¡ í•  featureë“¤ê³¼ ê°€ê¹Œìš´ featureë“¤ë¡œ êµ¬ì„±ëœ data point K ê°œì˜ yê°’ì˜ í‰ê· ê°’ìœ¼ë¡œ ì¶”ë¡ í•œë‹¤.\n",
                        "- Kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ Overfittingì´ ì¼ì–´ë‚  ìˆ˜ ìˆê³  Kê°€ ë„ˆë¬´ í¬ë©´ Underfittingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ.ipynb', 'lecture_title': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ', 'cell_type': 'markdown', 'cell_index': 6, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.6310111261149032}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6310111261149032\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \n",
                        "ì§€ë„í•™ìŠµ(Supervised Learning)\n",
                        "- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\n",
                        "- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 16, 'code_snippet': '', 'chunk_index': 12, 'original_score': 0.633398114008159}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.633398114008159\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "- ### ë¶„ë¥˜(Classification):\n",
                        "- **ë‘ê°œ ì´ìƒì˜ í´ë˜ìŠ¤(ë²”ì£¼)ì—ì„œ ì„ íƒì„ ë¬»ëŠ” ì§€ë„ í•™ìŠµë°©ë²•**\n",
                        "- **ì´ì§„ ë¶„ë¥˜** : ë§ëŠ”ì§€ í‹€ë¦°ì§€ë¥¼ ë¶„ë¥˜.\n",
                        "- **ë‹¤ì¤‘ ë¶„ë¥˜** : ì—¬ëŸ¬ê°œì˜ í´ë˜ìŠ¤ì¤‘ í•˜ë‚˜ë¥¼ ë¶„ë¥˜\n",
                        "- ### íšŒê·€(Regression):\n",
                        "- **ìˆ«ì(ì—°ì†ëœê°’)ë¥¼ ì˜ˆì¸¡ í•˜ëŠ” ì§€ë„í•™ìŠµ**' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 17, 'code_snippet': '', 'chunk_index': 13, 'original_score': 0.653060752}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.653060752\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ]\n",
                        "\n",
                        "- **ë¶„ë¥˜**\n",
                        "- ì¶”ë¡ í•  featureë“¤ê³¼ ê°€ê¹Œìš´ featureë“¤ë¡œ êµ¬ì„±ëœ data point K ê°œì˜ yì¤‘ ë‹¤ìˆ˜ì˜ classë¡œ ì¶”ë¡ í•œë‹¤.\n",
                        "- **íšŒê·€**\n",
                        "- ì¶”ë¡ í•  featureë“¤ê³¼ ê°€ê¹Œìš´ featureë“¤ë¡œ êµ¬ì„±ëœ data point K ê°œì˜ yê°’ì˜ í‰ê· ê°’ìœ¼ë¡œ ì¶”ë¡ í•œë‹¤.\n",
                        "- Kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ Overfittingì´ ì¼ì–´ë‚  ìˆ˜ ìˆê³  Kê°€ ë„ˆë¬´ í¬ë©´ Underfittingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ.ipynb', 'lecture_title': '08_ì§€ë„í•™ìŠµ_ìµœê·¼ì ‘ì´ì›ƒ', 'cell_type': 'markdown', 'cell_index': 6, 'code_snippet': '', 'chunk_index': 2, 'original_score': 0.6310111261149032}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6310111261149032\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)\n",
                        "- ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µ ì§€ëŠ¥ ë¶„ì•¼\n",
                        "- ëª…ì‹œì ì¸ ê·œì¹™ì„ í”„ë¡œê·¸ë˜ë°í•˜ì§€ ì•Šì•„ë„, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•´ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ìˆ ì„ ê°œë°œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 4, 'original_score': 0.7231668216491753}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7231668216491753\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´  \n",
                        "- ê¸°ì¡´ í”„ë¡œê·¸ë˜ë°ë°©ì‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜(ê·œì¹™)ì„ **ì‚¬ëŒì´** ìƒê°í•œë‹¤.\n",
                        "- ë¨¸ì‹ ëŸ¬ë‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜ì„ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ì°¾ì•„ë‚¸ë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.7245247239999999}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7245247239999999\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê°„ì˜ ì°¨ì´' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '', 'chunk_index': 7, 'original_score': 0.7226564448191126}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7226564448191126\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)\n",
                        "- ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µ ì§€ëŠ¥ ë¶„ì•¼\n",
                        "- ëª…ì‹œì ì¸ ê·œì¹™ì„ í”„ë¡œê·¸ë˜ë°í•˜ì§€ ì•Šì•„ë„, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•´ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ìˆ ì„ ê°œë°œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 4, 'original_score': 0.7231668216491753}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7231668216491753\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´  \n",
                        "- ê¸°ì¡´ í”„ë¡œê·¸ë˜ë°ë°©ì‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜(ê·œì¹™)ì„ **ì‚¬ëŒì´** ìƒê°í•œë‹¤.\n",
                        "- ë¨¸ì‹ ëŸ¬ë‹\n",
                        "- ì•Œê³ ë¦¬ì¦˜ì„ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ì°¾ì•„ë‚¸ë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 5, 'code_snippet': '', 'chunk_index': 3, 'original_score': 0.7245247239999999}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7245247239999999\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê°„ì˜ ì°¨ì´' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '', 'chunk_index': 7, 'original_score': 0.7226564448191126}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7226564448191126\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\n",
                        "- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\n",
                        "- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\n",
                        "- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\n",
                        "- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\n",
                        "- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.6442484086120732}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6442484086120732\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \n",
                        "ì§€ë„í•™ìŠµ(Supervised Learning)\n",
                        "- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\n",
                        "- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 16, 'code_snippet': '', 'chunk_index': 12, 'original_score': 0.7165329220000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7165329220000001\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê°„ì˜ ì°¨ì´' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '', 'chunk_index': 7, 'original_score': 0.5473317375085325}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5473317375085325\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\n",
                        "\n",
                        "ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•  \n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ê³µí†µì ì„ ê°€ì§„ë‹¤.\n",
                        "- íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° ëª©í‘œì— ì í•©í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì˜ êµ¬ì„±í•´ì•¼ í•œë‹¤.\n",
                        "- ì›ë³¸ ë°ì´í„°(raw data)ì—ëŠ” íŒ¨í„´ ì¸ì‹ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\n",
                        "- ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ **ë°ì´í„° ì „ì²˜ë¦¬(data preprocessing)** ë˜ëŠ” **íŠ¹ì„± ì¶”ì¶œ(feature extraction)** ì´ë¼ê³  í•œë‹¤.\n",
                        "- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ(feature extraction)ì„ ì‚¬ëŒì´ ì§ì ‘ ìˆ˜í–‰í•˜ê³ , ì´í›„ì— ìƒì„±ëœ íŠ¹ì„± ë²¡í„°(feature vector)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµëœë‹¤.\n",
                        "- ë”¥ëŸ¬ë‹ì€ ëª¨ë¸ì´ í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œê³¼ ëª¨ë¸ í•™ìŠµì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.\n",
                        "- ì´ëŸ¬í•œ êµ¬ì¡° ë•ë¶„ì— ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“±ê³¼ ê°™ì€ íŠ¹ì„± ì¶”ì¶œì„ ì˜ í•˜ê¸°ê°€ ì–´ë ¤ìš´ìš´ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë°˜ë©´, íŠ¹ì„± ì¶”ì¶œì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì •í˜• ë°ì´í„°ì—ì„œëŠ” ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤.' metadata={'source': '', 'source_file': '01_ë”¥ëŸ¬ë‹ ê°œìš”.ipynb', 'lecture_title': '01_ë”¥ëŸ¬ë‹ ê°œìš”', 'cell_type': 'markdown', 'cell_index': 7, 'code_snippet': '', 'chunk_index': 5, 'original_score': 0.6442484086120732}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6442484086120732\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \n",
                        "ì§€ë„í•™ìŠµ(Supervised Learning)\n",
                        "- ëª¨ë¸ì—ê²Œ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ì™€ ì •ë‹µ(Label)ì„ ì•Œë ¤ì£¼ë©° í•™ìŠµì‹œí‚¨ë‹¤.\n",
                        "- ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ì€ ì§€ë„í•™ìŠµì´ë‹¤.\n",
                        "- ì§€ë„í•™ìŠµì€ ë¶„ë¥˜ì™€ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤.' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 16, 'code_snippet': '', 'chunk_index': 12, 'original_score': 0.7165329220000001}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.7165329220000001\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\n",
                        "\n",
                        "ê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê°„ì˜ ì°¨ì´' metadata={'source': '', 'source_file': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”.ipynb', 'lecture_title': '01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”', 'cell_type': 'markdown', 'cell_index': 10, 'code_snippet': '', 'chunk_index': 7, 'original_score': 0.5473317375085325}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5473317375085325\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] f nothing methods\n",
                        "[TITLE] simple_stmts\n",
                        "[H1] The pass statement\n",
                        "\n",
                        "pass is a null operation --- when it is executed, nothing happens.\n",
                        "It is useful as a placeholder when a statement is required syntactically, but no\n",
                        "code needs to be executed, for example::\n",
                        "\n",
                        "   def f(arg): pass    # a function that does nothing (yet)\n",
                        "\n",
                        "   class C: pass       # a class with no methods (yet)' metadata={'source': '', 'title': 'simple_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/simple_stmts.rst', 'section': 'The pass statement', 'subsection': 'The pass statement', 'has_code': True, 'chunk_index': 19, 'snippet': '[KEYWORDS] f nothing methods [TITLE] simple_stmts [H1] The pass statement  pass is a null operation --- when it is executed, nothing happens. It is useful as a placeholder when a statement is required', 'original_score': 0.680919229619865}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.680919229619865\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] contain\n",
                        "[TITLE] compound_stmts\n",
                        "[H1] ROOT\n",
                        "\n",
                        "Compound statements\n",
                        "\n",
                        "Compound statements contain (groups of) other statements; they affect or control\n",
                        "the execution of those other statements in some way. In general, compound\n",
                        "statements span multiple lines, although in simple incarnations a whole compound\n",
                        "statement may be contained in one line.\n",
                        "\n",
                        "The if, while and for statements implement\n",
                        "traditional control flow constructs. try specifies exception\n",
                        "handlers and/or cleanup code for a group of statements, while the\n",
                        "with statement allows the execution of initialization and\n",
                        "finalization code around a block of code. Function and class definitions are\n",
                        "also syntactically compound statements.' metadata={'source': '', 'title': 'compound_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/compound_stmts.rst', 'section': 'ROOT', 'subsection': 'ROOT', 'has_code': True, 'chunk_index': 0, 'snippet': '[KEYWORDS] contain [TITLE] compound_stmts [H1] ROOT  Compound statements  Compound statements contain (groups of) other statements; they affect or control the execution of those other statements in so', 'original_score': 0.541517027135458}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.541517027135458\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] compound_stmts\n",
                        "[H1] ROOT\n",
                        "\n",
                        "A compound statement consists of one or more 'clauses.' A clause consists of a\n",
                        "header and a 'suite.' The clause headers of a particular compound statement are\n",
                        "all at the same indentation level. Each clause header begins with a uniquely\n",
                        "identifying keyword and ends with a colon. A suite is a group of statements\n",
                        "controlled by a clause. A suite can be one or more semicolon-separated simple\n",
                        "statements on the same line as the header, following the header's colon, or it\n",
                        "can be one or more indented statements on subsequent lines. Only the latter\n",
                        "form of a suite can contain nested compound statements; the following is illegal,\n",
                        "mostly because it wouldn't be clear to which if clause a following\n",
                        "else clause would belong::\n",
                        "\n",
                        "   if test1: if test2: print(x)' metadata={'source': '', 'title': 'compound_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/compound_stmts.rst', 'section': 'ROOT', 'subsection': 'ROOT', 'has_code': True, 'chunk_index': 1, 'snippet': \"[TITLE] compound_stmts [H1] ROOT  A compound statement consists of one or more 'clauses.' A clause consists of a header and a 'suite.' The clause headers of a particular compound statement are all at \", 'original_score': 0.525557401102462}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.525557401102462\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] f nothing methods\n",
                        "[TITLE] simple_stmts\n",
                        "[H1] The pass statement\n",
                        "\n",
                        "pass is a null operation --- when it is executed, nothing happens.\n",
                        "It is useful as a placeholder when a statement is required syntactically, but no\n",
                        "code needs to be executed, for example::\n",
                        "\n",
                        "   def f(arg): pass    # a function that does nothing (yet)\n",
                        "\n",
                        "   class C: pass       # a class with no methods (yet)' metadata={'source': '', 'title': 'simple_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/simple_stmts.rst', 'section': 'The pass statement', 'subsection': 'The pass statement', 'has_code': True, 'chunk_index': 19, 'snippet': '[KEYWORDS] f nothing methods [TITLE] simple_stmts [H1] The pass statement  pass is a null operation --- when it is executed, nothing happens. It is useful as a placeholder when a statement is required', 'original_score': 0.598549582992512}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.598549582992512\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] contain\n",
                        "[TITLE] compound_stmts\n",
                        "[H1] ROOT\n",
                        "\n",
                        "Compound statements\n",
                        "\n",
                        "Compound statements contain (groups of) other statements; they affect or control\n",
                        "the execution of those other statements in some way. In general, compound\n",
                        "statements span multiple lines, although in simple incarnations a whole compound\n",
                        "statement may be contained in one line.\n",
                        "\n",
                        "The if, while and for statements implement\n",
                        "traditional control flow constructs. try specifies exception\n",
                        "handlers and/or cleanup code for a group of statements, while the\n",
                        "with statement allows the execution of initialization and\n",
                        "finalization code around a block of code. Function and class definitions are\n",
                        "also syntactically compound statements.' metadata={'source': '', 'title': 'compound_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/compound_stmts.rst', 'section': 'ROOT', 'subsection': 'ROOT', 'has_code': True, 'chunk_index': 0, 'snippet': '[KEYWORDS] contain [TITLE] compound_stmts [H1] ROOT  Compound statements  Compound statements contain (groups of) other statements; they affect or control the execution of those other statements in so', 'original_score': 0.6128911622137163}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.6128911622137163\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[KEYWORDS] power operator\n",
                        "[TITLE] compound_stmts\n",
                        "[H1] The match statement\n",
                        "[H2] Overview\n",
                        "\n",
                        "Here's an overview of the logical flow of a match statement:\n",
                        "\n",
                        "#. The subject expression ``subject_expr`` is evaluated and a resulting subject\n",
                        "   value obtained. If the subject expression contains a comma, a tuple is\n",
                        "   constructed using the standard rules.\n",
                        "\n",
                        "#. Each pattern in a ``case_block`` is attempted to match with the subject value. The\n",
                        "   specific rules for success or failure are described below. The match attempt can also\n",
                        "   bind some or all of the standalone names within the pattern. The precise\n",
                        "   pattern binding rules vary per pattern type and are\n",
                        "   specified below.  **Name bindings made during a successful pattern match\n",
                        "   outlive the executed block and can be used after the match statement**.' metadata={'source': '', 'title': 'compound_stmts', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/compound_stmts.rst', 'section': 'The match statement', 'subsection': 'Overview', 'has_code': True, 'chunk_index': 25, 'snippet': \"[KEYWORDS] power operator [TITLE] compound_stmts [H1] The match statement [H2] Overview  Here's an overview of the logical flow of a match statement:  #. The subject expression ``subject_expr`` is eva\", 'original_score': 0.5359419318602381}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5359419318602381\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] power operator nested main run method asyncio run\n",
                        "[TITLE] asyncio-task\n",
                        "[H1] Awaitables\n",
                        "\n",
                        "We say that an object is an **awaitable** object if it can be used\n",
                        "in an await expression. Many asyncio APIs are designed to\n",
                        "accept awaitables.\n",
                        "\n",
                        "There are three main types of *awaitable* objects:\n",
                        "**coroutines**, **Tasks**, and **Futures**.\n",
                        "\n",
                        "Python coroutines are *awaitables* and therefore can be awaited from\n",
                        "other coroutines::\n",
                        "\n",
                        "    import asyncio\n",
                        "\n",
                        "    async def nested():\n",
                        "        return 42\n",
                        "\n",
                        "    async def main():\n",
                        "        # Nothing happens if we just call \"nested()\".\n",
                        "        # A coroutine object is created but not awaited,\n",
                        "        # so it *won't run at all*.\n",
                        "        nested()  # will raise a \"RuntimeWarning\".\n",
                        "\n",
                        "        # Let's do it differently now and await it:\n",
                        "        print(await nested())  # will print \"42\".\n",
                        "\n",
                        "    asyncio.run(main())\n",
                        "\n",
                        "   In this documentation the term \"coroutine\" can be used for\n",
                        "   two closely related concepts:\n",
                        "\n",
                        "   * a *coroutine function*: an async def function;' metadata={'source': '', 'title': 'asyncio-task', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/asyncio-task.rst', 'section': 'Awaitables', 'subsection': 'Awaitables', 'has_code': True, 'chunk_index': 6, 'snippet': '[KEYWORDS] power operator nested main run method asyncio run [TITLE] asyncio-task [H1] Awaitables  We say that an object is an **awaitable** object if it can be used in an await expression. Many async', 'original_score': 0.5992209989208459}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5992209989208459\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] __await__ __await__ method object\n",
                        "[TITLE] datamodel\n",
                        "[H1] Coroutines\n",
                        "[H2] Awaitable Objects\n",
                        "\n",
                        "An awaitable object generally implements an object.__await__ method.\n",
                        "Coroutine objects returned from async def functions\n",
                        "are awaitable.\n",
                        "\n",
                        "   The generator iterator objects returned from generators\n",
                        "   decorated with types.coroutine\n",
                        "   are also awaitable, but they do not implement object.__await__.\n",
                        "\n",
                        "method object.__await__(self)\n",
                        "\n",
                        "   Must return an iterator.  Should be used to implement\n",
                        "   this method to be compatible with the await expression.\n",
                        "   The object class itself is not awaitable and does not provide\n",
                        "   this method.\n",
                        "\n",
                        "      The language doesn't place any restriction on the type or value of the\n",
                        "      objects yielded by the iterator returned by ``__await__``, as this is\n",
                        "      specific to the implementation of the asynchronous execution framework\n",
                        "      (e.g. asyncio) that will be managing the awaitable object.' metadata={'source': '', 'title': 'datamodel', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/datamodel.rst', 'section': 'Coroutines', 'subsection': 'Awaitable Objects', 'has_code': False, 'chunk_index': 173, 'snippet': '[KEYWORDS] __await__ __await__ method object [TITLE] datamodel [H1] Coroutines [H2] Awaitable Objects  An awaitable object generally implements an object.__await__ method. Coroutine objects returned f', 'original_score': 0.5649833692265266}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5649833692265266\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] expressions\n",
                        "[H1] Await expression\n",
                        "\n",
                        "Suspend the execution of coroutine on an awaitable object.\n",
                        "Can only be used inside a coroutine function.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Await expression', 'subsection': 'Await expression', 'has_code': False, 'chunk_index': 65, 'snippet': '[TITLE] expressions [H1] Await expression  Suspend the execution of coroutine on an awaitable object. Can only be used inside a coroutine function.', 'original_score': 0.5890427524351278}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5890427524351278\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] power operator nested main run method asyncio run\n",
                        "[TITLE] asyncio-task\n",
                        "[H1] Awaitables\n",
                        "\n",
                        "We say that an object is an **awaitable** object if it can be used\n",
                        "in an await expression. Many asyncio APIs are designed to\n",
                        "accept awaitables.\n",
                        "\n",
                        "There are three main types of *awaitable* objects:\n",
                        "**coroutines**, **Tasks**, and **Futures**.\n",
                        "\n",
                        "Python coroutines are *awaitables* and therefore can be awaited from\n",
                        "other coroutines::\n",
                        "\n",
                        "    import asyncio\n",
                        "\n",
                        "    async def nested():\n",
                        "        return 42\n",
                        "\n",
                        "    async def main():\n",
                        "        # Nothing happens if we just call \"nested()\".\n",
                        "        # A coroutine object is created but not awaited,\n",
                        "        # so it *won't run at all*.\n",
                        "        nested()  # will raise a \"RuntimeWarning\".\n",
                        "\n",
                        "        # Let's do it differently now and await it:\n",
                        "        print(await nested())  # will print \"42\".\n",
                        "\n",
                        "    asyncio.run(main())\n",
                        "\n",
                        "   In this documentation the term \"coroutine\" can be used for\n",
                        "   two closely related concepts:\n",
                        "\n",
                        "   * a *coroutine function*: an async def function;' metadata={'source': '', 'title': 'asyncio-task', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/asyncio-task.rst', 'section': 'Awaitables', 'subsection': 'Awaitables', 'has_code': True, 'chunk_index': 6, 'snippet': '[KEYWORDS] power operator nested main run method asyncio run [TITLE] asyncio-task [H1] Awaitables  We say that an object is an **awaitable** object if it can be used in an await expression. Many async', 'original_score': 0.5992209989208459}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5992209989208459\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] __await__ __await__ method object\n",
                        "[TITLE] datamodel\n",
                        "[H1] Coroutines\n",
                        "[H2] Awaitable Objects\n",
                        "\n",
                        "An awaitable object generally implements an object.__await__ method.\n",
                        "Coroutine objects returned from async def functions\n",
                        "are awaitable.\n",
                        "\n",
                        "   The generator iterator objects returned from generators\n",
                        "   decorated with types.coroutine\n",
                        "   are also awaitable, but they do not implement object.__await__.\n",
                        "\n",
                        "method object.__await__(self)\n",
                        "\n",
                        "   Must return an iterator.  Should be used to implement\n",
                        "   this method to be compatible with the await expression.\n",
                        "   The object class itself is not awaitable and does not provide\n",
                        "   this method.\n",
                        "\n",
                        "      The language doesn't place any restriction on the type or value of the\n",
                        "      objects yielded by the iterator returned by ``__await__``, as this is\n",
                        "      specific to the implementation of the asynchronous execution framework\n",
                        "      (e.g. asyncio) that will be managing the awaitable object.' metadata={'source': '', 'title': 'datamodel', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/datamodel.rst', 'section': 'Coroutines', 'subsection': 'Awaitable Objects', 'has_code': False, 'chunk_index': 173, 'snippet': '[KEYWORDS] __await__ __await__ method object [TITLE] datamodel [H1] Coroutines [H2] Awaitable Objects  An awaitable object generally implements an object.__await__ method. Coroutine objects returned f', 'original_score': 0.5649833692265266}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5649833692265266\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] expressions\n",
                        "[H1] Await expression\n",
                        "\n",
                        "Suspend the execution of coroutine on an awaitable object.\n",
                        "Can only be used inside a coroutine function.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Await expression', 'subsection': 'Await expression', 'has_code': False, 'chunk_index': 65, 'snippet': '[TITLE] expressions [H1] Await expression  Suspend the execution of coroutine on an awaitable object. Can only be used inside a coroutine function.', 'original_score': 0.5890427524351278}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5890427524351278\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] power operator nested main run method asyncio run\n",
                        "[TITLE] asyncio-task\n",
                        "[H1] Awaitables\n",
                        "\n",
                        "We say that an object is an **awaitable** object if it can be used\n",
                        "in an await expression. Many asyncio APIs are designed to\n",
                        "accept awaitables.\n",
                        "\n",
                        "There are three main types of *awaitable* objects:\n",
                        "**coroutines**, **Tasks**, and **Futures**.\n",
                        "\n",
                        "Python coroutines are *awaitables* and therefore can be awaited from\n",
                        "other coroutines::\n",
                        "\n",
                        "    import asyncio\n",
                        "\n",
                        "    async def nested():\n",
                        "        return 42\n",
                        "\n",
                        "    async def main():\n",
                        "        # Nothing happens if we just call \"nested()\".\n",
                        "        # A coroutine object is created but not awaited,\n",
                        "        # so it *won't run at all*.\n",
                        "        nested()  # will raise a \"RuntimeWarning\".\n",
                        "\n",
                        "        # Let's do it differently now and await it:\n",
                        "        print(await nested())  # will print \"42\".\n",
                        "\n",
                        "    asyncio.run(main())\n",
                        "\n",
                        "   In this documentation the term \"coroutine\" can be used for\n",
                        "   two closely related concepts:\n",
                        "\n",
                        "   * a *coroutine function*: an async def function;' metadata={'source': '', 'title': 'asyncio-task', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/asyncio-task.rst', 'section': 'Awaitables', 'subsection': 'Awaitables', 'has_code': True, 'chunk_index': 6, 'snippet': '[KEYWORDS] power operator nested main run method asyncio run [TITLE] asyncio-task [H1] Awaitables  We say that an object is an **awaitable** object if it can be used in an await expression. Many async', 'original_score': 0.5992255649208459}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5992255649208459\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] __await__ __await__ method object\n",
                        "[TITLE] datamodel\n",
                        "[H1] Coroutines\n",
                        "[H2] Awaitable Objects\n",
                        "\n",
                        "An awaitable object generally implements an object.__await__ method.\n",
                        "Coroutine objects returned from async def functions\n",
                        "are awaitable.\n",
                        "\n",
                        "   The generator iterator objects returned from generators\n",
                        "   decorated with types.coroutine\n",
                        "   are also awaitable, but they do not implement object.__await__.\n",
                        "\n",
                        "method object.__await__(self)\n",
                        "\n",
                        "   Must return an iterator.  Should be used to implement\n",
                        "   this method to be compatible with the await expression.\n",
                        "   The object class itself is not awaitable and does not provide\n",
                        "   this method.\n",
                        "\n",
                        "      The language doesn't place any restriction on the type or value of the\n",
                        "      objects yielded by the iterator returned by ``__await__``, as this is\n",
                        "      specific to the implementation of the asynchronous execution framework\n",
                        "      (e.g. asyncio) that will be managing the awaitable object.' metadata={'source': '', 'title': 'datamodel', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/datamodel.rst', 'section': 'Coroutines', 'subsection': 'Awaitable Objects', 'has_code': False, 'chunk_index': 173, 'snippet': '[KEYWORDS] __await__ __await__ method object [TITLE] datamodel [H1] Coroutines [H2] Awaitable Objects  An awaitable object generally implements an object.__await__ method. Coroutine objects returned f', 'original_score': 0.5649921772265266}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5649921772265266\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] expressions\n",
                        "[H1] Await expression\n",
                        "\n",
                        "Suspend the execution of coroutine on an awaitable object.\n",
                        "Can only be used inside a coroutine function.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Await expression', 'subsection': 'Await expression', 'has_code': False, 'chunk_index': 65, 'snippet': '[TITLE] expressions [H1] Await expression  Suspend the execution of coroutine on an awaitable object. Can only be used inside a coroutine function.', 'original_score': 0.5890517344351279}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5890517344351279\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n",
                        "ğŸ”„ Reranking ì§„í–‰ (í›„ë³´ 3ê°œ)...\n",
                        "Compressor ê²°ê³¼:\n",
                        "1. page_content='[KEYWORDS] power operator nested main run method asyncio run\n",
                        "[TITLE] asyncio-task\n",
                        "[H1] Awaitables\n",
                        "\n",
                        "We say that an object is an **awaitable** object if it can be used\n",
                        "in an await expression. Many asyncio APIs are designed to\n",
                        "accept awaitables.\n",
                        "\n",
                        "There are three main types of *awaitable* objects:\n",
                        "**coroutines**, **Tasks**, and **Futures**.\n",
                        "\n",
                        "Python coroutines are *awaitables* and therefore can be awaited from\n",
                        "other coroutines::\n",
                        "\n",
                        "    import asyncio\n",
                        "\n",
                        "    async def nested():\n",
                        "        return 42\n",
                        "\n",
                        "    async def main():\n",
                        "        # Nothing happens if we just call \"nested()\".\n",
                        "        # A coroutine object is created but not awaited,\n",
                        "        # so it *won't run at all*.\n",
                        "        nested()  # will raise a \"RuntimeWarning\".\n",
                        "\n",
                        "        # Let's do it differently now and await it:\n",
                        "        print(await nested())  # will print \"42\".\n",
                        "\n",
                        "    asyncio.run(main())\n",
                        "\n",
                        "   In this documentation the term \"coroutine\" can be used for\n",
                        "   two closely related concepts:\n",
                        "\n",
                        "   * a *coroutine function*: an async def function;' metadata={'source': '', 'title': 'asyncio-task', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/library/asyncio-task.rst', 'section': 'Awaitables', 'subsection': 'Awaitables', 'has_code': True, 'chunk_index': 6, 'snippet': '[KEYWORDS] power operator nested main run method asyncio run [TITLE] asyncio-task [H1] Awaitables  We say that an object is an **awaitable** object if it can be used in an await expression. Many async', 'original_score': 0.5992184249208459}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5992184249208459\n",
                        "--------------------------------------------------\n",
                        "2. page_content='[KEYWORDS] __await__ __await__ method object\n",
                        "[TITLE] datamodel\n",
                        "[H1] Coroutines\n",
                        "[H2] Awaitable Objects\n",
                        "\n",
                        "An awaitable object generally implements an object.__await__ method.\n",
                        "Coroutine objects returned from async def functions\n",
                        "are awaitable.\n",
                        "\n",
                        "   The generator iterator objects returned from generators\n",
                        "   decorated with types.coroutine\n",
                        "   are also awaitable, but they do not implement object.__await__.\n",
                        "\n",
                        "method object.__await__(self)\n",
                        "\n",
                        "   Must return an iterator.  Should be used to implement\n",
                        "   this method to be compatible with the await expression.\n",
                        "   The object class itself is not awaitable and does not provide\n",
                        "   this method.\n",
                        "\n",
                        "      The language doesn't place any restriction on the type or value of the\n",
                        "      objects yielded by the iterator returned by ``__await__``, as this is\n",
                        "      specific to the implementation of the asynchronous execution framework\n",
                        "      (e.g. asyncio) that will be managing the awaitable object.' metadata={'source': '', 'title': 'datamodel', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/datamodel.rst', 'section': 'Coroutines', 'subsection': 'Awaitable Objects', 'has_code': False, 'chunk_index': 173, 'snippet': '[KEYWORDS] __await__ __await__ method object [TITLE] datamodel [H1] Coroutines [H2] Awaitable Objects  An awaitable object generally implements an object.__await__ method. Coroutine objects returned f', 'original_score': 0.5649863212265266}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.5649863212265266\n",
                        "--------------------------------------------------\n",
                        "3. page_content='[TITLE] expressions\n",
                        "[H1] Await expression\n",
                        "\n",
                        "Suspend the execution of coroutine on an awaitable object.\n",
                        "Can only be used inside a coroutine function.' metadata={'source': '', 'title': 'expressions', 'file_path': '/Users/kim/SKN/SKN21-4th-4Team/data/raw/python_rst/reference/expressions.rst', 'section': 'Await expression', 'subsection': 'Await expression', 'has_code': False, 'chunk_index': 65, 'snippet': '[TITLE] expressions [H1] Await expression  Suspend the execution of coroutine on an awaitable object. Can only be used inside a coroutine function.', 'original_score': 0.589071294435128}\n",
                        "   ì ìˆ˜: 0.0\n",
                        "   ì›ë³¸ ì ìˆ˜: 0.589071294435128\n",
                        "--------------------------------------------------\n",
                        "âœ… Reranking ì™„ë£Œ: ìƒìœ„ 3ê°œ ì„ íƒë¨\n"
                    ]
                }
            ],
            "source": [
                "#################################################################\n",
                "# 3. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
                "#################################################################\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from src.prompts import ANALYSIS_SYSTEM_PROMPT\n",
                "from langchain_core.runnables import RunnableLambda\n",
                "\n",
                "# LLM ì„¤ì •\n",
                "llm = ChatOpenAI(model=ConfigLLM.OPENAI_MODEL, temperature=0)\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸\n",
                "prompt = ChatPromptTemplate.from_template(ANALYSIS_SYSTEM_PROMPT)\n",
                "\n",
                "# ë¬¸ì„œ í¬ë§·íŒ…\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "# Chain êµ¬ì„±\n",
                "rag_chain = (\n",
                "    {\n",
                "        \"context\": RunnableLambda(advanced_retriever) | RunnableLambda(format_docs), \n",
                "        \"query\": RunnablePassthrough()\n",
                "    }\n",
                "    | prompt\n",
                "    | llm\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "# ë‹µë³€ ìƒì„± ë° Context ìˆ˜ì§‘\n",
                "answers = []\n",
                "contexts = []\n",
                "\n",
                "print(\"í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
                "for q in eval_data['question']:\n",
                "    # 1. ë¬¸ì„œ ê²€ìƒ‰\n",
                "    retrieved_docs = advanced_retriever(q)\n",
                "    context_text = [doc.page_content for doc in retrieved_docs]\n",
                "    contexts.append(context_text)\n",
                "    \n",
                "    # 2. ë‹µë³€ ìƒì„±\n",
                "    # Chainì„ ì§ì ‘ í˜¸ì¶œí•˜ë©´ contextë¥¼ ì¬ê²€ìƒ‰í•˜ë¯€ë¡œ(retrieverê°€ chain ì•ˆì— ìˆìŒ),\n",
                "    # íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¯¸ë¦¬ ê²€ìƒ‰í•œ docsë¥¼ ì£¼ì…í•˜ê±°ë‚˜, ê·¸ëƒ¥ chainì„ invokeí•©ë‹ˆë‹¤.\n",
                "    # ì—¬ê¸°ì„œëŠ” ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ chain invoke ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "    response = rag_chain.invoke(q)\n",
                "    answers.append(response)\n",
                "    # print(f\"Q: {q}\\nA: {response[:50]}...\\n\")\n",
                "\n",
                "# ë°ì´í„°ì…‹ì— ê²°ê³¼ ì¶”ê°€\n",
                "eval_data['answer'] = answers\n",
                "eval_data['contexts'] = contexts\n",
                "\n",
                "final_dataset = Dataset.from_dict(eval_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating:  15%|â–ˆâ–Œ        | 9/60 [00:19<01:24,  1.65s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 20/60 [00:37<00:45,  1.14s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/60 [00:40<00:51,  1.32s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/60 [00:57<00:42,  1.47s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [01:08<00:47,  1.98s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 41/60 [01:13<00:26,  1.40s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/60 [01:24<00:12,  1.23s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
                        "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:38<00:00,  1.64s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========== í‰ê°€ ê²°ê³¼ ==========\n",
                        "{'context_precision': 0.8333, 'context_recall': 0.7044, 'faithfulness': 0.8240, 'answer_relevancy': 0.5833}\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_input</th>\n",
                            "      <th>retrieved_contexts</th>\n",
                            "      <th>reference_contexts</th>\n",
                            "      <th>response</th>\n",
                            "      <th>reference</th>\n",
                            "      <th>persona_name</th>\n",
                            "      <th>query_style</th>\n",
                            "      <th>query_length</th>\n",
                            "      <th>context_precision</th>\n",
                            "      <th>context_recall</th>\n",
                            "      <th>faithfulness</th>\n",
                            "      <th>answer_relevancy</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>ì •ì„±ì  í‰ê°€ë€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>0.833333</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.344786</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤: **ì˜¤í”„ë¼ì¸ í‰ê°€*...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ ì˜¨ë¼ì¸ í‰ê°€ë¡œ ë‚˜ë‰˜...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.796023</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ë­ì—ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>[[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œ í•™ìƒì˜ í•™ìŠµ ì„±ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.624273</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>What are the key attributes of sklearn.cluster...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `n...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `l...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>0.833333</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.849916</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ë¬´ì—‡...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...</td>\n",
                            "      <td>Data Evaluation Analyst</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>LONG</td>\n",
                            "      <td>0.833333</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.770000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>sklearn.cluster.KMeansëŠ” ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>[[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ëŠ” K-í‰ê·  êµ°ì§‘í™” ...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ëŠ” K-í‰ê·  êµ°ì§‘í™” ...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.800000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.706331</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>tensor ì–´ë–»ê²Œ ìƒì„±í•˜ë‚˜ìš”?</td>\n",
                            "      <td>[[KEYWORDS] array floor division\\n[TITLE] arra...</td>\n",
                            "      <td>[[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: ê°•ì˜ ìë£Œì—ì„œëŠ” í…ì„œ ìƒì„±ì— ëŒ€í•œ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤....</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: tensorëŠ” `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>MISSPELLED</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.428571</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>How can I create an empty tensor using torch?</td>\n",
                            "      <td>[[TITLE] typing\\n[H1] Annotating tuples\\n\\n# E...</td>\n",
                            "      <td>[[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...</td>\n",
                            "      <td>ê°•ì˜ ìë£Œì—ì„œëŠ” ë¹ˆ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ˆ ten...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.400000</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>tensor ì–´ë–»ê²Œ ë§Œë“¤ì–´ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\nì›í•˜ëŠ” í˜•íƒœ(shape) í…ì„œ ìƒì„±\\n...</td>\n",
                            "      <td>[[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...</td>\n",
                            "      <td>í…ì„œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” `torch.tensor(ìë£Œêµ¬ì¡° [, dtype])` í•¨ìˆ˜...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: tensorëŠ” `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©...</td>\n",
                            "      <td>Data Evaluation Analyst</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.529952</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\n- ### ë¶„ë¥˜(Classification):\\...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜&nbsp;&nbsp;\\nì§€ë„í•™ìŠµ(Super...</td>\n",
                            "      <td>íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ **ìˆ«ì(ì—°ì†ëœ ê°’)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì—­í• **ì„ í•©ë‹ˆë‹¤. ì¦‰, ì£¼ì–´ì§„...</td>\n",
                            "      <td>íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì¦‰, ì£¼ì–´ì§„ ë°ì´í„°ì˜ íŠ¹...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>WEB_SEARCH_LIKE</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.750000</td>\n",
                            "      <td>0.857143</td>\n",
                            "      <td>0.999998</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>ë¨¸ì‹ ëŸ¬ë‹ì€ ë­ì—ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\nê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´&nbsp;&nbsp;...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜&nbsp;&nbsp;\\nì§€ë„í•™ìŠµ(Super...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ë¡œ, ëª…ì‹œì ì¸ ê·œì¹™ì„...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ê³¼ ì •ë‹µ(Label)ì„ ...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.750000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.550786</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•ì€ ë­ì—ìš”?</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜&nbsp;&nbsp;\\nì§€ë„í•™ìŠµ(Super...</td>\n",
                            "      <td>[[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜&nbsp;&nbsp;\\nì§€ë„í•™ìŠµ(Super...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•(Feature)ì€ ë°ì´í„°ì˜ ì¤‘ìš”í•œ ì†ì„±ì´ë‚˜ ì •...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•(Feature)ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ”...</td>\n",
                            "      <td>Data Evaluation Analyst</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.800000</td>\n",
                            "      <td>0.534458</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>12</th>\n",
                            "      <td>íŒŒì´ì¬ì—ì„œ 'pass'ëŠ” ë­í•˜ëŠ” ê±°ì•¼?</td>\n",
                            "      <td>[[KEYWORDS] f nothing methods\\n[TITLE] simple_...</td>\n",
                            "      <td>[[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€&nbsp;&nbsp;\\nì‹...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: íŒŒì´ì¬ì—ì„œ `pass`ëŠ” ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë„(null...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: íŒŒì´ì¬ì—ì„œ 'pass'ëŠ” ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë¹ˆ ë¬¸ì¥ì„ ë‚˜íƒ€ë‚´ëŠ”...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>POOR_GRAMMAR</td>\n",
                            "      <td>MEDIUM</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.600000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.486210</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>What is the significance of the term 'await' i...</td>\n",
                            "      <td>[[KEYWORDS] power operator nested main run met...</td>\n",
                            "      <td>[[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€&nbsp;&nbsp;\\nì‹...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: 'await'ëŠ” Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ í‚¤ì›Œë“œ...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: 'await'ëŠ” ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¡œ, ë¹„ë™ê¸° í•¨ìˆ˜ì˜...</td>\n",
                            "      <td>Data Science Student</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.666667</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.778169</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>What is the significance of the term 'await' i...</td>\n",
                            "      <td>[[KEYWORDS] power operator nested main run met...</td>\n",
                            "      <td>[[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€&nbsp;&nbsp;\\nì‹...</td>\n",
                            "      <td>**í•µì‹¬ ë‹µë³€**: 'await'ëŠ” Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ í‚¤ì›Œë“œ...</td>\n",
                            "      <td>í•µì‹¬ ë‹µë³€: 'await'ëŠ” ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¡œ, ë¹„ë™ê¸° í•¨ìˆ˜ì˜...</td>\n",
                            "      <td>Data Science Educator</td>\n",
                            "      <td>PERFECT_GRAMMAR</td>\n",
                            "      <td>SHORT</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>1.000000</td>\n",
                            "      <td>0.875000</td>\n",
                            "      <td>0.778169</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                           user_input  \\\n",
                            "0                                      ì •ì„±ì  í‰ê°€ë€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "1                         RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?   \n",
                            "2                                       ì˜¤í”„ë¼ì¸ í‰ê°€ê°€ ë­ì—ìš”?   \n",
                            "3   What are the key attributes of sklearn.cluster...   \n",
                            "4   sklearn.cluster.KMeansì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° n_clustersëŠ” ë¬´ì—‡...   \n",
                            "5                      sklearn.cluster.KMeansëŠ” ë¬´ì—‡ì¸ê°€ìš”?   \n",
                            "6                                   tensor ì–´ë–»ê²Œ ìƒì„±í•˜ë‚˜ìš”?   \n",
                            "7       How can I create an empty tensor using torch?   \n",
                            "8                                    tensor ì–´ë–»ê²Œ ë§Œë“¤ì–´ìš”?   \n",
                            "9                              íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?   \n",
                            "10                                         ë¨¸ì‹ ëŸ¬ë‹ì€ ë­ì—ìš”?   \n",
                            "11                                    ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•ì€ ë­ì—ìš”?   \n",
                            "12                              íŒŒì´ì¬ì—ì„œ 'pass'ëŠ” ë­í•˜ëŠ” ê±°ì•¼?   \n",
                            "13  What is the significance of the term 'await' i...   \n",
                            "14  What is the significance of the term 'await' i...   \n",
                            "\n",
                            "                                   retrieved_contexts  \\\n",
                            "0   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "1   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "2   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "3   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "4   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "5   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "6   [[KEYWORDS] array floor division\\n[TITLE] arra...   \n",
                            "7   [[TITLE] typing\\n[H1] Annotating tuples\\n\\n# E...   \n",
                            "8   [[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\nì›í•˜ëŠ” í˜•íƒœ(shape) í…ì„œ ìƒì„±\\n...   \n",
                            "9   [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\n- ### ë¶„ë¥˜(Classification):\\...   \n",
                            "10  [[ê°•ì˜: 01_ë”¥ëŸ¬ë‹ ê°œìš”]\\n\\nê¸°ì¡´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì°¨ì´  ...   \n",
                            "11  [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Super...   \n",
                            "12  [[KEYWORDS] f nothing methods\\n[TITLE] simple_...   \n",
                            "13  [[KEYWORDS] power operator nested main run met...   \n",
                            "14  [[KEYWORDS] power operator nested main run met...   \n",
                            "\n",
                            "                                   reference_contexts  \\\n",
                            "0   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "1   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "2   [[ê°•ì˜: 11_RAG_evaluation]\\n\\nRAG í‰ê°€ ê°œìš”\\n- RAG í‰...   \n",
                            "3   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "4   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "5   [[ê°•ì˜: 14 êµ°ì§‘_Clustering]\\n\\nKMeans\\n- sklearn.c...   \n",
                            "6   [[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...   \n",
                            "7   [[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...   \n",
                            "8   [[ê°•ì˜: 02. tensor ë‹¤ë£¨ê¸°]\\n\\në¹ˆ tensor ìƒì„±\\n- **torc...   \n",
                            "9   [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Super...   \n",
                            "10  [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Super...   \n",
                            "11  [[ê°•ì˜: 01_ë¨¸ì‹ ëŸ¬ë‹ê°œìš”]\\n\\në¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜  \\nì§€ë„í•™ìŠµ(Super...   \n",
                            "12  [[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹...   \n",
                            "13  [[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹...   \n",
                            "14  [[ê°•ì˜: 02_ë³€ìˆ˜ì™€ ë°ì´í„°íƒ€ì…]\\n\\nì‹ë³„ì ê·œì¹™ê³¼ ë³€ìˆ˜ì´ë¦„ ì£¼ëŠ” ê´€ë¡€  \\nì‹...   \n",
                            "\n",
                            "                                             response  \\\n",
                            "0   **í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...   \n",
                            "1   RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤: **ì˜¤í”„ë¼ì¸ í‰ê°€*...   \n",
                            "2   **í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œ í•™ìƒì˜ í•™ìŠµ ì„±ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´...   \n",
                            "3   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `n...   \n",
                            "4   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...   \n",
                            "5   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ëŠ” K-í‰ê·  êµ°ì§‘í™” ...   \n",
                            "6   í•µì‹¬ ë‹µë³€: ê°•ì˜ ìë£Œì—ì„œëŠ” í…ì„œ ìƒì„±ì— ëŒ€í•œ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤....   \n",
                            "7   ê°•ì˜ ìë£Œì—ì„œëŠ” ë¹ˆ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤...   \n",
                            "8   í…ì„œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” `torch.tensor(ìë£Œêµ¬ì¡° [, dtype])` í•¨ìˆ˜...   \n",
                            "9   íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ **ìˆ«ì(ì—°ì†ëœ ê°’)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì—­í• **ì„ í•©ë‹ˆë‹¤. ì¦‰, ì£¼ì–´ì§„...   \n",
                            "10  **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„° í•™ìŠµ ê¸°ë°˜ì˜ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ë¡œ, ëª…ì‹œì ì¸ ê·œì¹™ì„...   \n",
                            "11  **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•(Feature)ì€ ë°ì´í„°ì˜ ì¤‘ìš”í•œ ì†ì„±ì´ë‚˜ ì •...   \n",
                            "12  **í•µì‹¬ ë‹µë³€**: íŒŒì´ì¬ì—ì„œ `pass`ëŠ” ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë„(null...   \n",
                            "13  **í•µì‹¬ ë‹µë³€**: 'await'ëŠ” Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ í‚¤ì›Œë“œ...   \n",
                            "14  **í•µì‹¬ ë‹µë³€**: 'await'ëŠ” Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ìœ„í•œ í‚¤ì›Œë“œ...   \n",
                            "\n",
                            "                                            reference  \\\n",
                            "0   **í•µì‹¬ ë‹µë³€**: ì •ì„±ì  í‰ê°€ëŠ” ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆì„...   \n",
                            "1   **í•µì‹¬ ë‹µë³€**: RAG ì‹œìŠ¤í…œì˜ í‰ê°€ ë°©ë²•ì€ ì˜¤í”„ë¼ì¸ í‰ê°€ì™€ ì˜¨ë¼ì¸ í‰ê°€ë¡œ ë‚˜ë‰˜...   \n",
                            "2   **í•µì‹¬ ë‹µë³€**: ì˜¤í”„ë¼ì¸ í‰ê°€ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜...   \n",
                            "3   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ ì£¼ìš” ì†ì„±ì€ `l...   \n",
                            "4   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° `...   \n",
                            "5   **í•µì‹¬ ë‹µë³€**: `sklearn.cluster.KMeans`ëŠ” K-í‰ê·  êµ°ì§‘í™” ...   \n",
                            "6   **í•µì‹¬ ë‹µë³€**: tensorëŠ” `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©...   \n",
                            "7   **í•µì‹¬ ë‹µë³€**: `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ˆ ten...   \n",
                            "8   **í•µì‹¬ ë‹µë³€**: tensorëŠ” `torch.empty(*size)` í•¨ìˆ˜ë¥¼ ì‚¬ìš©...   \n",
                            "9   íšŒê·€ëŠ” ì§€ë„í•™ìŠµì—ì„œ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì¦‰, ì£¼ì–´ì§„ ë°ì´í„°ì˜ íŠ¹...   \n",
                            "10  **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì˜ íŠ¹ì§•(Feature)ê³¼ ì •ë‹µ(Label)ì„ ...   \n",
                            "11  **í•µì‹¬ ë‹µë³€**: ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ íŠ¹ì§•(Feature)ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ”...   \n",
                            "12  í•µì‹¬ ë‹µë³€: íŒŒì´ì¬ì—ì„œ 'pass'ëŠ” ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë¹ˆ ë¬¸ì¥ì„ ë‚˜íƒ€ë‚´ëŠ”...   \n",
                            "13  í•µì‹¬ ë‹µë³€: 'await'ëŠ” ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¡œ, ë¹„ë™ê¸° í•¨ìˆ˜ì˜...   \n",
                            "14  í•µì‹¬ ë‹µë³€: 'await'ëŠ” ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¡œ, ë¹„ë™ê¸° í•¨ìˆ˜ì˜...   \n",
                            "\n",
                            "               persona_name      query_style query_length  context_precision  \\\n",
                            "0     Data Science Educator  WEB_SEARCH_LIKE        SHORT           0.833333   \n",
                            "1      Data Science Student     POOR_GRAMMAR         LONG           1.000000   \n",
                            "2      Data Science Student     POOR_GRAMMAR       MEDIUM           1.000000   \n",
                            "3      Data Science Student  PERFECT_GRAMMAR       MEDIUM           0.833333   \n",
                            "4   Data Evaluation Analyst       MISSPELLED         LONG           0.833333   \n",
                            "5     Data Science Educator       MISSPELLED        SHORT           1.000000   \n",
                            "6     Data Science Educator       MISSPELLED        SHORT           0.000000   \n",
                            "7     Data Science Educator  PERFECT_GRAMMAR        SHORT           0.000000   \n",
                            "8   Data Evaluation Analyst     POOR_GRAMMAR        SHORT           1.000000   \n",
                            "9      Data Science Student  WEB_SEARCH_LIKE        SHORT           1.000000   \n",
                            "10    Data Science Educator     POOR_GRAMMAR        SHORT           1.000000   \n",
                            "11  Data Evaluation Analyst     POOR_GRAMMAR       MEDIUM           1.000000   \n",
                            "12    Data Science Educator     POOR_GRAMMAR       MEDIUM           1.000000   \n",
                            "13     Data Science Student  PERFECT_GRAMMAR        SHORT           1.000000   \n",
                            "14    Data Science Educator  PERFECT_GRAMMAR        SHORT           1.000000   \n",
                            "\n",
                            "    context_recall  faithfulness  answer_relevancy  \n",
                            "0         1.000000      1.000000          0.344786  \n",
                            "1         1.000000      1.000000          0.796023  \n",
                            "2         1.000000      0.000000          0.624273  \n",
                            "3         1.000000      1.000000          0.849916  \n",
                            "4         1.000000      1.000000          0.770000  \n",
                            "5         0.800000      1.000000          0.706331  \n",
                            "6         0.000000      0.428571          0.000000  \n",
                            "7         0.000000      0.400000          0.000000  \n",
                            "8         0.000000      1.000000          0.529952  \n",
                            "9         0.750000      0.857143          0.999998  \n",
                            "10        0.750000      1.000000          0.550786  \n",
                            "11        1.000000      0.800000          0.534458  \n",
                            "12        0.600000      1.000000          0.486210  \n",
                            "13        0.666667      1.000000          0.778169  \n",
                            "14        1.000000      0.875000          0.778169  "
                        ]
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#################################################################\n",
                "# 4. Ragas í‰ê°€ ì‹¤í–‰\n",
                "#################################################################\n",
                "\n",
                "result = evaluate(\n",
                "    final_dataset,\n",
                "    metrics=[\n",
                "        context_precision,\n",
                "        context_recall,\n",
                "        faithfulness,\n",
                "        answer_relevancy,\n",
                "    ],\n",
                "    llm=llm,\n",
                "    embeddings=OpenAIEmbeddings(model=ConfigDB.EMBEDDING_MODEL)\n",
                ")\n",
                "\n",
                "print(\"\\n========== í‰ê°€ ê²°ê³¼ ==========\")\n",
                "print(result)\n",
                "df_result = result.to_pandas()\n",
                "df_result"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
